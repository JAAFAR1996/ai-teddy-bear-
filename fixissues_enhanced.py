#!/usr/bin/env python3
"""
AI Teddy Bear Project Enhanced Deep Cleaner & Fixer
==================================================
Ù†Ø³Ø®Ø© Ù…Ø­Ø³Ù‘Ù†Ø© Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙˆØ£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª
- Ø§Ø³ØªØ®Ø¯Ø§Ù… os.scandir() Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† os.walk()
- Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª ØªÙƒØ±Ø§Ø± Ù…Ø­Ø³Ù‘Ù†Ø© O(n log n)
- AST Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¢Ù…Ù†
- Logging Ù…Ø±ÙƒØ²ÙŠ Ù…Ø¹ RotatingFileHandler
- Progress tracking
- Unit tests ready
"""

import os
import ast
import sys
import shutil
import hashlib
import json
import datetime
import logging
import argparse
import tempfile
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional, Iterator, Any
from collections import defaultdict
from logging.handlers import RotatingFileHandler
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

# === ENHANCED LOGGING CONFIGURATION ===
class EnhancedColoredFormatter(logging.Formatter):
    """Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ù„ÙˆÙ†Ø© Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ©"""
    
    COLORS = {
        'DEBUG': '\033[36m',    # Cyan
        'INFO': '\033[32m',     # Green  
        'WARNING': '\033[33m',  # Yellow
        'ERROR': '\033[31m',    # Red
        'CRITICAL': '\033[35m', # Magenta
        'PROGRESS': '\033[94m', # Blue
    }
    RESET = '\033[0m'
    
    def format(self, record):
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª thread Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø®ÙŠÙˆØ·
        if hasattr(record, 'thread_name'):
            record.thread_info = f"[{record.thread_name}]"
        else:
            record.thread_info = ""
            
        log_color = self.COLORS.get(record.levelname, self.RESET)
        record.levelname = f"{log_color}{record.levelname}{self.RESET}"
        return super().format(record)


def setup_enhanced_logging(log_file: str = "project_cleanup_enhanced.log", 
                          max_size: int = 10 * 1024 * 1024,  # 10MB
                          backup_count: int = 5) -> logging.Logger:
    """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… logging Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ RotatingFileHandler"""
    
    # Ø¥Ù†Ø´Ø§Ø¡ logger Ù…Ø®ØµØµ
    logger = logging.getLogger('project_cleaner')
    logger.setLevel(logging.DEBUG)
    
    # ØªØ¬Ù†Ø¨ Ø¥Ø¶Ø§ÙØ© handlers Ù…ØªØ¹Ø¯Ø¯Ø©
    if logger.handlers:
        return logger
    
    # Console handler with enhanced colors
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_formatter = EnhancedColoredFormatter(
        '%(asctime)s %(thread_info)s- %(levelname)s - %(message)s',
        datefmt='%H:%M:%S'
    )
    console_handler.setFormatter(console_formatter)
    
    # Rotating file handler Ù„Ù…Ù†Ø¹ ÙƒØ¨Ø± Ø­Ø¬Ù… Ø§Ù„Ø³Ø¬Ù„
    file_handler = RotatingFileHandler(
        log_file, 
        maxBytes=max_size, 
        backupCount=backup_count,
        encoding='utf-8'
    )
    file_handler.setLevel(logging.DEBUG)
    file_formatter = logging.Formatter(
        '%(asctime)s - %(threadName)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'
    )
    file_handler.setFormatter(file_formatter)
    
    # Ø¥Ø¶Ø§ÙØ© handlers
    logger.addHandler(console_handler)
    logger.addHandler(file_handler)
    
    # Ù…Ù†Ø¹ Ø§Ù„Ø§Ù†ØªØ´Ø§Ø± Ù„Ù„Ù€ root logger
    logger.propagate = False
    
    return logger


# === SAFE AST CODE TRANSFORMER ===
class SafeCodeTransformer(ast.NodeTransformer):
    """Ù…Ø­ÙˆÙ„ Ø¢Ù…Ù† Ù„Ù„ÙƒÙˆØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… AST Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† regex"""
    
    def __init__(self):
        self.changes_made = 0
        self.issues_found = []
        
    def visit_Call(self, node: ast.Call) -> Any:
        """Ø²ÙŠØ§Ø±Ø© Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„"""
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ print Ø¨Ù€ logger
        if (isinstance(node.func, ast.Name) and 
            node.func.id == 'print'):
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ logger.info
            logger_attr = ast.Attribute(
                value=ast.Name(id='logger', ctx=ast.Load()),
                attr='info',
                ctx=ast.Load()
            )
            
            new_call = ast.Call(
                func=logger_attr,
                args=node.args,
                keywords=node.keywords
            )
            
            self.changes_made += 1
            self.issues_found.append(f"Ø§Ø³ØªØ¨Ø¯Ø§Ù„ print Ø¨Ù€ logger ÙÙŠ Ø§Ù„Ø³Ø·Ø± {node.lineno}")
            return new_call
            
        # Ø¥Ø²Ø§Ù„Ø© eval/exec - Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¨Ù€ ast.literal_eval Ø£Ùˆ ØªØ­Ø°ÙŠØ±
        elif (isinstance(node.func, ast.Name) and 
              node.func.id in ['eval', 'exec']):
            
            if node.func.id == 'eval':
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ eval Ø¨Ù€ ast.literal_eval
                literal_eval = ast.Attribute(
                    value=ast.Name(id='ast', ctx=ast.Load()),
                    attr='literal_eval', 
                    ctx=ast.Load()
                )
                new_call = ast.Call(
                    func=literal_eval,
                    args=node.args,
                    keywords=node.keywords
                )
                self.changes_made += 1
                self.issues_found.append(f"Ø§Ø³ØªØ¨Ø¯Ø§Ù„ eval Ø¨Ù€ ast.literal_eval ÙÙŠ Ø§Ù„Ø³Ø·Ø± {node.lineno}")
                return new_call
            else:
                # Ø¥Ø²Ø§Ù„Ø© exec ÙˆØ¥Ø¶Ø§ÙØ© ØªØ­Ø°ÙŠØ±
                self.changes_made += 1
                self.issues_found.append(f"Ø¥Ø²Ø§Ù„Ø© exec Ø®Ø·Ø±Ø© ÙÙŠ Ø§Ù„Ø³Ø·Ø± {node.lineno}")
                return ast.Constant(value=None)  # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¨Ù€ None
                
        return self.generic_visit(node)
    
    def visit_ExceptHandler(self, node: ast.ExceptHandler) -> Any:
        """ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ø¶Ø¹ÙŠÙØ©"""
        if node.type is None:  # except: Ø¨Ø¯ÙˆÙ† ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹
            # Ø¥Ø¶Ø§ÙØ© Exception as e
            node.type = ast.Name(id='Exception', ctx=ast.Load())
            node.name = 'e'
            self.changes_made += 1
            self.issues_found.append(f"ØªØ­Ø³ÙŠÙ† Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø¶Ø¹ÙŠÙØ© ÙÙŠ Ø§Ù„Ø³Ø·Ø± {node.lineno}")
            
        return self.generic_visit(node)


# === ENHANCED FILE SCANNER ===
class EnhancedFileScanner:
    """Ù…Ø§Ø³Ø­ Ù…Ù„ÙØ§Øª Ù…Ø­Ø³Ù‘Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… os.scandir()"""
    
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.progress_count = 0
        self.progress_lock = threading.Lock()
        
    def iter_files(self, root: Path, 
                   extensions: Set[str] = None,
                   ignore_patterns: Set[str] = None) -> Iterator[Path]:
        """ØªÙƒØ±Ø§Ø± Ù…Ø­Ø³Ù‘Ù† Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… os.scandir()"""
        
        if ignore_patterns is None:
            ignore_patterns = {'.git', '__pycache__', 'venv', 'env', 
                             '.idea', '.vscode', 'node_modules'}
        
        try:
            with os.scandir(root) as entries:
                for entry in entries:
                    # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙ‚Ø¯Ù…
                    with self.progress_lock:
                        self.progress_count += 1
                        if self.progress_count % 1000 == 0:
                            self.logger.info(f"ğŸ” ÙØ­Øµ {self.progress_count} Ø¹Ù†ØµØ± Ø­ØªÙ‰ Ø§Ù„Ø¢Ù†...")
                    
                    if entry.is_dir(follow_symlinks=False):
                        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
                        if entry.name not in ignore_patterns:
                            yield from self.iter_files(
                                Path(entry.path), 
                                extensions, 
                                ignore_patterns
                            )
                    else:
                        # ÙÙ„ØªØ±Ø© Ø­Ø³Ø¨ Ø§Ù„Ø§Ù…ØªØ¯Ø§Ø¯Ø§Øª
                        if extensions is None or Path(entry.path).suffix in extensions:
                            yield Path(entry.path)
                            
        except (OSError, PermissionError) as e:
            self.logger.warning(f"ØªØ¹Ø°Ø± Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ {root}: {e}")


# === OPTIMIZED DUPLICATE DETECTOR ===
class OptimizedDuplicateDetector:
    """ÙƒØ§Ø´Ù ØªÙƒØ±Ø§Ø±Ø§Øª Ù…Ø­Ø³Ù‘Ù† Ø¨ØªØ¹Ù‚ÙŠØ¯ O(n log n)"""
    
    def __init__(self, logger: logging.Logger):
        self.logger = logger
        self.size_groups: Dict[int, List[Path]] = defaultdict(list)
        self.hash_cache: Dict[Path, str] = {}
        
    def find_duplicates(self, files: List[Path]) -> List[Dict]:
        """Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø¨ÙƒÙØ§Ø¡Ø© Ø¹Ø§Ù„ÙŠØ©"""
        
        self.logger.info("ğŸ” ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…...")
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…
        for file_path in files:
            try:
                size = file_path.stat().st_size
                self.size_groups[size].append(file_path)
            except (OSError, FileNotFoundError):
                continue
        
        self.logger.info(f"ğŸ“Š ÙˆÙØ¬Ø¯ {len(self.size_groups)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ø­Ø¬Ù… Ù…Ø®ØªÙ„ÙØ©")
        
        duplicates = []
        potential_groups = [(size, files) for size, files in self.size_groups.items() 
                           if len(files) > 1]
        
        self.logger.info(f"ğŸ” ÙØ­Øµ {len(potential_groups)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ø­ØªÙ…Ù„Ø© Ù„Ù„ØªÙƒØ±Ø§Ø±...")
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ÙØ­Øµ hash ÙÙ‚Ø· Ù„Ù„Ù…Ù„ÙØ§Øª Ø°Ø§Øª Ù†ÙØ³ Ø§Ù„Ø­Ø¬Ù…
        for size, size_group in potential_groups:
            hash_groups = self._group_by_hash(size_group)
            
            for file_hash, hash_group in hash_groups.items():
                if len(hash_group) > 1:
                    # ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠÙ‡Ø§ Ù†Ø­ØªÙØ¸ Ø¨Ù‡
                    sorted_files = self._sort_files_by_priority(hash_group)
                    keep_file = sorted_files[0]
                    
                    for duplicate_file in sorted_files[1:]:
                        duplicates.append({
                            'file1': str(keep_file),
                            'file2': str(duplicate_file),
                            'keep': str(keep_file),
                            'remove': str(duplicate_file),
                            'size': size,
                            'hash': file_hash
                        })
        
        self.logger.info(f"âœ… Ø§ÙƒØªØ´Ù {len(duplicates)} Ù…Ù„Ù Ù…ÙƒØ±Ø±")
        return duplicates
    
    def _group_by_hash(self, files: List[Path]) -> Dict[str, List[Path]]:
        """ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ hash"""
        hash_groups = defaultdict(list)
        
        for file_path in files:
            try:
                file_hash = self._get_file_hash(file_path)
                hash_groups[file_hash].append(file_path)
            except Exception as e:
                self.logger.debug(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ hash Ù„Ù€ {file_path}: {e}")
                continue
                
        return hash_groups
    
    def _get_file_hash(self, file_path: Path) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ø§Ù„Ù…Ù„Ù Ù…Ø¹ cache"""
        if file_path in self.hash_cache:
            return self.hash_cache[file_path]
            
        try:
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¹Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø©
                for chunk in iter(lambda: f.read(8192), b""):
                    hasher.update(chunk)
            
            file_hash = hasher.hexdigest()
            self.hash_cache[file_path] = file_hash
            return file_hash
            
        except Exception as e:
            raise Exception(f"ÙØ´Ù„ ÙÙŠ Ø­Ø³Ø§Ø¨ hash: {e}")
    
    def _sort_files_by_priority(self, files: List[Path]) -> List[Path]:
        """ØªØ±ØªÙŠØ¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© (Ø£ÙŠÙ‡Ø§ Ù†Ø­ØªÙØ¸ Ø¨Ù‡)"""
        def priority_key(file_path: Path) -> Tuple[int, int, str]:
            # 1. ØªØ¬Ù†Ø¨ backup folders
            backup_penalty = 0 if 'backup' in str(file_path).lower() else 1
            
            # 2. ØªÙØ¶ÙŠÙ„ src/ folder  
            src_bonus = 1 if 'src/' in str(file_path) else 0
            
            # 3. ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø§Ù„Ø£ØµÙ„ÙŠ)
            try:
                mtime = -file_path.stat().st_mtime  # Ø³Ø§Ù„Ø¨ Ù„Ù„ØªØ±ØªÙŠØ¨ Ø§Ù„Ø¹ÙƒØ³ÙŠ
            except:
                mtime = 0
                
            return (backup_penalty, src_bonus, mtime)
        
        return sorted(files, key=priority_key, reverse=True)


# === MAIN ENHANCED CLEANER CLASS ===
class EnhancedProjectCleaner:
    """Ù…Ù†Ø¸Ù Ù…Ø´Ø±ÙˆØ¹ Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª"""
    
    def __init__(self, project_root: str, dry_run: bool = True, 
                 max_workers: int = 4):
        self.project_root = Path(project_root).resolve()
        self.dry_run = dry_run
        self.max_workers = max_workers
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ logging
        self.logger = setup_enhanced_logging()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
        self.file_scanner = EnhancedFileScanner(self.logger)
        self.duplicate_detector = OptimizedDuplicateDetector(self.logger)
        self.ast_transformer = SafeCodeTransformer()
        
        # Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        self.backup_dir = self.project_root / f"enhanced_backup_{timestamp}"
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ø­Ø³Ù‘Ù†Ø©
        self.stats = {
            'files_scanned': 0,
            'directories_scanned': 0,
            'issues_found': 0,
            'issues_fixed': 0,
            'duplicates_removed': 0,
            'security_fixes': 0,
            'ast_transformations': 0,
            'encoding_fixes': 0,
            'performance_optimizations': 0
        }
        
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        self.issues = {
            'duplicates': [],
            'security_issues': [],
            'code_quality': [],
            'encoding_problems': [],
            'large_files': [],
            'empty_files': [],
            'misplaced_files': []
        }
    
    def create_safe_backup(self) -> bool:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¢Ù…Ù†Ø©"""
        if self.dry_run:
            self.logger.info("ğŸ” ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© - ØªØ®Ø·ÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
            return True
            
        try:
            self.logger.info(f"ğŸ’¾ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¢Ù…Ù†Ø© ÙÙŠ: {self.backup_dir}")
            
            def ignore_func(dir_path, filenames):
                """ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ© ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"""
                ignore = set()
                for filename in filenames:
                    if (filename.endswith(('.pyc', '.pyo')) or
                        filename in {'__pycache__', '.git', 'venv', 'env', 
                                   'node_modules', '.idea', '.vscode'}):
                        ignore.add(filename)
                return ignore
            
            shutil.copytree(self.project_root, self.backup_dir, ignore=ignore_func)
            self.logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {e}")
            return False 