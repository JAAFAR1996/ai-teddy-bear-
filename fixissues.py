#!/usr/bin/env python3
"""
AI Teddy Bear Project Deep Cleaner & Fixer
==========================================
ÙŠÙ‚ÙˆÙ… Ø¨Ø¥ØµÙ„Ø§Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù‚ÙŠÙ…ØªÙ‡ ÙˆÙ…ÙŠØ²Ø§ØªÙ‡
"""

import os
import shutil
import hashlib
import json
import re
import ast
import datetime
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional
from collections import defaultdict
import logging
import argparse
import yaml

# Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ³Ø¬ÙŠÙ„
class ColoredFormatter(logging.Formatter):
    """Ù…Ù„ÙˆÙ† Ù„Ù„Ø±Ø³Ø§Ø¦Ù„ ÙÙŠ console"""
    
    COLORS = {
        'DEBUG': '\033[36m',    # Cyan
        'INFO': '\033[32m',     # Green
        'WARNING': '\033[33m',  # Yellow
        'ERROR': '\033[31m',    # Red
        'CRITICAL': '\033[35m', # Magenta
    }
    RESET = '\033[0m'
    
    def format(self, record):
        log_color = self.COLORS.get(record.levelname, self.RESET)
        record.levelname = f"{log_color}{record.levelname}{self.RESET}"
        return super().format(record)

# Ø¥Ø¹Ø¯Ø§Ø¯ logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# Console handler with colors
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = ColoredFormatter('%(asctime)s - %(levelname)s - %(message)s')
console_handler.setFormatter(console_formatter)

# File handler
file_handler = logging.FileHandler('project_cleanup.log', encoding='utf-8')
file_handler.setLevel(logging.DEBUG)
file_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
file_handler.setFormatter(file_formatter)

logger.addHandler(console_handler)
logger.addHandler(file_handler)

class ProjectDeepCleaner:
    """Ù…Ù†Ø¸Ù ÙˆÙ…ØµÙ„Ø­ Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"""
    
    def __init__(self, project_root: str, dry_run: bool = True):
        self.project_root = Path(project_root).resolve()
        self.dry_run = dry_run
        self.backup_dir = self.project_root / f"backup_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.stats = {
            'files_analyzed': 0,
            'issues_found': 0,
            'issues_fixed': 0,
            'files_moved': 0,
            'duplicates_removed': 0,
            'security_issues_fixed': 0,
            'code_quality_fixes': 0,
            'encoding_fixes': 0
        }
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        self.issues = {
            'misplaced_files': [],
            'duplicate_files': [],
            'security_issues': [],
            'code_quality_issues': [],
            'encoding_issues': [],
            'empty_files': [],
            'large_files': [],
            'unused_imports': [],
            'todo_fixme': []
        }
        
        # Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
        self.correct_structure = {
            'src': {
                'api': ['endpoints', 'middleware', 'routes'],
                'application': ['services', 'use_cases', 'dto'],
                'domain': ['entities', 'repositories', 'value_objects'],
                'infrastructure': ['database', 'external_services', 'memory'],
                'presentation': ['ui', 'cli', 'web'],
                'compliance': ['checkers', 'managers', 'reports', 'alerts'],
                'monitoring': ['performance', 'logging', 'metrics'],
                'security': ['auth', 'encryption', 'validation']
            },
            'tests': {
                'unit': ['api', 'domain', 'services'],
                'integration': ['api', 'database', 'external'],
                'e2e': ['scenarios', 'fixtures']
            },
            'scripts': ['deployment', 'maintenance', 'analysis'],
            'docs': ['api', 'architecture', 'guides'],
            'config': ['development', 'production', 'testing'],
            'docker': ['services', 'volumes'],
            'kubernetes': ['deployments', 'services', 'configmaps']
        }
        
        # Ù…Ù„ÙØ§Øª ÙŠØ¬Ø¨ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„ÙŠÙ‡Ø§
        self.protected_files = {
            'README.md', 'LICENSE', '.gitignore', 'requirements.txt',
            'setup.py', 'pyproject.toml', 'Dockerfile', 'docker-compose.yml',
            '.env.example', 'Makefile'
        }
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
        self.sensitive_patterns = [
            r'api[_-]?key\s*=\s*["\'][\w-]+["\']',
            r'password\s*=\s*["\'][^"\']+["\']',
            r'secret\s*=\s*["\'][^"\']+["\']',
            r'token\s*=\s*["\'][^"\']+["\']',
            r'sk-[a-zA-Z0-9]{48}',  # OpenAI keys
            r'hume_[a-zA-Z0-9]+',   # Hume keys
        ]

    def create_backup(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡"""
        if not self.dry_run:
            logger.info(f"Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ: {self.backup_dir}")
            shutil.copytree(self.project_root, self.backup_dir, 
                          ignore=shutil.ignore_patterns('*.pyc', '__pycache__', '.git', 'venv', 'env'))
            logger.info("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¨Ù†Ø¬Ø§Ø­")

    def analyze_project(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
        logger.info("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        for root, dirs, files in os.walk(self.project_root):
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
            dirs[:] = [d for d in dirs if d not in {'.git', '__pycache__', 'venv', 'env', '.idea', '.vscode'}]
            
            for file in files:
                if file.endswith('.pyc'):
                    continue
                    
                file_path = Path(root) / file
                self.stats['files_analyzed'] += 1
                
                # ÙØ­Øµ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ù„Ù
                self._check_file_location(file_path)
                
                # ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±
                self._check_duplicates(file_path)
                
                # ÙØ­Øµ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©
                if file_path.suffix in ['.py', '.yml', '.yaml', '.json', '.env']:
                    self._check_security_issues(file_path)
                
                # ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯
                if file_path.suffix == '.py':
                    self._check_code_quality(file_path)
                
                # ÙØ­Øµ ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù
                self._check_encoding(file_path)
                
                # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ© ÙˆØ§Ù„ÙƒØ¨ÙŠØ±Ø©
                self._check_file_size(file_path)

    def _check_file_location(self, file_path: Path):
        """ÙØ­Øµ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­"""
        relative_path = file_path.relative_to(self.project_root)
        parts = relative_path.parts
        
        # Ù‚ÙˆØ§Ø¹Ø¯ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„ØµØ­ÙŠØ­
        correct_locations = {
            'cleanup_analyzer.py': 'scripts/maintenance/',
            'find_more_duplicates.py': 'scripts/analysis/',
            'project_cleanup_analyzer.py': 'scripts/maintenance/',
            'imports_checker.py': 'scripts/analysis/',
            'compatibility_test.py': 'tests/integration/',
            'demo_runner.py': 'scripts/demo/'
        }
        
        # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
        if len(parts) == 1 and file_path.name.endswith('.py'):
            if file_path.name not in self.protected_files:
                if file_path.name in correct_locations:
                    self.issues['misplaced_files'].append({
                        'file': str(file_path),
                        'current_location': '.',
                        'correct_location': correct_locations[file_path.name]
                    })
                elif any(keyword in file_path.name.lower() for keyword in ['test', 'spec']):
                    self.issues['misplaced_files'].append({
                        'file': str(file_path),
                        'current_location': '.',
                        'correct_location': 'tests/'
                    })
                elif any(keyword in file_path.name.lower() for keyword in ['script', 'tool', 'util']):
                    self.issues['misplaced_files'].append({
                        'file': str(file_path),
                        'current_location': '.',
                        'correct_location': 'scripts/'
                    })
        
        # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯Ø§Øª backup
        if 'backup' in str(file_path).lower() or 'final_backup' in str(file_path):
            self.issues['misplaced_files'].append({
                'file': str(file_path),
                'current_location': str(file_path.parent),
                'correct_location': 'DELETE_BACKUP'
            })

    def _check_duplicates(self, file_path: Path):
        """ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
                file_hash = hashlib.md5(content).hexdigest()
                
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰ Ø¨Ù†ÙØ³ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            for root, _, files in os.walk(self.project_root):
                for other_file in files:
                    other_path = Path(root) / other_file
                    if other_path != file_path and other_path.exists():
                        try:
                            with open(other_path, 'rb') as f:
                                other_content = f.read()
                                other_hash = hashlib.md5(other_content).hexdigest()
                                
                            if file_hash == other_hash:
                                # ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠ Ù…Ù„Ù ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡
                                keep_file = self._determine_file_to_keep(file_path, other_path)
                                remove_file = other_path if keep_file == file_path else file_path
                                
                                duplicate_entry = {
                                    'file1': str(file_path),
                                    'file2': str(other_path),
                                    'keep': str(keep_file),
                                    'remove': str(remove_file),
                                    'size': len(content)
                                }
                                
                                # ØªØ¬Ù†Ø¨ Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹ÙƒÙˆØ³Ø©
                                if not any(d['file1'] == str(other_path) and d['file2'] == str(file_path) 
                                         for d in self.issues['duplicate_files']):
                                    self.issues['duplicate_files'].append(duplicate_entry)
                                    
                        except Exception:
                            pass
                            
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø± Ù„Ù„Ù…Ù„Ù {file_path}: {e}")

    def _determine_file_to_keep(self, file1: Path, file2: Path) -> Path:
        """ØªØ­Ø¯ÙŠØ¯ Ø£ÙŠ Ù…Ù„Ù ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª Ø®Ø§Ø±Ø¬ Ù…Ø¬Ù„Ø¯Ø§Øª backup
        if 'backup' in str(file1).lower():
            return file2
        if 'backup' in str(file2).lower():
            return file1
            
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª ÙÙŠ src/
        if str(file1).startswith(str(self.project_root / 'src')):
            return file1
        if str(file2).startswith(str(self.project_root / 'src')):
            return file2
            
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…Ù„Ù Ø§Ù„Ø£Ù‚Ø¯Ù… (Ø§Ù„Ø£ØµÙ„ÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ø¬Ø­)
        if file1.stat().st_mtime < file2.stat().st_mtime:
            return file1
        return file2

    def _check_security_issues(self, file_path: Path):
        """ÙØ­Øµ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
                
            issues_found = []
            
            # ÙØ­Øµ hardcoded secrets
            for pattern in self.sensitive_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    issues_found.append({
                        'type': 'hardcoded_secret',
                        'pattern': pattern,
                        'matches': matches[:3]  # Ø£ÙˆÙ„ 3 Ù†ØªØ§Ø¦Ø¬ ÙÙ‚Ø·
                    })
            
            # ÙØ­Øµ eval/exec ÙÙŠ Python
            if file_path.suffix == '.py':
                if 'eval(' in content or 'exec(' in content:
                    issues_found.append({
                        'type': 'dangerous_function',
                        'functions': []
                    })
                    if 'eval(' in content:
                        issues_found[-1]['functions'].append('eval')
                    if 'exec(' in content:
                        issues_found[-1]['functions'].append('exec')
                
                # ÙØ­Øµ broad exception handling
                broad_except_pattern = r'except\s*:|except\s+Exception\s*:'
                if re.search(broad_except_pattern, content):
                    issues_found.append({
                        'type': 'broad_exception',
                        'count': len(re.findall(broad_except_pattern, content))
                    })
            
            if issues_found:
                self.issues['security_issues'].append({
                    'file': str(file_path),
                    'issues': issues_found
                })
                
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø§Ù„Ø£Ù…Ø§Ù† Ù„Ù„Ù…Ù„Ù {file_path}: {e}")

    def _check_code_quality(self, file_path: Path):
        """ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
                content = ''.join(lines)
            
            issues_found = []
            
            # ÙØ­Øµ print statements
            print_count = len(re.findall(r'\bprint\s*\(', content))
            if print_count > 0 and 'test' not in str(file_path).lower():
                issues_found.append({
                    'type': 'print_statements',
                    'count': print_count
                })
            
            # ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
            if len(lines) > 500:
                issues_found.append({
                    'type': 'large_file',
                    'lines': len(lines)
                })
            
            # ÙØ­Øµ TODO/FIXME
            todos = re.findall(r'#\s*(TODO|FIXME).*', content)
            if todos:
                issues_found.append({
                    'type': 'todo_fixme',
                    'items': todos[:5]  # Ø£ÙˆÙ„ 5 ÙÙ‚Ø·
                })
            
            # ÙØ­Øµ missing docstrings
            try:
                tree = ast.parse(content)
                for node in ast.walk(tree):
                    if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                        if not ast.get_docstring(node):
                            issues_found.append({
                                'type': 'missing_docstring',
                                'name': node.name,
                                'line': node.lineno
                            })
            except:
                pass
            
            # ÙØ­Øµ unused imports
            import_pattern = r'^(?:from\s+(\S+)\s+)?import\s+(.+)$'
            imports = re.findall(import_pattern, content, re.MULTILINE)
            used_names = set(re.findall(r'\b(\w+)\b', content))
            
            unused = []
            for from_module, import_names in imports:
                names = [n.strip().split(' as ')[-1] for n in import_names.split(',')]
                for name in names:
                    if name not in used_names:
                        unused.append(name)
            
            if unused:
                issues_found.append({
                    'type': 'unused_imports',
                    'imports': unused[:5]  # Ø£ÙˆÙ„ 5 ÙÙ‚Ø·
                })
            
            if issues_found:
                self.issues['code_quality_issues'].append({
                    'file': str(file_path),
                    'issues': issues_found
                })
                
        except Exception as e:
            logger.debug(f"Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ Ù„Ù„Ù…Ù„Ù {file_path}: {e}")

    def _check_encoding(self, file_path: Path):
        """ÙØ­Øµ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ²"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                f.read()
        except UnicodeDecodeError:
            self.issues['encoding_issues'].append(str(file_path))
        except Exception:
            pass

    def _check_file_size(self, file_path: Path):
        """ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù"""
        try:
            size = file_path.stat().st_size
            if size == 0:
                self.issues['empty_files'].append(str(file_path))
            elif size > 100 * 1024:  # Ø£ÙƒØ¨Ø± Ù…Ù† 100KB
                self.issues['large_files'].append({
                    'file': str(file_path),
                    'size': size
                })
        except Exception:
            pass

    def fix_issues(self):
        """Ø¥ØµÙ„Ø§Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©"""
        if self.dry_run:
            logger.info("ğŸ” ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø© - Ù„Ù† ÙŠØªÙ… Ø¥Ø¬Ø±Ø§Ø¡ Ø£ÙŠ ØªØºÙŠÙŠØ±Ø§Øª")
            return
            
        logger.info("ğŸ”§ Ø¨Ø¯Ø¡ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„...")
        
        # 1. Ø¥ØµÙ„Ø§Ø­ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        self._fix_misplaced_files()
        
        # 2. Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        self._fix_duplicate_files()
        
        # 3. Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©
        self._fix_security_issues()
        
        # 4. Ø¥ØµÙ„Ø§Ø­ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯
        self._fix_code_quality()
        
        # 5. Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ²
        self._fix_encoding_issues()
        
        # 6. Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        self._fix_empty_files()

    def _fix_misplaced_files(self):
        """Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Ø£Ù…Ø§ÙƒÙ†Ù‡Ø§ Ø§Ù„ØµØ­ÙŠØ­Ø©"""
        logger.info("ğŸ“ Ø¥ØµÙ„Ø§Ø­ Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        for issue in self.issues['misplaced_files']:
            source = Path(issue['file'])
            
            if issue['correct_location'] == 'DELETE_BACKUP':
                # Ø­Ø°Ù Ù…Ø¬Ù„Ø¯Ø§Øª backup
                if source.exists():
                    if source.is_dir():
                        shutil.rmtree(source)
                    else:
                        source.unlink()
                    self.stats['files_moved'] += 1
                    logger.info(f"âœ… Ø­Ø°Ù backup: {source}")
            else:
                # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù Ù„Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­
                target_dir = self.project_root / issue['correct_location']
                target_dir.mkdir(parents=True, exist_ok=True)
                
                target = target_dir / source.name
                if source.exists() and not target.exists():
                    shutil.move(str(source), str(target))
                    self.stats['files_moved'] += 1
                    logger.info(f"âœ… Ù†Ù‚Ù„: {source} -> {target}")

    def _fix_duplicate_files(self):
        """Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        logger.info("ğŸ”„ Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
        
        removed = set()
        for issue in self.issues['duplicate_files']:
            remove_file = Path(issue['remove'])
            if remove_file.exists() and str(remove_file) not in removed:
                remove_file.unlink()
                removed.add(str(remove_file))
                self.stats['duplicates_removed'] += 1
                logger.info(f"âœ… Ø­Ø°Ù Ù…ÙƒØ±Ø±: {remove_file}")

    def _fix_security_issues(self):
        """Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©"""
        logger.info("ğŸ” Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©...")
        
        for issue in self.issues['security_issues']:
            file_path = Path(issue['file'])
            if not file_path.exists():
                continue
                
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                original_content = content
                
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ hardcoded secrets
                for sec_issue in issue['issues']:
                    if sec_issue['type'] == 'hardcoded_secret':
                        for match in sec_issue['matches']:
                            # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø¨Ù…ØªØºÙŠØ± Ø¨ÙŠØ¦Ø©
                            env_var = self._extract_env_var_name(match)
                            replacement = f'os.getenv("{env_var}")'
                            content = content.replace(match, replacement)
                    
                    elif sec_issue['type'] == 'dangerous_function':
                        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ eval/exec Ø¨Ø­Ù„ÙˆÙ„ Ø¢Ù…Ù†Ø©
                        if 'eval' in sec_issue['functions']:
                            content = re.sub(r'\beval\s*\(', 'ast.literal_eval(', content)
                        if 'exec' in sec_issue['functions']:
                            # Ø¥Ø¶Ø§ÙØ© ØªØ­Ø°ÙŠØ± Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† exec
                            content = re.sub(r'\bexec\s*\(([^)]+)\)', 
                                          r'# SECURITY WARNING: exec removed\n# Original: exec(\1)', 
                                          content)
                    
                    elif sec_issue['type'] == 'broad_exception':
                        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ broad exceptions
                        content = re.sub(r'except\s*:', 'except Exception as e:', content)
                        content = re.sub(r'except\s+Exception\s*:', 'except Exception as e:', content)
                
                # Ø¥Ø¶Ø§ÙØ© imports Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                if 'os.getenv' in content and 'import os' not in content:
                    content = 'import os\n' + content
                if 'ast.literal_eval' in content and 'import ast' not in content:
                    content = 'import ast\n' + content
                
                if content != original_content:
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    self.stats['security_issues_fixed'] += 1
                    logger.info(f"âœ… Ø¥ØµÙ„Ø§Ø­ Ø£Ù…Ù†ÙŠ: {file_path}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ØµÙ„Ø§Ø­ {file_path}: {e}")

    def _extract_env_var_name(self, secret_string: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ø³Ù… Ù…ØªØºÙŠØ± Ø§Ù„Ø¨ÙŠØ¦Ø© Ù…Ù† Ø§Ù„Ø³Ù„Ø³Ù„Ø©"""
        if 'api_key' in secret_string.lower():
            return 'API_KEY'
        elif 'password' in secret_string.lower():
            return 'PASSWORD'
        elif 'secret' in secret_string.lower():
            return 'SECRET_KEY'
        elif 'token' in secret_string.lower():
            return 'AUTH_TOKEN'
        elif 'sk-' in secret_string:
            return 'OPENAI_API_KEY'
        elif 'hume_' in secret_string:
            return 'HUME_API_KEY'
        else:
            return 'SECRET_VALUE'

    def _fix_code_quality(self):
        """Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯"""
        logger.info("ğŸ“ Ø¥ØµÙ„Ø§Ø­ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯...")
        
        for issue in self.issues['code_quality_issues']:
            file_path = Path(issue['file'])
            if not file_path.exists():
                continue
                
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                original_content = content
                
                for quality_issue in issue['issues']:
                    if quality_issue['type'] == 'print_statements':
                        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ print Ø¨Ù€ logging
                        content = self._replace_print_with_logging(content)
                    
                    elif quality_issue['type'] == 'unused_imports':
                        # Ø­Ø°Ù imports ØºÙŠØ± Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…Ø©
                        for unused in quality_issue['imports']:
                            # Ø­Ø°Ù import ÙƒØ§Ù…Ù„ Ø¥Ø°Ø§ ÙƒØ§Ù† ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…
                            content = re.sub(f'\\bimport\\s+{unused}\\b.*\\n', '', content)
                            content = re.sub(f'\\bfrom\\s+\\S+\\s+import\\s+.*{unused}.*\\n', '', content)
                
                if content != original_content:
                    # Ø¥Ø¶Ø§ÙØ© import logging Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                    if 'logger.' in content and 'import logging' not in content:
                        content = 'import logging\n\nlogger = logging.getLogger(__name__)\n' + content
                    
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    self.stats['code_quality_fixes'] += 1
                    logger.info(f"âœ… ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø©: {file_path}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø³ÙŠÙ† {file_path}: {e}")

    def _replace_print_with_logging(self, content: str) -> str:
        """Ø§Ø³ØªØ¨Ø¯Ø§Ù„ print statements Ø¨Ù€ logging"""
        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ print() Ø¨Ù€ logger.info()
        content = re.sub(r'\bprint\s*\(\s*f["\']([^"\']+)["\']\s*\)', r'logger.info("\1")', content)
        content = re.sub(r'\bprint\s*\(\s*["\']([^"\']+)["\']\s*\)', r'logger.info("\1")', content)
        content = re.sub(r'\bprint\s*\(([^)]+)\)', r'logger.info(\1)', content)
        
        return content

    def _fix_encoding_issues(self):
        """Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ²"""
        logger.info("ğŸ”¤ Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ²...")
        
        for file_path in self.issues['encoding_issues']:
            file_path = Path(file_path)
            if not file_path.exists():
                continue
                
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ø¨ØªØ±Ù…ÙŠØ²Ø§Øª Ù…Ø®ØªÙ„ÙØ©
                content = None
                for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:
                    try:
                        with open(file_path, 'r', encoding=encoding) as f:
                            content = f.read()
                        break
                    except UnicodeDecodeError:
                        continue
                
                if content:
                    # Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ÙƒØªØ§Ø¨Ø© Ø¨ØªØ±Ù…ÙŠØ² UTF-8
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(content)
                    self.stats['encoding_fixes'] += 1
                    logger.info(f"âœ… Ø¥ØµÙ„Ø§Ø­ ØªØ±Ù…ÙŠØ²: {file_path}")
                    
            except Exception as e:
                logger.error(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø¥ØµÙ„Ø§Ø­ ØªØ±Ù…ÙŠØ² {file_path}: {e}")

    def _fix_empty_files(self):
        """Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ© ØºÙŠØ± Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©"""
        logger.info("ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©...")
        
        for file_path in self.issues['empty_files']:
            file_path = Path(file_path)
            if not file_path.exists():
                continue
                
            # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ __init__.py Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©
            if file_path.name == '__init__.py':
                # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù…Ù„ÙØ§Øª Python Ø£Ø®Ø±Ù‰ ÙÙŠ Ù†ÙØ³ Ø§Ù„Ù…Ø¬Ù„Ø¯
                parent_dir = file_path.parent
                other_py_files = list(parent_dir.glob('*.py'))
                if len(other_py_files) > 1:  # ÙŠÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰
                    continue  # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ __init__.py
            
            # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙØ§Ø±Øº
            file_path.unlink()
            self.stats['issues_fixed'] += 1
            logger.info(f"âœ… Ø­Ø°Ù Ù…Ù„Ù ÙØ§Ø±Øº: {file_path}")

    def generate_report(self) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ø¨ØªÙ†Ø³ÙŠÙ‚ Markdown"""
        report = []
        timestamp = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report.append("# ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ ÙˆØ¥ØµÙ„Ø§Ø­ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear")
        report.append(f"\n**Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª**: {timestamp}")
        report.append(f"\n**Ø§Ù„ÙˆØ¶Ø¹**: {'Ù…Ø¹Ø§ÙŠÙ†Ø© ÙÙ‚Ø·' if self.dry_run else 'ØªÙ†ÙÙŠØ° ÙØ¹Ù„ÙŠ'}")
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
        report.append("\n## ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©")
        report.append(f"- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ­ÙˆØµØ©**: {self.stats['files_analyzed']:,}")
        report.append(f"- **Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©**: {self._count_total_issues():,}")
        report.append(f"- **Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙØµÙ„Ø­Ø©**: {self.stats['issues_fixed']:,}")
        
        # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª
        if not self.dry_run:
            report.append("\n## âœ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª Ø§Ù„Ù…Ù†ÙØ°Ø©")
            report.append(f"- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø©**: {self.stats['files_moved']}")
            report.append(f"- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©**: {self.stats['duplicates_removed']}")
            report.append(f"- **Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ© Ø§Ù„Ù…ÙØµÙ„Ø­Ø©**: {self.stats['security_issues_fixed']}")
            report.append(f"- **ØªØ­Ø³ÙŠÙ†Ø§Øª Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯**: {self.stats['code_quality_fixes']}")
            report.append(f"- **Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…ÙØµÙ„Ø­Ø©**: {self.stats['encoding_fixes']}")
        
        # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        report.append("\n## ğŸ” ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©")
        
        # 1. Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ø®Ø§Ø·Ø¦Ø©
        if self.issues['misplaced_files']:
            report.append(f"\n### ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ø®Ø§Ø·Ø¦Ø© ({len(self.issues['misplaced_files'])})")
            report.append("\n| Ø§Ù„Ù…Ù„Ù | Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ | Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„ØµØ­ÙŠØ­ | Ø§Ù„Ø­Ø§Ù„Ø© |")
            report.append("|-------|---------------|---------------|--------|")
            
            for issue in self.issues['misplaced_files'][:20]:  # Ø£ÙˆÙ„ 20 ÙÙ‚Ø·
                status = "âœ… ØªÙ… Ø§Ù„Ù†Ù‚Ù„" if not self.dry_run else "â³ ÙÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±"
                current = issue['current_location']
                correct = issue['correct_location']
                if correct == 'DELETE_BACKUP':
                    correct = "ğŸ—‘ï¸ Ø­Ø°Ù (backup)"
                report.append(f"| {Path(issue['file']).name} | {current} | {correct} | {status} |")
            
            if len(self.issues['misplaced_files']) > 20:
                report.append(f"\n*... Ùˆ {len(self.issues['misplaced_files']) - 20} Ù…Ù„Ù Ø¢Ø®Ø±*")
        
        # 2. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        if self.issues['duplicate_files']:
            report.append(f"\n### ğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ({len(self.issues['duplicate_files'])})")
            report.append("\n| Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ | Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…ÙƒØ±Ø± | Ø§Ù„Ø­Ø¬Ù… | Ø§Ù„Ø­Ø§Ù„Ø© |")
            report.append("|---------------|--------------|--------|--------|")
            
            for issue in self.issues['duplicate_files'][:15]:
                status = "âœ… ØªÙ… Ø§Ù„Ø­Ø°Ù" if not self.dry_run else "â³ ÙÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±"
                size = self._format_size(issue['size'])
                keep_name = Path(issue['keep']).name
                remove_name = Path(issue['remove']).name
                report.append(f"| {keep_name} | {remove_name} | {size} | {status} |")
            
            if len(self.issues['duplicate_files']) > 15:
                report.append(f"\n*... Ùˆ {len(self.issues['duplicate_files']) - 15} Ù…Ù„Ù Ù…ÙƒØ±Ø± Ø¢Ø®Ø±*")
        
        # 3. Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©
        if self.issues['security_issues']:
            report.append(f"\n### ğŸ” Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ© ({len(self.issues['security_issues'])})")
            
            security_summary = defaultdict(int)
            for issue in self.issues['security_issues']:
                for sec_issue in issue['issues']:
                    security_summary[sec_issue['type']] += 1
            
            report.append("\n| Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© | Ø§Ù„Ø¹Ø¯Ø¯ | Ø§Ù„Ø­Ø§Ù„Ø© |")
            report.append("|-------------|-------|--------|")
            
            type_names = {
                'hardcoded_secret': 'ğŸ”‘ Ù…ÙØ§ØªÙŠØ­ Ù…ÙƒØ´ÙˆÙØ©',
                'dangerous_function': 'âš ï¸ Ø¯ÙˆØ§Ù„ Ø®Ø·Ø±Ø© (eval/exec)',
                'broad_exception': 'ğŸ•³ï¸ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ø¶Ø¹ÙŠÙØ©'
            }
            
            for issue_type, count in security_summary.items():
                status = "âœ… ØªÙ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­" if not self.dry_run else "â³ ÙÙŠ Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø±"
                report.append(f"| {type_names.get(issue_type, issue_type)} | {count} | {status} |")
            
            # Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©
            report.append("\n**Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©:**")
            for issue in self.issues['security_issues'][:5]:
                report.append(f"- `{Path(issue['file']).name}`")
        
        # 4. Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯
        if self.issues['code_quality_issues']:
            report.append(f"\n### ğŸ“ Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ ({len(self.issues['code_quality_issues'])})")
            
            quality_summary = defaultdict(int)
            for issue in self.issues['code_quality_issues']:
                for q_issue in issue['issues']:
                    quality_summary[q_issue['type']] += 1
            
            report.append("\n| Ù†ÙˆØ¹ Ø§Ù„Ù…Ø´ÙƒÙ„Ø© | Ø§Ù„Ø¹Ø¯Ø¯ | Ø§Ù„Ø­Ø§Ù„Ø© |")
            report.append("|-------------|-------|--------|")
            
            type_names = {
                'print_statements': 'ğŸ–¨ï¸ Ø§Ø³ØªØ®Ø¯Ø§Ù… print',
                'large_file': 'ğŸ“ Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹',
                'todo_fixme': 'ğŸ“Œ TODO/FIXME',
                'missing_docstring': 'ğŸ“„ Ù†Ù‚Øµ Ø§Ù„ØªÙˆØ«ÙŠÙ‚',
                'unused_imports': 'ğŸ“¦ imports ØºÙŠØ± Ù…Ø³ØªØ®Ø¯Ù…Ø©'
            }
            
            for issue_type, count in quality_summary.items():
                status = "âœ… ØªÙ… Ø§Ù„Ø¥ØµÙ„Ø§Ø­" if not self.dry_run and issue_type in ['print_statements', 'unused_imports'] else "â³ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø©"
                report.append(f"| {type_names.get(issue_type, issue_type)} | {count} | {status} |")
        
        # 5. Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ²
        if self.issues['encoding_issues']:
            report.append(f"\n### ğŸ”¤ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªØ±Ù…ÙŠØ² ({len(self.issues['encoding_issues'])})")
            report.append("\n**Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©:**")
            for file_path in self.issues['encoding_issues'][:10]:
                status = "âœ…" if not self.dry_run else "â³"
                report.append(f"- {status} `{Path(file_path).name}`")
            
            if len(self.issues['encoding_issues']) > 10:
                report.append(f"\n*... Ùˆ {len(self.issues['encoding_issues']) - 10} Ù…Ù„Ù Ø¢Ø®Ø±*")
        
        # 6. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        if self.issues['empty_files']:
            report.append(f"\n### ğŸ—‘ï¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ© ({len(self.issues['empty_files'])})")
            empty_by_type = defaultdict(list)
            for file_path in self.issues['empty_files']:
                ext = Path(file_path).suffix or 'no_extension'
                empty_by_type[ext].append(file_path)
            
            for ext, files in empty_by_type.items():
                report.append(f"\n**{ext} ({len(files)} Ù…Ù„Ù):**")
                for file_path in files[:5]:
                    status = "âœ… Ù…Ø­Ø°ÙˆÙ" if not self.dry_run else "â³"
                    report.append(f"- {status} `{Path(file_path).name}`")
                if len(files) > 5:
                    report.append(f"  *... Ùˆ {len(files) - 5} Ù…Ù„Ù Ø¢Ø®Ø±*")
        
        # 7. Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        if self.issues['large_files']:
            report.append(f"\n### ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ ({len(self.issues['large_files'])})")
            report.append("\n| Ø§Ù„Ù…Ù„Ù | Ø§Ù„Ø­Ø¬Ù… | Ø§Ù„Ø³Ø·ÙˆØ± | Ø§Ù„ØªÙˆØµÙŠØ© |")
            report.append("|-------|--------|---------|----------|")
            
            sorted_large = sorted(self.issues['large_files'], key=lambda x: x['size'], reverse=True)
            for issue in sorted_large[:10]:
                file_name = Path(issue['file']).name
                size = self._format_size(issue['size'])
                lines = "N/A"
                if Path(issue['file']).suffix == '.py':
                    try:
                        with open(issue['file'], 'r', encoding='utf-8', errors='ignore') as f:
                            lines = str(len(f.readlines()))
                    except:
                        pass
                report.append(f"| {file_name} | {size} | {lines} | ÙŠØ­ØªØ§Ø¬ ØªÙ‚Ø³ÙŠÙ… |")
        
        # Ø§Ù„ØªÙˆØµÙŠØ§Øª
        report.append("\n## ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª")
        report.append("\n### Ù„Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ù†Ø¸Ø§ÙØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:")
        report.append("1. **Ø§Ø³ØªØ®Ø¯Ø§Ù… pre-commit hooks** Ù„ÙØ­Øµ Ø§Ù„ÙƒÙˆØ¯ Ù‚Ø¨Ù„ Ø§Ù„Ù€ commit")
        report.append("2. **ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§ÙŠÙŠØ± ÙƒØªØ§Ø¨Ø© ÙƒÙˆØ¯ ÙˆØ§Ø¶Ø­Ø©** (PEP 8 for Python)")
        report.append("3. **Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ø¥Ù„Ø²Ø§Ù…ÙŠØ©** Ù„ÙƒÙ„ Pull Request")
        report.append("4. **Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø¯ÙˆØ§Øª ØªØ­Ù„ÙŠÙ„ ØªÙ„Ù‚Ø§Ø¦ÙŠØ©** (pylint, flake8, black)")
        report.append("5. **ØªÙˆØ«ÙŠÙ‚ ÙˆØ§Ø¶Ø­** Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ§Ù„Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù…ØªØ¨Ø¹Ø©")
        
        report.append("\n### Ù„Ù„Ø£Ù…Ø§Ù†:")
        report.append("1. **Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø©** Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ù„Ø£Ø³Ø±Ø§Ø±")
        report.append("2. **ØªØ¬Ù†Ø¨ eval/exec** Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹")
        report.append("3. **Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£Ø®Ø·Ø§Ø¡ Ù…Ø­Ø¯Ø¯Ø©** Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† catch-all")
        report.append("4. **Ù…Ø±Ø§Ø¬Ø¹Ø© Ø£Ù…Ù†ÙŠØ© Ø¯ÙˆØ±ÙŠØ©** Ù„Ù„ÙƒÙˆØ¯ ÙˆØ§Ù„ØªØ¨Ø¹ÙŠØ§Øª")
        
        # Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©
        if self.dry_run:
            report.append("\n## ğŸš€ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©")
            report.append("\n**Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§ØªØŒ Ø´ØºÙ„ Ø§Ù„Ø£Ù…Ø± Ø§Ù„ØªØ§Ù„ÙŠ:**")
            report.append("```bash")
            report.append("python project_deep_cleaner.py --execute")
            report.append("```")
            report.append("\nâš ï¸ **ØªØ­Ø°ÙŠØ±**: ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„ØªÙ†ÙÙŠØ°!")
        else:
            report.append("\n## âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡")
            report.append(f"\n**Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ**: `{self.backup_dir}`")
            report.append("\n**ÙŠÙÙ†ØµØ­ Ø¨Ù€**:")
            report.append("1. Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ git: `git diff`")
            report.append("2. ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª: `pytest`")
            report.append("3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¹Ù…Ù„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚")
        
        return '\n'.join(report)

    def _count_total_issues(self) -> int:
        """Ø­Ø³Ø§Ø¨ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©"""
        total = 0
        total += len(self.issues['misplaced_files'])
        total += len(self.issues['duplicate_files'])
        total += len(self.issues['security_issues'])
        total += len(self.issues['code_quality_issues'])
        total += len(self.issues['encoding_issues'])
        total += len(self.issues['empty_files'])
        total += len(self.issues['large_files'])
        return total

    def _format_size(self, size_bytes: int) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if size_bytes < 1024.0:
                return f"{size_bytes:.1f} {unit}"
            size_bytes /= 1024.0
        return f"{size_bytes:.1f} TB"

    def save_report(self, report: str):
        """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ù…Ù„Ù"""
        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = self.project_root / f"cleanup_report_{timestamp}.md"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info(f"ğŸ“„ ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {report_file}")

    def run(self):
        """ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ø¥ØµÙ„Ø§Ø­ Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ø¥ØµÙ„Ø§Ø­...")
        logger.info(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {self.project_root}")
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        if not self.dry_run:
            self.create_backup()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
        self.analyze_project()
        
        # Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
        self.fix_issues()
        
        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = self.generate_report()
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        print("\n" + "="*80)
        print(report)
        print("="*80 + "\n")
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        self.save_report(report)
        
        logger.info("âœ… Ø§Ù†ØªÙ‡Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„Ø¥ØµÙ„Ø§Ø­!")


def main():
    """Ù†Ù‚Ø·Ø© Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    parser = argparse.ArgumentParser(
        description='Ù…Ù†Ø¸Ù ÙˆÙ…ØµÙ„Ø­ Ø¹Ù…ÙŠÙ‚ Ù„Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
  python project_deep_cleaner.py --dry-run     # Ù…Ø¹Ø§ÙŠÙ†Ø© ÙÙ‚Ø· (Ø§ÙØªØ±Ø§Ø¶ÙŠ)
  python project_deep_cleaner.py --execute     # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª
  python project_deep_cleaner.py --path /path/to/project --execute
        """
    )
    
    parser.add_argument(
        '--path',
        type=str,
        default='.',
        help='Ù…Ø³Ø§Ø± Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø­Ø§Ù„ÙŠ)'
    )
    
    parser.add_argument(
        '--dry-run',
        action='store_true',
        default=True,
        help='Ù…Ø¹Ø§ÙŠÙ†Ø© ÙÙ‚Ø· Ø¨Ø¯ÙˆÙ† Ø¥Ø¬Ø±Ø§Ø¡ ØªØºÙŠÙŠØ±Ø§Øª (Ø§ÙØªØ±Ø§Ø¶ÙŠ)'
    )
    
    parser.add_argument(
        '--execute',
        action='store_true',
        help='ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥ØµÙ„Ø§Ø­Ø§Øª ÙØ¹Ù„ÙŠØ§Ù‹'
    )
    
    args = parser.parse_args()
    
    # ØªØ­Ø¯ÙŠØ¯ ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„
    dry_run = not args.execute
    
    # Ø¥Ù†Ø´Ø§Ø¡ ÙˆØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù†Ø¸Ù
    cleaner = ProjectDeepCleaner(args.path, dry_run=dry_run)
    cleaner.run()


if __name__ == "__main__":
    main()