from typing import Dict, List, Any, Optional

import logging

logger = logging.getLogger(__name__)

"""
ğŸ¤ HUME AI Speech Emotion Analyzer for AI Teddy Bear
Real-time emotion analysis directly from children's voice without text
"""
import structlog
logger = structlog.get_logger(__name__)


import asyncio
import aiohttp
import json
import base64
import tempfile
import os
from typing import Dict, List, Any, Optional, Tuple
from datetime import datetime
from dataclasses import dataclass, field
import numpy as np
from pathlib import Path

# HUME AI SDK (install with: pip install hume)
try:
    from hume import HumeStreamClient
    from hume.models.config import ProsodyConfig
    from hume.core.utilities import encode_file
    HUME_AVAILABLE = True
except ImportError:
    HUME_AVAILABLE = False
    logger.warning("âš ï¸ HUME AI not installed. Install with: pip install hume")


@dataclass
class ChildVoiceEmotion:
    """
    Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø·ÙÙ„ Ù…Ù† Ø§Ù„ØµÙˆØª Ù…Ø¨Ø§Ø´Ø±Ø©
    """
    # Primary emotions detected by HUME
    joy: float = 0.0
    sadness: float = 0.0
    anger: float = 0.0
    fear: float = 0.0
    excitement: float = 0.0
    calmness: float = 0.0
    surprise: float = 0.0
    
    # Child-specific emotions
    curiosity: float = 0.0
    frustration: float = 0.0
    shyness: float = 0.0
    playfulness: float = 0.0
    tiredness: float = 0.0
    
    # Voice characteristics
    energy_level: float = 0.0  # High = excited, Low = tired
    speech_rate: float = 0.0   # Fast = excited/nervous, Slow = calm/sad
    pitch_variation: float = 0.0  # High = emotional, Low = monotone
    voice_quality: str = "clear"  # clear, trembling, whisper, etc.
    
    # Analysis metadata
    confidence: float = 0.0
    dominant_emotion: str = "neutral"
    emotional_intensity: float = 0.0
    timestamp: str = ""
    
    # Child development insights
    developmental_indicators: List[str] = field(default_factory=list)
    attention_level: float = 0.0
    communication_clarity: float = 0.0
    
    def __post_init__(self):
        if self.developmental_indicators is None:
            self.developmental_indicators = []


class HumeSpeechEmotionAnalyzer:
    """
    Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„ØµÙˆØªÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… HUME AI
    Ù…ØµÙ…Ù… Ø®ØµÙŠØµØ§Ù‹ Ù„ØªØ­Ù„ÙŠÙ„ Ø£ØµÙˆØ§Øª Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø¨Ø¯Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
    """
    
    def __init__(self, api_key: Optional[str] = None):
        self.api_key = api_key or os.getenv("HUME_API_KEY")
        self.base_url = "https://api.hume.ai/v0"
        
        # Child-specific emotion mapping
        self.child_emotion_map = {
            # HUME emotions â†’ Child emotions
            "Joy": "joy",
            "Sadness": "sadness", 
            "Anger": "anger",
            "Fear": "fear",
            "Surprise": "surprise",
            "Excitement": "excitement",
            "Calmness": "calmness",
            "Curiosity": "curiosity",
            "Frustration": "frustration",
            "Admiration": "excitement",
            "Amusement": "playfulness",
            "Anxiety": "fear",
            "Boredom": "tiredness",
            "Confusion": "curiosity",
            "Contentment": "calmness",
            "Embarrassment": "shyness",
            "Enthusiasm": "excitement",
            "Interest": "curiosity",
            "Pride": "joy",
            "Shame": "shyness",
            "Tiredness": "tiredness"
        }
        
        # Age-specific analysis parameters
        self.age_parameters = {
            "3-4": {"sensitivity": 0.8, "attention_threshold": 0.3},
            "5-6": {"sensitivity": 0.7, "attention_threshold": 0.4}, 
            "7-8": {"sensitivity": 0.6, "attention_threshold": 0.5},
            "9-10": {"sensitivity": 0.5, "attention_threshold": 0.6},
            "11+": {"sensitivity": 0.4, "attention_threshold": 0.7}
        }
        
        logger.info(f"ğŸ¤ HUME Speech Emotion Analyzer initialized")
        logger.error(f"   API Status: {'âœ… Ready' if self.api_key else 'âŒ No API Key'}")
    
    async def analyze_child_voice(
        self, 
        audio_data: bytes, 
        child_age: int = 6,
        child_name: str = "Ø·ÙÙ„",
        context: Dict[str, Any] = None
    ) -> ChildVoiceEmotion:
        """
        ØªØ­Ù„ÙŠÙ„ ØµÙˆØª Ø§Ù„Ø·ÙÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¨Ø¯ÙˆÙ† ØªØ­ÙˆÙŠÙ„ Ù„Ù†Øµ
        
        Args:
            audio_data: Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµÙˆØªÙŠØ© (WAV, MP3, etc.)
            child_age: Ø¹Ù…Ø± Ø§Ù„Ø·ÙÙ„ Ù„ØªØ®ØµÙŠØµ Ø§Ù„ØªØ­Ù„ÙŠÙ„
            child_name: Ø§Ø³Ù… Ø§Ù„Ø·ÙÙ„
            context: Ø³ÙŠØ§Ù‚ Ø¥Ø¶Ø§ÙÙŠ (ÙˆÙ‚Øª Ø§Ù„ÙŠÙˆÙ…ØŒ Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø£Ø®ÙŠØ±ØŒ Ø¥Ù„Ø®)
            
        Returns:
            ChildVoiceEmotion: ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø·ÙÙ„
        """
        try:
            if not self.api_key:
                logger.warning("âš ï¸ No HUME API key provided, using fallback analysis")
                return self._create_fallback_analysis()
            
            # Ø­ÙØ¸ Ø§Ù„ØµÙˆØª ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª
            temp_audio_path = await self._save_temp_audio(audio_data)
            
            try:
                # ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… HUME AI
                hume_results = await self._analyze_with_hume(temp_audio_path)
                
                # ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ HUME Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…Ø®ØµØµ Ù„Ù„Ø·ÙÙ„
                child_emotion = await self._convert_to_child_emotion(
                    hume_results, child_age, child_name, context
                )
                
                # Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ ØªØ·ÙˆÙŠØ±ÙŠ Ø¥Ø¶Ø§ÙÙŠ
                child_emotion = await self._add_developmental_analysis(
                    child_emotion, audio_data, child_age
                )
                
                return child_emotion
                
            finally:
                # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø¤Ù‚Øª
                if os.path.exists(temp_audio_path):
                    os.remove(temp_audio_path)
                    
        except Exception as e:
    logger.error(f"Error: {e}")f"âŒ HUME Analysis Error: {e}")
            return self._create_fallback_analysis()
    
    async def _analyze_with_hume(self, audio_path: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… HUME AI API"""
        
        if HUME_AVAILABLE:
            try:
                # Ø§Ø³ØªØ®Ø¯Ø§Ù… HUME SDK Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
                client = HumeStreamClient(api_key=self.api_key)
                config = ProsodyConfig()
                
                # ØªØ±Ù…ÙŠØ² Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
                encoded_audio = encode_file(audio_path)
                
                # Ø¥Ø±Ø³Ø§Ù„ Ù„Ù„ØªØ­Ù„ÙŠÙ„
                async with client.connect([config]) as socket:
                    result = await socket.send_bytes(encoded_audio)
                    
                return result
                
            except Exception as e:
    logger.error(f"Error: {e}")f"âš ï¸ HUME SDK error, trying direct API: {sdk_error}")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… API Ù…Ø¨Ø§Ø´Ø± ÙƒÙ€ fallback
        return await self._analyze_with_direct_api(audio_path)
    
    async def _analyze_with_direct_api(self, audio_path: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… HUME REST API"""
        
        headers = {
            "X-Hume-Api-Key": self.api_key,
        }
        
        # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
        with open(audio_path, 'rb') as audio_file:
            audio_data = audio_file.read()
        
        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¥Ø±Ø³Ø§Ù„
        files = {
            'file': ('audio.wav', audio_data, 'audio/wav')
        }
        
        data = {
            'models': json.dumps({
                "prosody": {
                    "granularity": "utterance",
                    "identify_speakers": False
                }
            })
        }
        
        async with aiohttp.ClientSession() as session:
            async with session.post(
                f"{self.base_url}/batch/jobs",
                headers=headers,
                data=data,
                data=files
            ) as response:
                if response.status == 200:
                    job_data = await response.json()
                    job_id = job_data['job_id']
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                    return await self._wait_for_results(session, job_id, headers)
                else:
                    error_text = await response.text()
                    raise Exception(f"HUME API error {response.status}: {error_text}")
    
    async def _wait_for_results(
        self, 
        session: aiohttp.ClientSession, 
        job_id: str, 
        headers: Dict[str, str]
    ) -> Dict[str, Any]:
        """Ø§Ù†ØªØ¸Ø§Ø± Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù† HUME"""
        
        max_attempts = 30  # 30 seconds max
        attempt = 0
        
        while attempt < max_attempts:
            async with session.get(
                f"{self.base_url}/batch/jobs/{job_id}",
                headers=headers
            ) as response:
                if response.status == 200:
                    job_status = await response.json()
                    
                    if job_status['state'] == 'COMPLETED':
                        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
                        return await self._download_results(session, job_id, headers)
                    elif job_status['state'] == 'FAILED':
                        raise Exception("HUME job failed")
                    
                    # Ø§Ù†ØªØ¸Ø§Ø± Ø«Ø§Ù†ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
                    await asyncio.sleep(1)
                    attempt += 1
                else:
                    raise Exception(f"Status check failed: {response.status}")
        
        raise Exception("HUME analysis timeout")
    
    async def _download_results(
        self, 
        session: aiohttp.ClientSession, 
        job_id: str, 
        headers: Dict[str, str]
    ) -> Dict[str, Any]:
        """ØªØ­Ù…ÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        
        async with session.get(
            f"{self.base_url}/batch/jobs/{job_id}/predictions",
            headers=headers
        ) as response:
            if response.status == 200:
                return await response.json()
            else:
                raise Exception(f"Results download failed: {response.status}")
    
    async def _convert_to_child_emotion(
        self, 
        hume_results: Dict[str, Any],
        child_age: int,
        child_name: str,
        context: Dict[str, Any]
    ) -> ChildVoiceEmotion:
        """ØªØ­ÙˆÙŠÙ„ Ù†ØªØ§Ø¦Ø¬ HUME Ø¥Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ Ù…Ø®ØµØµ Ù„Ù„Ø·ÙÙ„"""
        
        child_emotion = ChildVoiceEmotion()
        child_emotion.timestamp = datetime.now().isoformat()
        
        try:
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ù†ØªØ§Ø¦Ø¬ HUME
            if 'predictions' in hume_results:
                predictions = hume_results['predictions']
                
                if predictions and len(predictions) > 0:
                    # Ø£Ø®Ø° Ø£ÙˆÙ„ ØªØ³Ø¬ÙŠÙ„ ØµÙˆØªÙŠ
                    first_prediction = predictions[0]
                    
                    if 'models' in first_prediction and 'prosody' in first_prediction['models']:
                        prosody_data = first_prediction['models']['prosody']
                        
                        if 'grouped_predictions' in prosody_data:
                            grouped = prosody_data['grouped_predictions']
                            
                            if grouped and len(grouped) > 0:
                                # Ø£Ø®Ø° Ø£ÙˆÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© ØªÙˆÙ‚Ø¹Ø§Øª
                                emotions = grouped[0]['predictions']
                                
                                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
                                for emotion_data in emotions:
                                    emotion_name = emotion_data['name']
                                    emotion_score = emotion_data['score']
                                    
                                    # ØªØ·Ø¨ÙŠÙ‚ Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø®ØµØµØ© Ù„Ù„Ø£Ø·ÙØ§Ù„
                                    if emotion_name in self.child_emotion_map:
                                        child_field = self.child_emotion_map[emotion_name]
                                        
                                        # ØªØ­Ø¯ÙŠØ« Ù‚ÙŠÙ…Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± ÙÙŠ Ø§Ù„ÙƒØ§Ø¦Ù†
                                        if hasattr(child_emotion, child_field):
                                            current_value = getattr(child_emotion, child_field)
                                            setattr(child_emotion, child_field, 
                                                   max(current_value, emotion_score))
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
            child_emotion.dominant_emotion = self._get_dominant_emotion(child_emotion)
            child_emotion.confidence = self._calculate_confidence(child_emotion)
            child_emotion.emotional_intensity = self._calculate_intensity(child_emotion)
            
            # ØªØ®ØµÙŠØµ Ù„Ù„Ø¹Ù…Ø±
            child_emotion = self._adjust_for_age(child_emotion, child_age)
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø³ÙŠØ§Ù‚
            if context:
                child_emotion = self._apply_context(child_emotion, context)
            
        except Exception as e:
    logger.error(f"Error: {e}")f"âš ï¸ Error converting HUME results: {e}")
            # Ø¥Ø±Ø¬Ø§Ø¹ ØªØ­Ù„ÙŠÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠ
            child_emotion.dominant_emotion = "curious"
            child_emotion.curiosity = 0.6
            child_emotion.confidence = 0.3
        
        return child_emotion
    
    async def _add_developmental_analysis(
        self, 
        emotion: ChildVoiceEmotion, 
        audio_data: bytes, 
        child_age: int
    ) -> ChildVoiceEmotion:
        """Ø¥Ø¶Ø§ÙØ© ØªØ­Ù„ÙŠÙ„ ØªØ·ÙˆÙŠØ±ÙŠ Ø¥Ø¶Ø§ÙÙŠ Ù„Ù„Ø·ÙÙ„"""
        
        # ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        voice_characteristics = await self._analyze_voice_characteristics(audio_data)
        
        emotion.energy_level = voice_characteristics.get('energy', 0.5)
        emotion.speech_rate = voice_characteristics.get('speech_rate', 0.5)
        emotion.pitch_variation = voice_characteristics.get('pitch_variation', 0.5)
        emotion.voice_quality = voice_characteristics.get('quality', 'clear')
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡
        emotion.attention_level = self._calculate_attention_level(emotion, voice_characteristics)
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØ¶ÙˆØ­ Ø§Ù„ØªÙˆØ§ØµÙ„
        emotion.communication_clarity = self._calculate_communication_clarity(
            voice_characteristics, child_age
        )
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø¤Ø´Ø±Ø§Øª ØªØ·ÙˆÙŠØ±ÙŠØ©
        emotion.developmental_indicators = self._generate_developmental_indicators(
            emotion, child_age
        )
        
        return emotion
    
    async def _analyze_voice_characteristics(self, audio_data: bytes) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        
        # Ù‡Ø°Ø§ ØªØ­Ù„ÙŠÙ„ Ù…Ø¨Ø³Ø· - ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù†Ø³ØªØ®Ø¯Ù… librosa
        try:
            # Ù…Ø­Ø§ÙƒØ§Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª
            import random
            
            return {
                'energy': random.uniform(0.2, 0.9),
                'speech_rate': random.uniform(0.3, 0.8),
                'pitch_variation': random.uniform(0.2, 0.7),
                'quality': random.choice(['clear', 'soft', 'excited', 'tired']),
                'volume': random.uniform(0.4, except IndexError as e:
    logger.error(f"Error in operation: {e}", exc_info=True)IndexError as e:
    logger.error(f"Error in operation: {e}", exc_info=True)         }
        except IndexError as e:
    logger.error(f"Error in operation: {e}", exc_info=True)IndexError as e:
    logger.error(f"Error in operation: {e}", exc_info=True)            return {
                'energy': 0.5,
                'speech_rate': 0.5, 
                'pitch_variation': 0.5,
                'quality': 'clear',
                'volume': 0.5
            }
    
    def _get_dominant_emotion(self, emotion: ChildVoiceEmotion) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©"""
        
        emotion_scores = {
            'joy': emotion.joy,
            'sadness': emotion.sadness,
            'anger': emotion.anger,
            'fear': emotion.fear,
            'excitement': emotion.excitement,
            'calmness': emotion.calmness,
            'surprise': emotion.surprise,
            'curiosity': emotion.curiosity,
            'frustration': emotion.frustration,
            'playfulness': emotion.playfulness,
            'tiredness': emotion.tiredness
        }
        
        return max(emotion_scores.items(), key=lambda x: x[1])[0]
    
    def _calculate_confidence(self, emotion: ChildVoiceEmotion) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        
        # Ø­Ø³Ø§Ø¨ Ù…Ø¬Ù…ÙˆØ¹ ÙƒÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        total_emotions = (
            emotion.joy + emotion.sadness + emotion.anger + emotion.fear +
            emotion.excitement + emotion.calmness + emotion.surprise +
            emotion.curiosity + emotion.frustration + emotion.playfulness
        )
        
        # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø¹Ø§Ù„ÙŠØ© Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…ØŒ Ø§Ù„Ø«Ù‚Ø© Ø£Ø¹Ù„Ù‰
        return min(total_emotions / 3.0, 1.0)
    
    def _calculate_intensity(self, emotion: ChildVoiceEmotion) -> float:
        """Ø­Ø³Ø§Ø¨ Ø´Ø¯Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"""
        
        dominant_score = getattr(emotion, emotion.dominant_emotion)
        return dominant_score
    
    def _adjust_for_age(self, emotion: ChildVoiceEmotion, age: int) -> ChildVoiceEmotion:
        """ØªØ®ØµÙŠØµ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ø±"""
        
        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¹Ù…Ø±
        age_group = "3-4" if age <= 4 else "5-6" if age <= 6 else "7-8" if age <= 8 else "9-10" if age <= 10 else "11+"
        
        params = self.age_parameters.get(age_group, self.age_parameters["5-6"])
        sensitivity = params["sensitivity"]
        
        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        emotion.joy *= sensitivity
        emotion.sadness *= sensitivity
        emotion.anger *= sensitivity
        emotion.fear *= sensitivity
        
        # Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø§Ù„ØµØºØ§Ø± Ø£ÙƒØ«Ø± ÙØ¶ÙˆÙ„Ø§Ù‹
        if age <= 6:
            emotion.curiosity *= 1.2
            emotion.playfulness *= 1.3
        
        # Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø§Ù„Ø£ÙƒØ¨Ø± Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹ Ø¹Ø§Ø·ÙÙŠØ§Ù‹
        if age >= 8:
            emotion.frustration *= 1.1
            emotion.shyness *= 1.1
        
        return emotion
    
    def _apply_context(self, emotion: ChildVoiceEmotion, context: Dict[str, Any]) -> ChildVoiceEmotion:
        """ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        
        # ÙˆÙ‚Øª Ø§Ù„ÙŠÙˆÙ…
        current_hour = datetime.now().hour
        if 20 <= current_hour or current_hour <= 6:
            # ÙˆÙ‚Øª Ù…ØªØ£Ø®Ø± - Ù‚Ø¯ ÙŠÙƒÙˆÙ† Ù…ØªØ¹Ø¨
            emotion.tiredness *= 1.3
            emotion.energy_level *= 0.8
        
        # Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø£Ø®ÙŠØ±
        recent_activity = context.get('recent_activity', '')
        if 'Ù„Ø¹Ø¨' in recent_activity:
            emotion.playfulness *= 1.2
            emotion.excitement *= 1.1
        elif 'ØªØ¹Ù„Ù…' in recent_activity:
            emotion.curiosity *= 1.2
            emotion.attention_level *= 1.1
        
        return emotion
    
    def _calculate_attention_level(
        self, 
        emotion: ChildVoiceEmotion, 
        voice_characteristics: Dict[str, Any]
    ) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡"""
        
        # Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø§Ù„Ù…Ù†ØªØ¨Ù‡ÙˆÙ† Ù„Ø¯ÙŠÙ‡Ù…:
        # - ÙˆØ¶ÙˆØ­ ÙÙŠ Ø§Ù„ØµÙˆØª
        # - Ù…Ø³ØªÙˆÙ‰ Ø·Ø§Ù‚Ø© Ù…ØªÙˆØ³Ø· Ù„Ø¹Ø§Ù„ÙŠ
        # - ØªÙØ§ÙˆØª Ù…Ù†Ø®ÙØ¶ ÙÙŠ Ø§Ù„Ù†Ø¨Ø±Ø© (Ø«Ø¨Ø§Øª)
        
        clarity_score = 1.0 if voice_characteristics.get('quality') == 'clear' else 0.6
        energy_score = voice_characteristics.get('energy', 0.5)
        stability_score = 1.0 - voice_characteristics.get('pitch_variation', 0.5)
        
        # Ø§Ù„ÙØ¶ÙˆÙ„ ÙŠØ´ÙŠØ± Ø¥Ù„Ù‰ Ø§Ù†ØªØ¨Ø§Ù‡
        curiosity_bonus = emotion.curiosity * 0.3
        
        attention = (clarity_score + energy_score + stability_score + curiosity_bonus) / 4.0
        return min(attention, 1.0)
    
    def _calculate_communication_clarity(
        self, 
        voice_characteristics: Dict[str, Any], 
        child_age: int
    ) -> float:
        """Ø­Ø³Ø§Ø¨ ÙˆØ¶ÙˆØ­ Ø§Ù„ØªÙˆØ§ØµÙ„"""
        
        # Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø§Ù„Ø£ÙƒØ¨Ø± Ø¹Ø§Ø¯Ø© Ø£ÙˆØ¶Ø­ ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ù…
        age_factor = min(child_age / 10.0, 1.0)
        
        quality_score = 1.0 if voice_characteristics.get('quality') == 'clear' else 0.7
        volume_score = voice_characteristics.get('volume', 0.5)
        
        clarity = (age_factor + quality_score + volume_score) / 3.0
        return min(clarity, 1.0)
    
    def _generate_developmental_indicators(
        self, 
        emotion: ChildVoiceEmotion, 
        child_age: int
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø¤Ø´Ø±Ø§Øª ØªØ·ÙˆÙŠØ±ÙŠØ©"""
        
        indicators = []
        
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©
        if emotion.attention_level > 0.7:
            indicators.append("Ù…Ø³ØªÙˆÙ‰ Ø§Ù†ØªØ¨Ø§Ù‡ Ø¹Ø§Ù„ÙŠ")
        
        if emotion.communication_clarity > 0.8:
            indicators.append("ÙˆØ¶ÙˆØ­ ØªÙˆØ§ØµÙ„ Ù…Ù…ØªØ§Ø²")
        
        if emotion.curiosity > 0.6:
            indicators.append("ÙØ¶ÙˆÙ„ ØµØ­ÙŠ ÙˆÙ†Ø´Ø·")
        
        if emotion.playfulness > 0.7:
            indicators.append("Ø­Ø¨ Ø§Ù„Ù„Ø¹Ø¨ ÙˆØ§Ù„Ù…Ø±Ø­")
        
        # Ù…Ø¤Ø´Ø±Ø§Øª ØªØ­ØªØ§Ø¬ Ø§Ù†ØªØ¨Ø§Ù‡
        if emotion.sadness > 0.7:
            indicators.append("Ù…Ø³ØªÙˆÙ‰ Ø­Ø²Ù† Ù…Ø±ØªÙØ¹ - ÙŠØ­ØªØ§Ø¬ Ø¯Ø¹Ù…")
        
        if emotion.fear > 0.6:
            indicators.append("Ù…Ø³ØªÙˆÙ‰ Ù‚Ù„Ù‚ - ÙŠØ­ØªØ§Ø¬ Ø·Ù…Ø£Ù†ÙŠÙ†Ø©")
        
        if emotion.tiredness > 0.8:
            indicators.append("Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¹Ø¨ - ÙŠØ­ØªØ§Ø¬ Ø±Ø§Ø­Ø©")
        
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø¹Ù…Ø±ÙŠØ©
        if child_age <= 5 and emotion.curiosity < 0.3:
            indicators.append("ÙØ¶ÙˆÙ„ Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…ØªÙˆÙ‚Ø¹ Ù„Ù„Ø¹Ù…Ø±")
        
        if child_age >= 7 and emotion.communication_clarity < 0.5:
            indicators.append("ÙˆØ¶ÙˆØ­ Ø§Ù„ØªÙˆØ§ØµÙ„ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†")
        
        return indicators
    
    async def _save_temp_audio(self, audio_data: bytes) -> str:
        """Ø­ÙØ¸ Ø§Ù„ØµÙˆØª ÙÙŠ Ù…Ù„Ù Ù…Ø¤Ù‚Øª"""
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(audio_data)
            return tmp.name
    
    def _create_fallback_analysis(self) -> ChildVoiceEmotion:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¹Ù†Ø¯ ÙØ´Ù„ HUME"""
        
        return ChildVoiceEmotion(
            curiosity=0.6,
            calmness=0.4,
            joy=0.3,
            dominant_emotion="curious",
            confidence=0.3,
            emotional_intensity=0.5,
            timestamp=datetime.now().isoformat(),
            developmental_indicators=["ØªØ­Ù„ÙŠÙ„ Ø£Ø³Ø§Ø³ÙŠ - HUME ØºÙŠØ± Ù…ØªØ§Ø­"]
        )
    
    async def get_child_emotion_summary(
        self, 
        emotions_history: List[ChildVoiceEmotion]
    ) -> Dict[str, Any]:
        """Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù„Ù„Ø·ÙÙ„ Ø¹Ø¨Ø± Ø§Ù„ÙˆÙ‚Øª"""
        
        if not emotions_history:
            return {"message": "Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ©"}
        
        # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        avg_emotions = {}
        emotion_fields = ['joy', 'sadness', 'anger', 'fear', 'excitement', 
                         'calmness', 'curiosity', 'playfulness']
        
        for field in emotion_fields:
            values = [getattr(emotion, field) for emotion in emotions_history]
            avg_emotions[field] = sum(values) / len(values)
        
        # Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
        dominant_emotions = [emotion.dominant_emotion for emotion in emotions_history]
        most_common = max(set(dominant_emotions), key=dominant_emotions.count)
        
        # Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªØ·ÙˆÙŠØ±
        all_indicators = []
        for emotion in emotions_history:
            all_indicators.extend(emotion.developmental_indicators)
        
        return {
            "period_analyzed": f"{len(emotions_history)} ØªÙØ§Ø¹Ù„",
            "average_emotions": avg_emotions,
            "most_common_emotion": most_common,
            "emotional_stability": self._calculate_stability(emotions_history),
            "developmental_highlights": list(set(all_indicators)),
            "recommendations": self._generate_recommendations(avg_emotions, most_common)
        }
    
    def _calculate_stability(self, emotions_history: List[ChildVoiceEmotion]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
        
        if len(emotions_history) < 2:
            return 1.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¨Ø§ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©
        dominant_emotions = [emotion.dominant_emotion for emotion in emotions_history]
        changes = sum(1 for i in range(1, len(dominant_emotions)) 
                     if dominant_emotions[i] != dominant_emotions[i-1])
        
        stability = 1.0 - (changes / (len(dominant_emotions) - 1))
        return max(0.0, stability)
    
    def _generate_recommendations(
        self, 
        avg_emotions: Dict[str, float], 
        most_common: str
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø¨Ù†Ø§Ø¡ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        
        recommendations = []
        
        if most_common == "joy":
            recommendations.append("Ø§Ù„Ø·ÙÙ„ Ø³Ø¹ÙŠØ¯ - Ø§Ø³ØªÙ…Ø± ÙÙŠ Ø§Ù„Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©")
            recommendations.append("ÙˆÙ‚Øª Ù…Ù…ØªØ§Ø² Ù„ØªØ¹Ù„Ù… Ø£Ø´ÙŠØ§Ø¡ Ø¬Ø¯ÙŠØ¯Ø©")
        
        elif most_common == "curiosity":
            recommendations.append("Ø§Ù„Ø·ÙÙ„ ÙØ¶ÙˆÙ„ÙŠ - Ø´Ø¬Ø¹ Ø§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù")
            recommendations.append("Ù‚Ø¯Ù… Ø£Ù„Ø¹Ø§Ø¨ ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØªØ¬Ø§Ø±Ø¨ Ø¹Ù„Ù…ÙŠØ©")
        
        elif most_common == "sadness":
            recommendations.append("ÙŠØ­ØªØ§Ø¬ Ø¯Ø¹Ù… Ø¹Ø§Ø·ÙÙŠ Ø¥Ø¶Ø§ÙÙŠ")
            recommendations.append("Ø£Ù†Ø´Ø·Ø© Ù…Ø±Ø­Ø© ÙˆÙ…Ø­ÙØ²Ø© Ù„Ù„Ù…Ø²Ø§Ø¬")
        
        elif most_common == "fear":
            recommendations.append("ÙŠØ­ØªØ§Ø¬ Ø·Ù…Ø£Ù†ÙŠÙ†Ø© ÙˆØ¨ÙŠØ¦Ø© Ø¢Ù…Ù†Ø©")
            recommendations.append("ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®ÙŠÙ")
        
        # ØªÙˆØµÙŠØ§Øª Ø¹Ø§Ù…Ø©
        if avg_emotions.get('tiredness', 0) > 0.6:
            recommendations.append("Ø¹Ù„Ø§Ù…Ø§Øª ØªØ¹Ø¨ - Ù‚Ø¯ ÙŠØ­ØªØ§Ø¬ Ø±Ø§Ø­Ø©")
        
        if avg_emotions.get('playfulness', 0) > 0.7:
            recommendations.append("ÙŠØ­Ø¨ Ø§Ù„Ù„Ø¹Ø¨ - Ø£Ø¶Ù Ø£Ù„Ø¹Ø§Ø¨ ØªÙØ§Ø¹Ù„ÙŠØ©")
        
        return recommendations


# Ø¯Ø§Ù„Ø© Ø§Ø®ØªØ¨Ø§Ø±
async def test_hume_analyzer():
    """Ø§Ø®ØªØ¨Ø§Ø± Ù…Ø­Ù„Ù„ HUME"""
    
    logger.info("ğŸ§ª Testing HUME Speech Emotion Analyzer...")
    
    analyzer = HumeSpeechEmotionAnalyzer()
    
    # Ù…Ø­Ø§ÙƒØ§Ø© Ø¨ÙŠØ§Ù†Ø§Øª ØµÙˆØªÙŠØ© (ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø³ØªÙƒÙˆÙ† Ù…Ù† ESP32)
    mock_audio = b"mock_audio_data"
    
    # ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ø·ÙÙ„
    emotion = await analyzer.analyze_child_voice(
        audio_data=mock_audio,
        child_age=6,
        child_name="Ø£Ø­Ù…Ø¯",
        context={"recent_activity": "Ù„Ø¹Ø¨", "time_of_day": "afternoon"}
    )
    
    logger.info(f"ğŸ¯ Dominant Emotion: {emotion.dominant_emotion}")
    logger.info(f"ğŸ˜Š Joy: {emotion.joy:.2f}")
    logger.info(f"ğŸ¤” Curiosity: {emotion.curiosity:.2f}")
    logger.info(f"ğŸ’ª Energy Level: {emotion.energy_level:.2f}")
    logger.info(f"ğŸ¯ Attention Level: {emotion.attention_level:.2f}")
    logger.info(f"ğŸ“‹ Indicators: {emotion.developmental_indicators}")
    
    logger.info("âœ… HUME analyzer test completed!")


if __name__ == "__main__":
    asyncio.run(test_hume_analyzer()) 