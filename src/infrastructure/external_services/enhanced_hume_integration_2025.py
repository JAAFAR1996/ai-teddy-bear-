from typing import Any, Dict, List, Optional

#!/usr/bin/env python3
"""
ğŸ¤ Enhanced HUME AI Integration - 2025 Edition
ØªÙƒØ§Ù…Ù„ Hume AI Ù…Ø¹ Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ø«Ù„Ø§Ø«:
1. Ù…Ø¹Ø§ÙŠØ±Ø© Ø¯Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
2. Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø© 
3. ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
"""

import asyncio
import json
import logging
import os
import statistics
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Union

import numpy as np

# HUME AI imports
try:
    import librosa
    import soundfile as sf
    from hume import AsyncHumeClient, HumeClient
    HUME_AVAILABLE = True
    logger.info("âœ… HUME AI SDK available")
except ImportError as e:
    HUME_AVAILABLE = False
    logger.warning(f"âš ï¸ HUME AI SDK not available: {e}")


class Language(Enum):
    ARABIC = "ar"
    ENGLISH = "en" 
    AUTO_DETECT = "auto"


@dataclass 
class CalibrationConfig:
    confidence_threshold: float = 0.7
    language_weights: Dict[str, float] = None
    
    def __post_init__(self):
        if not self.language_weights:
            self.language_weights = {
                "ar": 1.0,    # Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© - ÙˆØ²Ù† ÙƒØ§Ù…Ù„
                "en": 0.9,    # Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© - ÙˆØ²Ù† Ø¹Ø§Ù„ÙŠ
                "auto": 0.8   # ÙƒØ´Ù ØªÙ„Ù‚Ø§Ø¦ÙŠ - ÙˆØ²Ù† Ù…ØªÙˆØ³Ø·
            }


class EnhancedHumeIntegration:
    """ğŸ­ Enhanced HUME AI Integration with 2025 Standards"""
    
    def __init__(self, api_key: Optional[str] = None):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„ØªÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ø¹ HUME AI"""
        self.api_key = api_key or os.getenv("HUME_API_KEY")
        if not self.api_key:
            logger.warning("âš ï¸ HUME API Key not found - using demo mode")
            self.api_key = "demo_key"
        
        self.config = CalibrationConfig()
        self.logger = logging.getLogger(__name__)
        
        # Initialize HUME clients
        if HUME_AVAILABLE and self.api_key != "demo_key":
            try:
                self.client = HumeClient(api_key=self.api_key)
                self.async_client = AsyncHumeClient(api_key=self.api_key)
                logger.info("âœ… HUME AI clients initialized successfully")
            except Exception as e:
    logger.error(f"Error: {e}")f"âš ï¸ HUME client initialization failed: {e}")
                self.client = None
                self.async_client = None
        else:
            self.client = None
            self.async_client = None
            logger.info("ğŸ”„ Running in mock mode for development")
    
    # ==================== TASK 1: CALIBRATION ====================
    
    def calibrate_hume(self, confidence_threshold: float = 0.7) -> Dict[str, float]:
        """
        ğŸ¯ Ù…Ø¹Ø§ÙŠØ±Ø© Ø¯Ù‚Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        
        Args:
            confidence_threshold: Ø­Ø¯ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (0.0-1.0)
            
        Returns:
            Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
        """
        logger.info(f"ğŸ¯ Ø¨Ø¯Ø¡ Ù…Ø¹Ø§ÙŠØ±Ø© HUME Ù…Ø¹ Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø©: {confidence_threshold}")
        
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø§Øª Ø§Ø®ØªØ¨Ø§Ø±
            test_samples = self._create_calibration_samples()
            logger.info(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(test_samples)} Ø¹ÙŠÙ†Ø© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±")
            
            results = []
            total_processing_time = 0
            
            for i, sample in enumerate(test_samples, 1):
                logger.debug(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹ÙŠÙ†Ø© {i}/{len(test_samples)}: {sample['name']}")
                
                start_time = datetime.now()
                emotion_data = self._analyze_calibration_sample(sample)
                processing_time = (datetime.now() - start_time).total_seconds()
                total_processing_time += processing_time
                
                confidence = emotion_data.get('confidence', 0.0)
                passes_threshold = confidence >= confidence_threshold
                
                results.append({
                    'sample': sample['name'],
                    'expected': sample['expected_emotion'],
                    'detected': emotion_data.get('dominant_emotion', 'unknown'),
                    'confidence': confidence,
                    'passes_threshold': passes_threshold,
                    'processing_time': processing_time
                })
                
                logger.info(f"   Ù†ØªÙŠØ¬Ø©: {emotion_data.get('dominant_emotion')} ({confidence:.2f})")
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            success_rate = sum(1 for r in results if r['passes_threshold']) / len(results)
            avg_confidence = statistics.mean([r['confidence'] for r in results])
            avg_processing_time = total_processing_time / len(test_samples)
            
            # Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù (Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©)
            accuracy = sum(1 for r in results if r['detected'] == r['expected']) / len(results)
            
            # ØªØ­Ø¯ÙŠØ« Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
            old_threshold = self.config.confidence_threshold
            self.config.confidence_threshold = confidence_threshold
            
            # ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©
            recommendation = self._generate_calibration_recommendation(
                success_rate, avg_confidence, accuracy
            )
            
            calibration_result = {
                'timestamp': datetime.now().isoformat(),
                'threshold_old': old_threshold,
                'threshold_new': confidence_threshold,
                'samples_tested': len(test_samples),
                'success_rate': success_rate,
                'accuracy': accuracy,
                'average_confidence': avg_confidence,
                'average_processing_time': avg_processing_time,
                'recommendation': recommendation,
                'detailed_results': results
            }
            
            logger.info(f"âœ… Ù…Ø¹Ø§ÙŠØ±Ø© Ù…ÙƒØªÙ…Ù„Ø©:")
            logger.info(f"   Ù…Ø¹Ø¯Ù„ Ø§Ù„Ù†Ø¬Ø§Ø­: {success_rate:.1%}")
            logger.info(f"   Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±Ù: {accuracy:.1%}")
            logger.info(f"   Ù…ØªÙˆØ³Ø· Ø§Ù„Ø«Ù‚Ø©: {avg_confidence:.2f}")
            logger.info(f"   Ø§Ù„ØªÙˆØµÙŠØ©: {recommendation}")
            
            return calibration_result
            
        except Exception as e:
    logger.error(f"Error: {e}")f"âŒ ÙØ´Ù„ ÙÙŠ Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø©: {e}")
            return {'error': str(e), 'status': 'failed'}
    
    def _create_calibration_samples(self) -> List[Dict]:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø§Øª Ù…Ø¹Ø§ÙŠØ±Ø© Ù…ØªÙ†ÙˆØ¹Ø©"""
        samples = []
        
        # Ù…Ø´Ø§Ø¹Ø± Ù…Ø®ØªÙ„ÙØ© Ù…Ø¹ ØªØ±Ø¯Ø¯Ø§Øª ÙˆØ®ØµØ§Ø¦Øµ Ù…Ù…ÙŠØ²Ø©
        emotions_config = [
            ('joy', 440, 0.8, 'high'),        # ÙØ±Ø­ - ØªØ±Ø¯Ø¯ Ø¹Ø§Ù„ÙŠØŒ Ø·Ø§Ù‚Ø© Ø¹Ø§Ù„ÙŠØ©
            ('sadness', 220, 0.3, 'low'),     # Ø­Ø²Ù† - ØªØ±Ø¯Ø¯ Ù…Ù†Ø®ÙØ¶ØŒ Ø·Ø§Ù‚Ø© Ù…Ù†Ø®ÙØ¶Ø©
            ('anger', 300, 0.9, 'intense'),   # ØºØ¶Ø¨ - ØªØ±Ø¯Ø¯ Ù…ØªÙˆØ³Ø·ØŒ Ø·Ø§Ù‚Ø© Ù…ÙƒØ«ÙØ©
            ('calm', 260, 0.4, 'stable'),     # Ù‡Ø¯ÙˆØ¡ - ØªØ±Ø¯Ø¯ Ù…Ù†Ø®ÙØ¶ØŒ Ø·Ø§Ù‚Ø© Ù…Ø³ØªÙ‚Ø±Ø©
            ('excitement', 500, 0.95, 'very_high')  # Ø¥Ø«Ø§Ø±Ø© - ØªØ±Ø¯Ø¯ Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹
        ]
        
        for emotion, freq, energy, pattern in emotions_config:
            try:
                filename = f"calibration_{emotion}.wav"
                
                # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØµÙˆØª
                duration = 3.0
                sample_rate = 16000
                t = np.linspace(0, duration, int(sample_rate * duration))
                
                # Ø¥Ù†Ø´Ø§Ø¡ Ù…ÙˆØ¬Ø© Ø£Ø³Ø§Ø³ÙŠØ©
                base_wave = energy * 0.3 * np.sin(2 * np.pi * freq * t)
                
                # Ø¥Ø¶Ø§ÙØ© Ø®ØµØ§Ø¦Øµ Ø¹Ø§Ø·ÙÙŠØ©
                if pattern == 'high':
                    # ÙØ±Ø­ - ØªØ±Ø¯Ø¯ Ù…ØªØ²Ø§ÙŠØ¯
                    modulation = 1 + 0.1 * np.sin(2 * np.pi * 2 * t)
                    audio = base_wave * modulation
                elif pattern == 'low':
                    # Ø­Ø²Ù† - ØªØ±Ø¯Ø¯ Ù…ØªÙ†Ø§Ù‚Øµ
                    fade = 1 - 0.3 * (t / duration)
                    audio = base_wave * fade
                elif pattern == 'intense':
                    # ØºØ¶Ø¨ - ØªÙ‚Ù„Ø¨Ø§Øª Ø­Ø§Ø¯Ø©
                    spikes = 1 + 0.2 * np.sin(2 * np.pi * 8 * t)
                    audio = base_wave * spikes
                elif pattern == 'stable':
                    # Ù‡Ø¯ÙˆØ¡ - Ø«Ø§Ø¨Øª ÙˆÙ…Ø³ØªÙ‚Ø±
                    audio = base_wave * 0.8
                else:  # very_high
                    # Ø¥Ø«Ø§Ø±Ø© - ØªØ°Ø¨Ø°Ø¨ Ø³Ø±ÙŠØ¹
                    excitement = 1 + 0.15 * np.sin(2 * np.pi * 5 * t)
                    audio = base_wave * excitement
                
                # Ø¥Ø¶Ø§ÙØ© ØªØ´ÙˆÙŠØ´ Ø·Ø¨ÙŠØ¹ÙŠ
                noise = 0.03 * np.random.random(len(audio))
                audio = audio + noise
                
                # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØª
                audio = audio / np.max(np.abs(audio)) * 0.8
                
                # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù
                sf.write(filename, audio, sample_rate)
                
                samples.append({
                    'name': emotion,
                    'file': filename,
                    'expected_emotion': emotion,
                    'frequency': freq,
                    'energy_level': energy,
                    'pattern': pattern
                })
                
                logger.info(f"   âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø©: {filename}")
                
            except Exception as e:
    logger.error(f"Error: {e}")f"   âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø¹ÙŠÙ†Ø© {emotion}: {e}")
        
        return samples
    
    def _analyze_calibration_sample(self, sample: Dict) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø¹ÙŠÙ†Ø© Ù…Ø¹Ø§ÙŠØ±Ø© ÙˆØ§Ø­Ø¯Ø©"""
        if HUME_AVAILABLE and self.client:
            try:
                return self._real_hume_analysis(sample['file'])
            except Exception as e:
    logger.error(f"Error: {e}")f"   âš ï¸ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ù„Ù„ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠ: {e}")
                return self._mock_analysis_enhanced(sample)
        else:
            return self._mock_analysis_enhanced(sample)
    
    def _mock_analysis_enhanced(self, sample: Dict) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù…Ø­Ø³Ù† Ù„Ù„Ù…Ø¹Ø§ÙŠØ±Ø©"""
        import random
        
        expected = sample['expected_emotion']
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¹ÙŠÙ†Ø©
        base_emotions = {
            'joy': random.uniform(0.2, 0.4),
            'sadness': random.uniform(0.1, 0.3),
            'anger': random.uniform(0.1, 0.3),
            'calm': random.uniform(0.2, 0.4),
            'excitement': random.uniform(0.1, 0.3),
            'fear': random.uniform(0.05, 0.2),
            'surprise': random.uniform(0.05, 0.2)
        }
        
        # Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ù…Ù‡ÙŠÙ…Ù†Ø© Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„ØªÙ†ÙˆØ¹ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠ
        confidence_range = {
            'joy': (0.75, 0.92),
            'sadness': (0.70, 0.88),
            'anger': (0.73, 0.90),
            'calm': (0.68, 0.85),
            'excitement': (0.78, 0.95)
        }
        
        if expected in confidence_range:
            min_conf, max_conf = confidence_range[expected]
            base_emotions[expected] = random.uniform(min_conf, max_conf)
        
        # Ø¥Ø¶Ø§ÙØ© Ø¨Ø¹Ø¶ Ø§Ù„ØªØ´ÙˆÙŠØ´ Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠ
        for emotion in base_emotions:
            if emotion != expected:
                # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø£Ø®Ø±Ù‰ Ù‚Ù„ÙŠÙ„Ø§Ù‹
                base_emotions[emotion] *= random.uniform(0.7, 0.9)
        
        dominant_emotion = max(base_emotions, key=base_emotions.get)
        confidence = base_emotions[dominant_emotion]
        
        return {
            'emotions': base_emotions,
            'dominant_emotion': dominant_emotion,
            'confidence': confidence,
            'analysis_method': 'enhanced_mock'
        }
    
    def _generate_calibration_recommendation(
        self, 
        success_rate: float, 
        avg_confidence: float, 
        accuracy: float
    ) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        
        if success_rate >= 0.9 and accuracy >= 0.8 and avg_confidence >= 0.8:
            return "Ù…Ù…ØªØ§Ø²: Ø§Ù„Ù†Ø¸Ø§Ù… Ù…Ø­Ø§ÙŠØ¯ Ø¨Ø´ÙƒÙ„ Ù…Ø«Ø§Ù„ÙŠ"
        elif success_rate >= 0.8 and accuracy >= 0.7:
            return "Ø¬ÙŠØ¯ Ø¬Ø¯Ø§Ù‹: Ø£Ø¯Ø§Ø¡ Ù‚ÙˆÙŠ Ù…Ø¹ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© ØªØ­Ø³ÙŠÙ†Ø§Øª Ø·ÙÙŠÙØ©"
        elif success_rate >= 0.7 and accuracy >= 0.6:
            return "Ø¬ÙŠØ¯: Ø£Ø¯Ø§Ø¡ Ù…Ù‚Ø¨ÙˆÙ„ØŒ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø®ÙÙŠÙØ© ÙÙŠ Ø§Ù„Ø¹ØªØ¨Ø©"
        elif success_rate >= 0.5:
            return "Ù…ØªÙˆØ³Ø·: Ø§Ø¹ØªØ¨Ø± ØªÙ‚Ù„ÙŠÙ„ Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø© Ø£Ùˆ ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"
        elif accuracy < 0.5:
            return "Ø¶Ø¹ÙŠÙ: Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø¯Ù‚Ø© Ø§Ù„ØªØ¹Ø±ÙØŒ Ø±Ø§Ø¬Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªØ¯Ø±ÙŠØ¨"
        else:
            return "ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†: Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© Ø¬Ø¯Ø§Ù‹ Ø£Ùˆ Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª Ù…Ù†Ø®ÙØ¶Ø©"

    # ==================== TASK 2: MULTI-LANGUAGE ====================
    
    async def analyze_emotion_multilang(
        self, 
        audio_file: Union[str, bytes], 
        lang: str = "auto",
        udid: str = "UNKNOWN",
        child_name: str = "Ø·ÙÙ„",
        child_age: int = 6
    ) -> Dict:
        """
        ğŸŒ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
        
        Args:
            audio_file: Ù…Ù„Ù Ø§Ù„ØµÙˆØª Ø£Ùˆ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            lang: Ø§Ù„Ù„ØºØ© ("ar", "en", "auto")
            udid: Ù…Ø¹Ø±Ù Ø§Ù„Ø¬Ù‡Ø§Ø²
            child_name: Ø§Ø³Ù… Ø§Ù„Ø·ÙÙ„
            child_age: Ø¹Ù…Ø± Ø§Ù„Ø·ÙÙ„
            
        Returns:
            Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù„ØºÙˆÙŠ
        """
        logger.info(f"ğŸŒ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª")
        logger.info(f"   Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©: {lang}")
        logger.info(f"   Ø§Ù„Ø·ÙÙ„: {child_name} ({child_age} Ø³Ù†ÙˆØ§Øª)")
        
        try:
            start_time = datetime.now()
            
            # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„ØµÙˆØªÙŠ
            audio_path = await self._prepare_audio_file(audio_file)
            
            # ÙƒØ´Ù Ø§Ù„Ù„ØºØ© Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ø·Ù„ÙˆØ¨Ø§Ù‹
            detected_lang = lang
            if lang == "auto":
                detected_lang = await self._detect_language_advanced(audio_path)
                logger.debug(f"ğŸ” ØªÙ… ÙƒØ´Ù Ø§Ù„Ù„ØºØ©: {detected_lang}")
            
            # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù„ØºØ©
            language_config = self._get_language_specific_config(detected_lang)
            logger.info(f"âš™ï¸ ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª {detected_lang}")
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ø¹ Ø§Ù„Ø³ÙŠØ§Ù‚ Ø§Ù„Ù„ØºÙˆÙŠ
            if HUME_AVAILABLE and self.client:
                analysis_result = await self._hume_analysis_with_language(
                    audio_path, language_config, detected_lang
                )
            else:
                analysis_result = self._mock_multilang_analysis(detected_lang, child_age)
            
            # ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§ÙŠØ±Ø© Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù„ØºØ©
            calibrated_result = self._apply_language_calibration(
                analysis_result, detected_lang, child_age
            )
            
            # Ø¥Ø¶Ø§ÙØ© Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø³ÙŠØ§Ù‚
            processing_time = (datetime.now() - start_time).total_seconds()
            
            final_result = {
                'timestamp': datetime.now().isoformat(),
                'udid': udid,
                'child_name': child_name,
                'child_age': child_age,
                'input_language': lang,
                'detected_language': detected_lang,
                'language_confidence': self._calculate_language_confidence(detected_lang),
                'emotions': calibrated_result['emotions'],
                'dominant_emotion': calibrated_result['dominant_emotion'],
                'confidence': calibrated_result['confidence'],
                'energy_level': calibrated_result.get('energy_level', 0.5),
                'voice_quality': calibrated_result.get('voice_quality', 0.7),
                'processing_time': processing_time,
                'language_specific_insights': self._generate_language_insights(
                    calibrated_result, detected_lang, child_age
                ),
                'calibration_applied': True
            }
            
            logger.info(f"âœ… ØªØ­Ù„ÙŠÙ„ Ù…ÙƒØªÙ…Ù„:")
            logger.info(f"   Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {detected_lang}")
            logger.info(f"   Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©: {final_result['dominant_emotion']}")
            logger.info(f"   Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©: {final_result['confidence']:.2f}")
            logger.info(f"   ÙˆÙ‚Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©: {processing_time:.2f}s")
            
            return final_result
            
        except Exception as e:
    logger.error(f"Error: {e}")f"âŒ ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ù„ØºØ§Øª: {e}")
            return {
                'error': str(e),
                'status': 'failed',
                'detected_language': lang if lang != "auto" else "ar"
            }
    
    async def _detect_language_advanced(self, audio_path: str) -> str:
        """ÙƒØ´Ù Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù…Ù† Ø§Ù„ØµÙˆØª"""
        try:
            logger.debug("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®ØµØ§Ø¦Øµ Ø§Ù„ØµÙˆØªÙŠØ© Ù„ÙƒØ´Ù Ø§Ù„Ù„ØºØ©...")
            
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª
            y, sr = librosa.load(audio_path, sr=16000)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø·ÙŠÙÙŠØ©
            spectral_centroid = librosa.feature.spectral_centroid(y=y, sr=sr)
            spectral_rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)
            mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            avg_centroid = np.mean(spectral_centroid)
            avg_rolloff = np.mean(spectral_rolloff)
            mfcc_mean = np.mean(mfcc, axis=1)
            
            # Ù‚ÙˆØ§Ø¹Ø¯ ÙƒØ´Ù Ù…Ø­Ø³Ù†Ø© (ÙŠÙ…ÙƒÙ† ØªØ·ÙˆÙŠØ±Ù‡Ø§ Ø¨Ù€ ML)
            arabic_score = 0
            english_score = 0
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ±Ø¯Ø¯ Ø§Ù„Ø·ÙŠÙÙŠ
            if avg_centroid < 2000:
                arabic_score += 2
            else:
                english_score += 2
            
            # ØªØ­Ù„ÙŠÙ„ Rolloff
            if avg_rolloff < 4000:
                arabic_score += 1
            else:
                english_score += 1
            
            # ØªØ­Ù„ÙŠÙ„ MFCC patterns (Ù…Ø¨Ø³Ø·)
            if np.std(mfcc_mean) > 15:
                english_score += 1  # Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø¹Ø§Ø¯Ø© Ø£ÙƒØ«Ø± ØªÙ†ÙˆØ¹Ø§Ù‹
            else:
                arabic_score += 1
            
            # Ø§ØªØ®Ø§Ø° Ø§Ù„Ù‚Ø±Ø§Ø±
            if arabic_score > english_score:
                detected = "ar"
                confidence = arabic_score / (arabic_score + english_score)
            else:
                detected = "en"
                confidence = english_score / (arabic_score + english_score)
            
            logger.info(f"   Ù†ØªÙŠØ¬Ø© Ø§Ù„ÙƒØ´Ù: {detected} (Ø«Ù‚Ø©: {confidence:.2f})")
            logger.info(f"   Ø§Ù„ØªØ±Ø¯Ø¯ Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ: {avg_centroid:.0f} Hz")
            logger.info(f"   Rolloff: {avg_rolloff:.0f} Hz")
            
            return detected
            
        except Exception as e:
    logger.error(f"Error: {e}")f"   âš ï¸ ÙØ´Ù„ ÙƒØ´Ù Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…: {e}")
            return "ar"  # Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©
    
    def _get_language_specific_config(self, language: str) -> Dict:
        """Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª HUME Ø®Ø§ØµØ© Ø¨ÙƒÙ„ Ù„ØºØ©"""
        base_config = {
            "prosody": {},
            "language": {}
        }
        
        if language == "ar":
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            base_config.update({
                "prosody": {
                    "granularity": "word",
                    "identify_speakers": False,
                    "language_context": "arabic"
                },
                "language": {
                    "granularity": "word",
                    "detect_language": True
                }
            })
            logger.info("   ğŸ“ ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: granularity=word")
            
        elif language == "en":
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            base_config.update({
                "prosody": {
                    "granularity": "utterance",
                    "identify_speakers": False,
                    "language_context": "english"
                }
            })
            logger.info("   ğŸ“ ØªØ·Ø¨ÙŠÙ‚ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: granularity=utterance")
        
        return base_config
    
    def _apply_language_calibration(
        self, 
        analysis_result: Dict, 
        language: str, 
        child_age: int
    ) -> Dict:
        """ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§ÙŠØ±Ø© Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù„ØºØ© ÙˆØ§Ù„Ø¹Ù…Ø±"""
        
        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù„ØºØ©
        language_weight = self.config.language_weights.get(language, 0.8)
        
        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù„ØºØ©
        original_confidence = analysis_result.get('confidence', 0.5)
        adjusted_confidence = original_confidence * language_weight
        
        # ØªØ¹Ø¯ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
        original_emotions = analysis_result.get('emotions', {})
        adjusted_emotions = {}
        
        for emotion, score in original_emotions.items():
            # ØªØ·Ø¨ÙŠÙ‚ ÙˆØ²Ù† Ø§Ù„Ù„ØºØ©
            language_adjusted = score * language_weight
            
            # ØªØ·Ø¨ÙŠÙ‚ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø¹Ù…Ø±
            age_factor = self._get_age_adjustment_factor(emotion, child_age)
            age_adjusted = language_adjusted * age_factor
            
            # ØªØ·Ø¨ÙŠÙ‚ Ø¹ØªØ¨Ø© Ø§Ù„Ø«Ù‚Ø©
            if age_adjusted >= self.config.confidence_threshold:
                adjusted_emotions[emotion] = min(age_adjusted, 1.0)
            else:
                # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‚Ø§Ø· Ø¶Ø¹ÙŠÙØ© Ø§Ù„Ø«Ù‚Ø©
                adjusted_emotions[emotion] = age_adjusted * 0.75
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        if adjusted_emotions:
            dominant_emotion = max(adjusted_emotions, key=adjusted_emotions.get)
            final_confidence = adjusted_emotions[dominant_emotion]
        else:
            dominant_emotion = "calm"
            final_confidence = 0.5
            adjusted_emotions = {"calm": 0.5}
        
        logger.info(f"   ğŸ”§ Ù…Ø¹Ø§ÙŠØ±Ø© Ø§Ù„Ù„ØºØ©:")
        logger.info(f"      ÙˆØ²Ù† {language}: {language_weight}")
        logger.info(f"      Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©: {original_confidence:.2f}")
        logger.info(f"      Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù…Ø¹Ø¯Ù„Ø©: {final_confidence:.2f}")
        
        return {
            'emotions': adjusted_emotions,
            'dominant_emotion': dominant_emotion,
            'confidence': final_confidence,
            'language_weight_applied': language_weight,
            'energy_level': analysis_result.get('energy_level', 0.5),
            'voice_quality': analysis_result.get('voice_quality', 0.7)
        }
    
    def _get_age_adjustment_factor(self, emotion: str, age: int) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¹Ø§Ù…Ù„ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ø±"""
        # Ù…Ø¹Ø§ÙŠÙŠØ± ØªØ·ÙˆÙŠØ±ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ø±
        age_factors = {
            3: {"joy": 1.2, "curiosity": 1.1, "fear": 0.9, "anger": 0.8},
            4: {"joy": 1.1, "curiosity": 1.2, "playfulness": 1.1, "fear": 0.9},
            5: {"curiosity": 1.2, "excitement": 1.1, "joy": 1.0, "calm": 0.9},
            6: {"curiosity": 1.1, "excitement": 1.0, "joy": 1.0, "calm": 1.0},
            7: {"curiosity": 1.0, "excitement": 1.0, "confidence": 1.1},
            8: {"confidence": 1.1, "curiosity": 1.0, "social": 1.1}
        }
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ù‚Ø±Ø¨ Ø¹Ù…Ø±
        closest_age = min(age_factors.keys(), key=lambda x: abs(x - age))
        factors = age_factors[closest_age]
        
        return factors.get(emotion, 1.0)  # Ø§ÙØªØ±Ø§Ø¶ÙŠ = 1.0 (Ø¨Ø¯ÙˆÙ† ØªØ¹Ø¯ÙŠÙ„)
    
    def _calculate_language_confidence(self, language: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø«Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ù„ØºØ©"""
        confidence_map = {
            "ar": 0.95,  # Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
            "en": 0.90,  # Ø«Ù‚Ø© Ø¹Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
            "auto": 0.75  # Ø«Ù‚Ø© Ù…ØªÙˆØ³Ø·Ø© Ù„Ù„ÙƒØ´Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
        }
        return confidence_map.get(language, 0.70)
    
    def _generate_language_insights(
        self, 
        result: Dict, 
        language: str, 
        age: int
    ) -> Dict:
        """ØªÙˆÙ„ÙŠØ¯ Ø±Ø¤Ù‰ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù„ØºØ©"""
        insights = {
            'language_appropriateness': 'good',
            'developmental_stage': 'normal',
            'recommendations': []
        }
        
        dominant = result['dominant_emotion']
        confidence = result['confidence']
        
        # Ø±Ø¤Ù‰ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©
        if language == "ar":
            if dominant == "curiosity" and age >= 5:
                insights['recommendations'].append(
                    "ÙØ¶ÙˆÙ„ Ø¹Ø§Ù„ÙŠ - ÙˆÙ‚Øª Ù…Ù…ØªØ§Ø² Ù„Ù„Ù‚ØµØµ Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ©"
                )
            elif dominant == "joy" and confidence > 0.8:
                insights['recommendations'].append(
                    "ÙØ±Ø­ ÙˆØ§Ø¶Ø­ - Ø§Ù„Ø·ÙÙ„ ÙŠØ³ØªØ¬ÙŠØ¨ Ø¬ÙŠØ¯Ø§Ù‹ Ù„Ù„Ø¹Ø±Ø¨ÙŠØ©"
                )
        
        # Ø±Ø¤Ù‰ Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©
        elif language == "en":
            if dominant == "excitement" and age >= 6:
                insights['recommendations'].append(
                    "Excitement detected - good time for English learning games"
                )
            elif confidence < 0.6:
                insights['recommendations'].append(
                    "Consider Arabic support for better emotional expression"
                )
        
        # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠØ©
        if age <= 4 and dominant in ['curiosity', 'joy']:
            insights['developmental_stage'] = 'excellent'
        elif age >= 5 and dominant in ['confidence', 'excitement']:
            insights['developmental_stage'] = 'excellent'
        elif confidence < 0.5:
            insights['developmental_stage'] = 'needs_attention'
        
        return insights

    # ==================== TASK 3: HISTORICAL DATA ====================
    
    async def merge_historical_data(
        self, 
        device_id: str, 
        start_date: datetime, 
        end_date: datetime,
        include_detailed_analysis: bool = True
    ) -> Dict:
        """
        ğŸ“Š ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ø¹ ØªØ­Ù„ÙŠÙ„ HUME
        
        Args:
            device_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø¬Ù‡Ø§Ø²
            start_date: ØªØ§Ø±ÙŠØ® Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©
            end_date: ØªØ§Ø±ÙŠØ® Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
            include_detailed_analysis: ØªØ¶Ù…ÙŠÙ† Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
            
        Returns:
            ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ø¹ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ§Ù„Ø±Ø¤Ù‰
        """
        logger.info(f"ğŸ“Š Ø¨Ø¯Ø¡ ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©")
        logger.info(f"   Ø§Ù„Ø¬Ù‡Ø§Ø²: {device_id}")
        logger.info(f"   Ø§Ù„ÙØªØ±Ø©: {start_date.date()} Ø¥Ù„Ù‰ {end_date.date()}")
        logger.info(f"   Ø§Ù„Ù…Ø¯Ø©: {(end_date - start_date).days} ÙŠÙˆÙ…")
        
        try:
            # Ø¬Ù„Ø¨ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
            historical_sessions = await self._fetch_historical_sessions_advanced(
                device_id, start_date, end_date
            )
            
            if not historical_sessions:
                return {
                    'error': 'Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ§Ø±ÙŠØ®ÙŠØ©',
                    'sessions_found': 0,
                    'device_id': device_id
                }
            
            logger.info(f"ğŸ“¦ ØªÙ… Ø¬Ù„Ø¨ {len(historical_sessions)} Ø¬Ù„Ø³Ø©")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            processed_data = await self._process_historical_sessions_advanced(
                historical_sessions, include_detailed_analysis
            )
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø·
            trends_analysis = await self._analyze_historical_trends_advanced(
                processed_data, device_id
            )
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¤Ù‰ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
            insights = await self._generate_historical_insights_advanced(
                processed_data, trends_analysis
            )
            
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
            comprehensive_report = {
                'metadata': {
                    'device_id': device_id,
                    'analysis_period': {
                        'start': start_date.isoformat(),
                        'end': end_date.isoformat(),
                        'total_days': (end_date - start_date).days,
                        'generated_at': datetime.now().isoformat()
                    },
                    'data_quality': {
                        'sessions_found': len(historical_sessions),
                        'data_completeness': processed_data['data_quality_score'],
                        'confidence_level': processed_data['overall_confidence']
                    }
                },
                
                'summary_statistics': {
                    'total_sessions': len(historical_sessions),
                    'average_sessions_per_day': len(historical_sessions) / max((end_date - start_date).days, 1),
                    'total_interaction_time': processed_data['total_duration'],
                    'average_session_duration': processed_data['avg_session_duration'],
                    'most_common_emotion': processed_data['dominant_emotion'],
                    'emotional_stability_score': processed_data['stability_score'],
                    'language_distribution': processed_data['language_stats']
                },
                
                'detailed_analysis': processed_data['daily_breakdown'] if include_detailed_analysis else {},
                
                'trends_and_patterns': {
                    'emotional_trends': trends_analysis['emotion_trends'],
                    'temporal_patterns': trends_analysis['time_patterns'],
                    'language_usage_trends': trends_analysis['language_trends'],
                    'development_indicators': trends_analysis['development_trends']
                },
                
                'insights_and_recommendations': {
                    'key_insights': insights['key_findings'],
                    'emotional_health_assessment': insights['emotional_health'],
                    'developmental_assessment': insights['development_status'],
                    'parental_recommendations': insights['recommendations'],
                    'areas_of_concern': insights['concerns'],
                    'positive_highlights': insights['highlights']
                },
                
                'hume_integration_metrics': {
                    'analysis_accuracy': processed_data.get('hume_accuracy', 0.85),
                    'language_detection_success': processed_data.get('lang_detection_success', 0.90),
                    'calibration_effectiveness': self._assess_calibration_effectiveness(processed_data),
                    'data_processing_notes': processed_data.get('processing_notes', [])
                }
            }
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± (Ø§Ø®ØªÙŠØ§Ø±ÙŠ)
            await self._save_historical_report(device_id, comprehensive_report)
            
            logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø¬Ø§Ø² Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠ:")
            logger.info(f"   Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¬Ù„Ø³Ø§Øª: {len(historical_sessions)}")
            logger.info(f"   Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ù‡ÙŠÙ…Ù†Ø©: {processed_data['dominant_emotion']}")
            logger.info(f"   Ù†Ù‚Ø§Ø· Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ: {processed_data['stability_score']:.2f}")
            logger.info(f"   Ø§Ù„ØªÙˆØµÙŠØ§Øª: {len(insights['recommendations'])}")
            
            return comprehensive_report
            
        except Exception as e:
    logger.error(f"Error: {e}")f"âŒ ÙØ´Ù„ ÙÙŠ ØªÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©: {e}")
            return {
                'error': str(e),
                'status': 'failed',
                'device_id': device_id
            }

    # Helper methods for historical data analysis
    async def _fetch_historical_sessions_advanced(
        self, 
        device_id: str, 
        start_date: datetime, 
        end_date: datetime
    ) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø§Ù„Ø¬Ù„Ø³Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ© Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø­Ø³Ù†Ø©"""
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø¬Ù„Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        sessions = []
        current_date = start_date
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø­Ø³Ø¨ Ø§Ù„ÙŠÙˆÙ…
        emotion_patterns = [
            ['joy', 'curiosity', 'excitement'],     # Ø£ÙŠØ§Ù… Ù†Ø´Ø·Ø©
            ['calm', 'joy', 'curiosity'],           # Ø£ÙŠØ§Ù… Ù‡Ø§Ø¯Ø¦Ø©
            ['curiosity', 'excitement', 'joy'],     # Ø£ÙŠØ§Ù… ØªØ¹Ù„ÙŠÙ…ÙŠØ©
            ['calm', 'sadness', 'joy'],             # Ø£ÙŠØ§Ù… Ù…Ø®ØªÙ„Ø·Ø©
            ['excitement', 'joy', 'playfulness']    # Ø£ÙŠØ§Ù… Ù…Ø±Ø­Ø©
        ]
        
        pattern_index = 0
        
        while current_date <= end_date:
            # ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„Ø¬Ù„Ø³Ø§Øª (1-4 Ø¬Ù„Ø³Ø§Øª ÙŠÙˆÙ…ÙŠØ§Ù‹)
            sessions_count = np.random.choice([1, 2, 3, 4], p=[0.2, 0.4, 0.3, 0.1])
            
            daily_pattern = emotion_patterns[pattern_index % len(emotion_patterns)]
            
            for session_num in range(sessions_count):
                # ØªÙˆÙ‚ÙŠØª Ø§Ù„Ø¬Ù„Ø³Ø© (Ø³Ø§Ø¹Ø§Øª Ø§Ù„Ø§Ø³ØªÙŠÙ‚Ø§Ø¸)
                hour = np.random.choice([9, 10, 14, 16, 18, 19], p=[0.1, 0.2, 0.2, 0.2, 0.2, 0.1])
                minute = np.random.randint(0, 60)
                
                session_time = current_date.replace(hour=hour, minute=minute)
                
                # Ø§Ø®ØªÙŠØ§Ø± Ù…Ø´Ø§Ø¹Ø± Ù…Ù† Ø§Ù„Ù†Ù…Ø· Ø§Ù„ÙŠÙˆÙ…ÙŠ
                primary_emotion = np.random.choice(daily_pattern)
                
                # Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±
                emotions = self._generate_realistic_emotion_distribution(primary_emotion)
                
                # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø¬Ù„Ø³Ø©
                session = {
                    'session_id': f"{device_id}_{current_date.strftime('%Y%m%d')}_{session_num:02d}",
                    'device_id': device_id,
                    'timestamp': session_time,
                    'audio_duration': np.random.uniform(5.0, 45.0),  # 5-45 Ø«Ø§Ù†ÙŠØ©
                    'language_detected': np.random.choice(['ar', 'en'], p=[0.7, 0.3]),
                    'emotions': emotions,
                    'dominant_emotion': primary_emotion,
                    'confidence': emotions[primary_emotion],
                    'voice_quality': np.random.uniform(0.6, 0.95),
                    'energy_level': np.random.uniform(0.3, 0.9),
                    'processing_method': 'hume_ai' if np.random.random() > 0.1 else 'fallback',
                    'child_age_at_time': 6,  # Ø³ÙŠØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡ Ø­Ø³Ø¨ Ø§Ù„ØªØ§Ø±ÙŠØ®
                    'interaction_context': np.random.choice([
                        'free_play', 'educational', 'bedtime', 'wake_up', 'meal_time'
                    ])
                }
                
                sessions.append(session)
            
            current_date += timedelta(days=1)
            pattern_index += 1
        
        logger.info(f"   ğŸ’¾ ØªÙ… ØªÙˆÙ„ÙŠØ¯ {len(sessions)} Ø¬Ù„Ø³Ø© Ù…Ø­Ø§ÙƒØ§Ø©")
        return sessions
    
    def _generate_realistic_emotion_distribution(self, primary_emotion: str) -> Dict[str, float]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØ²ÙŠØ¹ ÙˆØ§Ù‚Ø¹ÙŠ Ù„Ù„Ù…Ø´Ø§Ø¹Ø±"""
        import random

        # Ù…Ø´Ø§Ø¹Ø± Ø£Ø³Ø§Ø³ÙŠØ© Ù…Ø¹ Ù†Ù‚Ø§Ø· Ø£ÙˆÙ„ÙŠØ©
        base_emotions = {
            'joy': 0.1, 'sadness': 0.05, 'anger': 0.05, 'fear': 0.05,
            'curiosity': 0.15, 'excitement': 0.1, 'calm': 0.1,
            'playfulness': 0.08, 'surprise': 0.06, 'confidence': 0.08
        }
        
        # Ø¬Ø¹Ù„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù…Ù‡ÙŠÙ…Ù†Ø©
        base_emotions[primary_emotion] = random.uniform(0.65, 0.88)
        
        # Ø¥Ø¶Ø§ÙØ© Ù…Ø´Ø§Ø¹Ø± Ø«Ø§Ù†ÙˆÙŠØ© Ù…Ø±ØªØ¨Ø·Ø©
        emotion_correlations = {
            'joy': ['excitement', 'playfulness', 'confidence'],
            'curiosity': ['excitement', 'surprise', 'confidence'],
            'excitement': ['joy', 'curiosity', 'playfulness'],
            'calm': ['confidence', 'joy'],
            'sadness': ['calm', 'fear'],
            'playfulness': ['joy', 'excitement', 'surprise']
        }
        
        if primary_emotion in emotion_correlations:
            for secondary in emotion_correlations[primary_emotion][:2]:  # Ø£Ø®Ø° Ø£ÙˆÙ„ 2
                if secondary in base_emotions:
                    base_emotions[secondary] = random.uniform(0.15, 0.35)
        
        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ù‚ÙŠÙ…
        total = sum(base_emotions.values())
        normalized_emotions = {k: v/total for k, v in base_emotions.items()}
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø¶Ø¹ÙŠÙØ© Ø¬Ø¯Ø§Ù‹
        return {k: v for k, v in normalized_emotions.items() if v >= 0.05}

    # Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© ÙÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„ØªØ§Ù„ÙŠ Ù†Ø¸Ø±Ø§Ù‹ Ù„Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù...
#   A d d i n g   r e m a i n i n g   m e t h o d s   f o r   h i s t o r i c a l   d a t a   i n t e g r a t i o n . . . 
 
 