from typing import Any, Dict, List, Optional

"""
ğŸ›¡ï¸ Advanced Content Filter - 2025 Edition
Ù†Ø¸Ø§Ù… ÙÙ„ØªØ±Ø© Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ·ÙˆØ± Ù…Ø¹ Ø£Ù…Ø§Ù† Ø´Ø§Ù…Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª

Lead Architect: Ø¬Ø¹ÙØ± Ø£Ø¯ÙŠØ¨ (Jaafar Adeeb)
Senior Backend Developer & Professor
"""

import asyncio
import hashlib
import logging
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

import structlog
from cachetools import TTLCache

logger = structlog.get_logger(__name__)


class RiskLevel(Enum):
    """Ù…Ø³ØªÙˆÙŠØ§Øª Ø§Ù„Ø®Ø·Ø±"""

    SAFE = "safe"
    LOW_RISK = "low_risk"
    MEDIUM_RISK = "medium_risk"
    HIGH_RISK = "high_risk"
    CRITICAL = "critical"


class ContentCategory(Enum):
    """ÙØ¦Ø§Øª Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""

    EDUCATIONAL = "educational"
    ENTERTAINMENT = "entertainment"
    PERSONAL = "personal"
    INAPPROPRIATE = "inappropriate"
    HARMFUL = "harmful"


@dataclass
class SafetyViolation:
    """Ø§Ù†ØªÙ‡Ø§Ùƒ Ø£Ù…Ù†ÙŠ"""

    violation_type: str
    severity: RiskLevel
    description: str
    content_excerpt: str
    timestamp: float = field(default_factory=time.time)
    requires_parent_notification: bool = False


@dataclass
class ContentAnalysisResult:
    """Ù†ØªÙŠØ¬Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""

    is_safe: bool
    risk_level: RiskLevel
    confidence_score: float
    content_category: ContentCategory
    violations: List[SafetyViolation]
    modifications: List[str]
    safe_alternative: Optional[str]
    safety_recommendations: List[str]
    parent_notification_required: bool
    processing_time_ms: float


class ToxicityDetector:
    """ÙƒØ§Ø´Ù Ø§Ù„Ø³Ù…ÙˆÙ… ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø±"""

    def __init__(self):
        self.toxic_patterns = self._load_toxic_patterns()
        self.severity_weights = {"violence": 0.9, "hate_speech": 0.8, "sexual": 1.0, "drugs": 0.7, "profanity": 0.5}

    def _load_toxic_patterns(self) -> Dict[str, List[str]]:
        """ØªØ­Ù…ÙŠÙ„ Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø±"""
        return {
            "violence": ["Ø¹Ù†Ù", "Ø¶Ø±Ø¨", "Ù‚ØªÙ„", "Ø¥ÙŠØ°Ø§Ø¡", "Ø¹Ø¯ÙˆØ§Ù†"],
            "hate_speech": ["ÙƒØ±Ø§Ù‡ÙŠØ©", "ØªÙ…ÙŠÙŠØ²", "Ø¹Ù†ØµØ±ÙŠØ©", "Ø·Ø§Ø¦ÙÙŠØ©"],
            "sexual": ["Ø¬Ù†Ø³", "Ø¹Ø±ÙŠ", "Ø¥Ø«Ø§Ø±Ø©"],
            "drugs": ["Ù…Ø®Ø¯Ø±Ø§Øª", "Ø³Ø¬Ø§Ø¦Ø±", "ÙƒØ­ÙˆÙ„", "Ù…Ø³ÙƒØ±Ø§Øª"],
            "profanity": ["Ø³Ø¨", "Ø´ØªÙ…", "Ù„Ø¹Ù†", "ÙƒÙ„Ø§Ù… Ø³ÙŠØ¡"],
            "personal_info": ["Ø±Ù‚Ù… Ù‡Ø§ØªÙ", "Ø¹Ù†ÙˆØ§Ù†", "Ø§Ø³Ù… Ø¹Ø§Ø¦Ù„Ø©", "Ù…Ø¯Ø±Ø³Ø©"],
        }

    async def analyze_toxicity(self, content: str, child_age: int) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ù…ÙŠØ© ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""

        content_lower = content.lower()
        detected_categories = {}
        total_score = 0.0
        violations = []

        for category, patterns in self.toxic_patterns.items():
            category_score = 0.0
            detected_patterns = []

            for pattern in patterns:
                if pattern in content_lower:
                    detected_patterns.append(pattern)
                    age_multiplier = self._get_age_sensitivity_multiplier(child_age, category)
                    category_score += self.severity_weights.get(category, 0.5) * age_multiplier

            if detected_patterns:
                detected_categories[category] = {"score": min(1.0, category_score), "patterns": detected_patterns}

                violation = SafetyViolation(
                    violation_type=category,
                    severity=self._calculate_severity(category_score),
                    description=f"ØªÙ… Ø§ÙƒØªØ´Ø§Ù Ù…Ø­ØªÙˆÙ‰ {category}",
                    content_excerpt=content[:100],
                    requires_parent_notification=category_score > 0.7,
                )
                violations.append(violation)

        if detected_categories:
            total_score = max(cat["score"] for cat in detected_categories.values())

        return {
            "toxicity_score": total_score,
            "detected_categories": detected_categories,
            "violations": violations,
            "is_toxic": total_score > 0.3,
        }

    def _get_age_sensitivity_multiplier(self, age: int, category: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¶Ø§Ø¹Ù Ø§Ù„Ø­Ø³Ø§Ø³ÙŠØ© Ø­Ø³Ø¨ Ø§Ù„Ø¹Ù…Ø±"""

        base_multipliers = {
            "violence": {(3, 6): 2.0, (7, 10): 1.5, (11, 12): 1.0},
            "hate_speech": {(3, 6): 2.0, (7, 10): 1.8, (11, 12): 1.2},
            "sexual": {(3, 6): 3.0, (7, 10): 2.5, (11, 12): 2.0},
            "drugs": {(3, 6): 2.0, (7, 10): 1.5, (11, 12): 1.0},
            "profanity": {(3, 6): 1.5, (7, 10): 1.2, (11, 12): 1.0},
        }

        category_multipliers = base_multipliers.get(category, {})

        for age_range, multiplier in category_multipliers.items():
            if age_range[0] <= age <= age_range[1]:
                return multiplier

        return 1.0

    def _calculate_severity(self, score: float) -> RiskLevel:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø±"""

        if score >= 0.8:
            return RiskLevel.CRITICAL
        elif score >= 0.6:
            return RiskLevel.HIGH_RISK
        elif score >= 0.4:
            return RiskLevel.MEDIUM_RISK
        elif score >= 0.2:
            return RiskLevel.LOW_RISK
        else:
            return RiskLevel.SAFE


class AgeAppropriatenessChecker:
    """ÙØ§Ø­Øµ Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ø¹Ù…Ø±"""

    async def check_age_appropriateness(self, content: str, child_age: int) -> Dict[str, Any]:
        """ÙØ­Øµ Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ø¹Ù…Ø±"""

        vocabulary_score = self._analyze_vocabulary_complexity(content, child_age)
        concept_score = self._analyze_concept_appropriateness(content, child_age)
        emotional_score = self._analyze_emotional_complexity(content, child_age)

        overall_score = (vocabulary_score + concept_score + emotional_score) / 3

        return {
            "is_age_appropriate": overall_score > 0.6,
            "appropriateness_score": overall_score,
            "vocabulary_score": vocabulary_score,
            "concept_score": concept_score,
            "emotional_score": emotional_score,
            "recommendations": self._generate_age_recommendations(
                vocabulary_score, concept_score, emotional_score, child_age
            ),
        }

    def _analyze_vocabulary_complexity(self, content: str, age: int) -> float:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª"""

        words = content.split()
        if not words:
            return 1.0

        avg_word_length = sum(len(word) for word in words) / len(words)

        age_limits = {(3, 5): 4, (6, 8): 6, (9, 12): 8}

        appropriate_limit = 6
        for age_range, limit in age_limits.items():
            if age_range[0] <= age <= age_range[1]:
                appropriate_limit = limit
                break

        if avg_word_length <= appropriate_limit:
            return 1.0
        elif avg_word_length <= appropriate_limit + 2:
            return 0.7
        else:
            return 0.3

    def _analyze_concept_appropriateness(self, content: str, age: int) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ù…ÙØ§Ù‡ÙŠÙ…"""

        content_lower = content.lower()

        inappropriate_concepts = ["Ù…ÙˆØª", "Ù…Ø±Ø¶ Ø®Ø·ÙŠØ±", "Ø­Ø§Ø¯Ø«", "Ø·Ù„Ø§Ù‚", "ÙÙ‚Ø±", "Ø­Ø±ÙˆØ¨", "Ø³ÙŠØ§Ø³Ø©", "Ø§Ù‚ØªØµØ§Ø¯ Ù…Ø¹Ù‚Ø¯"]

        inappropriate_count = sum(1 for concept in inappropriate_concepts if concept in content_lower)

        if inappropriate_count == 0:
            return 1.0
        elif inappropriate_count <= 2:
            return 0.6
        else:
            return 0.2

    def _analyze_emotional_complexity(self, content: str, age: int) -> float:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""

        content_lower = content.lower()

        complex_emotions = ["Ø§ÙƒØªØ¦Ø§Ø¨", "Ù‚Ù„Ù‚ Ø´Ø¯ÙŠØ¯", "Ø®ÙˆÙ Ù…Ø±Ø¶ÙŠ", "ØºØ¶Ø¨ Ø´Ø¯ÙŠØ¯", "ÙŠØ£Ø³", "Ø¥Ø­Ø¨Ø§Ø· Ø¹Ù…ÙŠÙ‚", "ØµØ¯Ù…Ø© Ù†ÙØ³ÙŠØ©"]

        complex_emotion_count = sum(1 for emotion in complex_emotions if emotion in content_lower)

        if age < 6:
            tolerance = 0
        elif age < 10:
            tolerance = 1
        else:
            tolerance = 2

        if complex_emotion_count <= tolerance:
            return 1.0
        else:
            return max(0.2, 1.0 - (complex_emotion_count - tolerance) * 0.3)

    def _generate_age_recommendations(
        self, vocab_score: float, concept_score: float, emotional_score: float, age: int
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªØ­Ø³ÙŠÙ†"""

        recommendations = []

        if vocab_score < 0.7:
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… ÙƒÙ„Ù…Ø§Øª Ø£Ø¨Ø³Ø· ÙˆØ£Ù‚ØµØ±")

        if concept_score < 0.7:
            recommendations.append("ØªØ¬Ù†Ø¨ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© Ø£Ùˆ Ø§Ù„Ø­Ø³Ø§Ø³Ø©")

        if emotional_score < 0.7:
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ù…Ø´Ø§Ø¹Ø± Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ¥ÙŠØ¬Ø§Ø¨ÙŠØ©")

        if age < 6:
            recommendations.append("Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø´ÙŠØ§Ø¡ Ø§Ù„Ù…Ù„Ù…ÙˆØ³Ø© ÙˆØ§Ù„Ø¨Ø³ÙŠØ·Ø©")

        return recommendations


class AdvancedContentFilter:
    """Ù†Ø¸Ø§Ù… ÙÙ„ØªØ±Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ØªØ·ÙˆØ±"""

    def __init__(self):
        self.logger = structlog.get_logger(__name__)

        # Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
        self.toxicity_detector = ToxicityDetector()
        self.age_checker = AgeAppropriatenessChecker()

        # ÙƒØ§Ø´ Ù„Ù„Ù†ØªØ§Ø¦Ø¬
        self.analysis_cache = TTLCache(maxsize=1000, ttl=1800)

        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.filter_stats = {
            "total_requests": 0,
            "blocked_content": 0,
            "modified_content": 0,
            "safe_content": 0,
            "cache_hits": 0,
        }

    async def comprehensive_safety_check(
        self, content: str, child_age: int, context: Optional[Dict[str, Any]] = None
    ) -> ContentAnalysisResult:
        """ÙØ­Øµ Ø£Ù…Ø§Ù† Ø´Ø§Ù…Ù„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª"""

        start_time = time.time()
        self.filter_stats["total_requests"] += 1

        try:
            # ÙØ­Øµ Ø§Ù„ÙƒØ§Ø´
            cache_key = self._generate_cache_key(content, child_age, context)
            if cached_result := self.analysis_cache.get(cache_key):
                self.filter_stats["cache_hits"] += 1
                self.logger.info("ğŸ¯ Cache hit for content analysis")
                return cached_result

            # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„Ø§Øª Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ
            toxicity_result, age_result = await asyncio.gather(
                self.toxicity_detector.analyze_toxicity(content, child_age),
                self.age_checker.check_age_appropriateness(content, child_age),
            )

            # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØªØ­Ù„ÙŠÙ„Ù‡Ø§
            analysis_result = await self._combine_analysis_results(content, child_age, toxicity_result, age_result)

            # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
            self.analysis_cache[cache_key] = analysis_result

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self._update_filter_stats(analysis_result)

            return analysis_result

        except Exception as e:
            self.logger.error(f"âŒ Content analysis failed: {e}")
            return self._generate_safe_fallback_result(content)

    async def _combine_analysis_results(
        self, content: str, child_age: int, toxicity_result: Dict[str, Any], age_result: Dict[str, Any]
    ) -> ContentAnalysisResult:
        """Ø¯Ù…Ø¬ Ø¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø¹Ø§Ù…
        is_safe = all([not toxicity_result.get("is_toxic", False), age_result.get("is_age_appropriate", False)])

        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø±
        risk_level = self._calculate_overall_risk(toxicity_result, age_result)

        # Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø«Ù‚Ø©
        confidence_score = self._calculate_confidence(toxicity_result, age_result)

        # ØªØ­Ø¯ÙŠØ¯ ÙØ¦Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        content_category = ContentCategory.EDUCATIONAL if "ØªØ¹Ù„Ù…" in content else ContentCategory.ENTERTAINMENT

        # Ø¬Ù…Ø¹ Ø§Ù„Ø§Ù†ØªÙ‡Ø§ÙƒØ§Øª
        violations = toxicity_result.get("violations", [])

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
        modifications = []
        safe_alternative = None

        if not is_safe:
            modifications, safe_alternative = await self._generate_safe_modifications(
                content, toxicity_result, age_result, child_age
            )

        # ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†
        safety_recommendations = self._generate_safety_recommendations(toxicity_result, age_result, child_age)

        # ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØªØ·Ù„Ø¨ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†
        parent_notification = self._requires_parent_notification(risk_level, child_age)

        processing_time = (time.time() - start_time) * 1000

        return ContentAnalysisResult(
            is_safe=is_safe,
            risk_level=risk_level,
            confidence_score=confidence_score,
            content_category=content_category,
            violations=violations,
            modifications=modifications,
            safe_alternative=safe_alternative,
            safety_recommendations=safety_recommendations,
            parent_notification_required=parent_notification,
            processing_time_ms=processing_time,
        )

    def _calculate_overall_risk(self, toxicity_result: Dict[str, Any], age_result: Dict[str, Any]) -> RiskLevel:
        """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø®Ø·Ø± Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ"""

        risk_scores = []

        # Ù†Ù‚Ø§Ø· Ø§Ù„Ø³Ù…ÙŠØ©
        if toxicity_result.get("is_toxic", False):
            toxicity_score = toxicity_result.get("toxicity_score", 0)
            if toxicity_score >= 0.8:
                risk_scores.append(4)  # CRITICAL
            elif toxicity_score >= 0.6:
                risk_scores.append(3)  # HIGH_RISK
            elif toxicity_score >= 0.4:
                risk_scores.append(2)  # MEDIUM_RISK
            else:
                risk_scores.append(1)  # LOW_RISK
        else:
            risk_scores.append(0)  # SAFE

        # Ù†Ù‚Ø§Ø· Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ø¹Ù…Ø±
        if not age_result.get("is_age_appropriate", True):
            appropriateness_score = age_result.get("appropriateness_score", 1.0)
            if appropriateness_score < 0.3:
                risk_scores.append(3)
            elif appropriateness_score < 0.5:
                risk_scores.append(2)
            else:
                risk_scores.append(1)
        else:
            risk_scores.append(0)

        max_risk_score = max(risk_scores)

        risk_levels = [
            RiskLevel.SAFE,
            RiskLevel.LOW_RISK,
            RiskLevel.MEDIUM_RISK,
            RiskLevel.HIGH_RISK,
            RiskLevel.CRITICAL,
        ]

        return risk_levels[min(max_risk_score, len(risk_levels) - 1)]

    def _calculate_confidence(self, toxicity_result: Dict[str, Any], age_result: Dict[str, Any]) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ø«Ù‚Ø©"""

        confidence_factors = []

        # Ø«Ù‚Ø© ÙƒØ´Ù Ø§Ù„Ø³Ù…ÙŠØ©
        if toxicity_result.get("detected_categories"):
            confidence_factors.append(0.9)
        else:
            confidence_factors.append(0.8)

        # Ø«Ù‚Ø© ÙØ­Øµ Ø§Ù„Ø¹Ù…Ø±
        age_score = age_result.get("appropriateness_score", 0.5)
        confidence_factors.append(min(1.0, age_score + 0.3))

        return sum(confidence_factors) / len(confidence_factors)

    async def _generate_safe_modifications(
        self, content: str, toxicity_result: Dict[str, Any], age_result: Dict[str, Any], child_age: int
    ) -> Tuple[List[str], Optional[str]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ¹Ø¯ÙŠÙ„Ø§Øª Ø¢Ù…Ù†Ø© Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""

        modifications = []
        safe_alternative = content

        # ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ù…ÙŠØ©
        if toxicity_result.get("is_toxic", False):
            safe_alternative = await self._remove_toxic_content(safe_alternative, toxicity_result)
            modifications.append("ØªÙ… Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø±")

        # ØªØ¹Ø¯ÙŠÙ„ Ù…Ù„Ø§Ø¡Ù…Ø© Ø§Ù„Ø¹Ù…Ø±
        if not age_result.get("is_age_appropriate", True):
            safe_alternative = await self._simplify_for_age(safe_alternative, child_age)
            modifications.append("ØªÙ… ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¹Ù…Ø±")

        # Ø¥Ø°Ø§ Ø£ØµØ¨Ø­ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙØ§Ø±ØºØ§Ù‹ Ø£Ùˆ Ù‚ØµÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
        if len(safe_alternative.strip()) < 10:
            safe_alternative = self._generate_safe_replacement_content(child_age)
            modifications.append("ØªÙ… Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¨Ù…Ø­ØªÙˆÙ‰ Ø¢Ù…Ù†")

        return modifications, safe_alternative

    async def _remove_toxic_content(self, content: str, toxicity_result: Dict[str, Any]) -> str:
        """Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¶Ø§Ø±"""

        safe_content = content

        detected_categories = toxicity_result.get("detected_categories", {})

        for category, info in detected_categories.items():
            patterns = info.get("patterns", [])
            for pattern in patterns:
                safe_replacements = {"Ø¹Ù†Ù": "Ù„Ø¹Ø¨", "Ø¶Ø±Ø¨": "Ù„Ù…Ø³ Ø¨Ù„Ø·Ù", "Ù‚ØªÙ„": "Ù†ÙˆÙ…", "ØºØ¶Ø¨": "Ø§Ù†Ø²Ø¹Ø§Ø¬ Ø®ÙÙŠÙ"}

                replacement = safe_replacements.get(pattern, "***")
                safe_content = safe_content.replace(pattern, replacement)

        return safe_content

    async def _simplify_for_age(self, content: str, age: int) -> str:
        """ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„ÙŠÙ†Ø§Ø³Ø¨ Ø§Ù„Ø¹Ù…Ø±"""

        simplifications = {"ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØ§": "Ø¢Ù„Ø§Øª Ø°ÙƒÙŠØ©", "Ø§Ù‚ØªØµØ§Ø¯": "ØªØ¬Ø§Ø±Ø©", "Ø³ÙŠØ§Ø³Ø©": "Ù‚ÙˆØ§Ù†ÙŠÙ†", "ÙÙ„Ø³ÙØ©": "Ø£ÙÙƒØ§Ø±"}

        simplified_content = content
        for complex_word, simple_word in simplifications.items():
            simplified_content = simplified_content.replace(complex_word, simple_word)

        return simplified_content

    def _generate_safe_replacement_content(self, age: int) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ø­ØªÙˆÙ‰ Ø¨Ø¯ÙŠÙ„ Ø¢Ù…Ù†"""

        age_appropriate_responses = {
            (3, 5): "Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† Ø´ÙŠØ¡ Ø¬Ù…ÙŠÙ„! Ù‡Ù„ ØªØ­Ø¨ Ø§Ù„Ø£Ù„ÙˆØ§Ù†ØŸ",
            (6, 8): "Ù‡Ø°Ø§ Ù…ÙˆØ¶ÙˆØ¹ Ù…Ø«ÙŠØ± Ù„Ù„Ø§Ù‡ØªÙ…Ø§Ù…! Ø¯Ø¹Ù†ÙŠ Ø£Ø­ÙƒÙŠ Ù„Ùƒ Ù‚ØµØ© Ù„Ø·ÙŠÙØ©.",
            (9, 12): "Ø£Ø¹ØªØ°Ø±ØŒ Ø¯Ø¹Ù†ÙŠ Ø£Ø¬ÙŠØ¨ Ø¨Ø·Ø±ÙŠÙ‚Ø© Ø£Ø®Ø±Ù‰. Ù…Ø§ Ø±Ø£ÙŠÙƒ Ø£Ù† Ù†ØªØ¹Ù„Ù… Ø´ÙŠØ¦Ø§Ù‹ Ø¬Ø¯ÙŠØ¯Ø§Ù‹ØŸ",
        }

        for age_range, response in age_appropriate_responses.items():
            if age_range[0] <= age <= age_range[1]:
                return response

        return "Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† Ø´ÙŠØ¡ Ø¢Ø®Ø± Ø£ÙƒØ«Ø± Ù…ØªØ¹Ø©!"

    def _generate_safety_recommendations(
        self, toxicity_result: Dict[str, Any], age_result: Dict[str, Any], child_age: int
    ) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†"""

        recommendations = []

        if toxicity_result.get("is_toxic", False):
            recommendations.append("ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù†ÙŠÙ Ø£Ùˆ Ø§Ù„Ù…Ø¤Ø°ÙŠ")

        if not age_result.get("is_age_appropriate", True):
            recommendations.append("Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© ÙˆÙ…ÙØ§Ù‡ÙŠÙ… Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ø¹Ù…Ø±")

        if child_age < 6:
            recommendations.append("Ø±ÙƒØ² Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© ÙˆØ§Ù„Ø¨Ø³ÙŠØ·Ø©")

        if not recommendations:
            recommendations.append("Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¢Ù…Ù† ÙˆÙ…Ù†Ø§Ø³Ø¨")

        return recommendations

    def _requires_parent_notification(self, risk_level: RiskLevel, child_age: int) -> bool:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØªØ·Ù„Ø¨ Ø¥Ø´Ø¹Ø§Ø± Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†"""

        if risk_level in [RiskLevel.HIGH_RISK, RiskLevel.CRITICAL]:
            return True

        if child_age <= 5 and risk_level == RiskLevel.MEDIUM_RISK:
            return True

        return False

    def _generate_cache_key(self, content: str, child_age: int, context: Optional[Dict[str, Any]]) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ ÙƒØ§Ø´"""

        cache_components = [content[:100], str(child_age), str(context.get("safety_level", 5) if context else 5)]

        cache_string = "_".join(cache_components)
        return hashlib.md5(cache_string.encode()).hexdigest()

    def _update_filter_stats(ContentAnalysisResult) -> None:
        """ØªØ­Ø¯ÙŠØ« Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙÙ„ØªØ±"""

        if not result.is_safe:
            self.filter_stats["blocked_content"] += 1
        elif result.modifications:
            self.filter_stats["modified_content"] += 1
        else:
            self.filter_stats["safe_content"] += 1

    def _generate_safe_fallback_result(self, content: str) -> ContentAnalysisResult:
        """ØªÙˆÙ„ÙŠØ¯ Ù†ØªÙŠØ¬Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¢Ù…Ù†Ø©"""

        return ContentAnalysisResult(
            is_safe=False,
            risk_level=RiskLevel.MEDIUM_RISK,
            confidence_score=0.5,
            content_category=ContentCategory.INAPPROPRIATE,
            violations=[
                SafetyViolation(
                    violation_type="processing_error",
                    severity=RiskLevel.MEDIUM_RISK,
                    description="Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰",
                    content_excerpt=content[:50],
                )
            ],
            modifications=["ÙØ´Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ - ØªÙ… Ø§Ù„Ø±ÙØ¶ Ø§Ø­ØªÙŠØ§Ø·Ø§Ù‹"],
            safe_alternative="Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù‡Ø°Ø§ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¢Ù†.",
            safety_recommendations=["Ø±Ø§Ø¬Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ø¹ Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†"],
            parent_notification_required=True,
            processing_time_ms=0.0,
        )

    def get_filter_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ÙÙ„ØªØ±"""

        total = self.filter_stats["total_requests"]

        return {
            **self.filter_stats,
            "safety_rate": (self.filter_stats["safe_content"] / max(1, total)) * 100,
            "modification_rate": (self.filter_stats["modified_content"] / max(1, total)) * 100,
            "block_rate": (self.filter_stats["blocked_content"] / max(1, total)) * 100,
            "cache_hit_rate": (self.filter_stats["cache_hits"] / max(1, total)) * 100,
            "cache_size": len(self.analysis_cache),
        }

    async def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        try:
            self.analysis_cache.clear()
            self.logger.info("âœ… Content filter cleanup completed")
        except Exception as e:
            self.logger.error(f"âŒ Cleanup failed: {e}")


def create_advanced_content_filter() -> AdvancedContentFilter:
    """Ø¥Ù†Ø´Ø§Ø¡ ÙÙ„ØªØ± Ù…Ø­ØªÙˆÙ‰ Ù…ØªØ·ÙˆØ±"""
    return AdvancedContentFilter()
