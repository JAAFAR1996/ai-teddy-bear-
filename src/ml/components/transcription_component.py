# ===================================================================
# ðŸŽ¤ AI Teddy Bear - Speech Transcription Component
# Child-Optimized Speech Recognition
# AI Team Lead: Senior AI Engineer
# Date: January 2025
# ===================================================================

from kubeflow.dsl import component, Input, Output, Dataset, Artifact
import logging
from typing import Dict, List, Optional, Tuple
import json
from datetime import datetime

logger = logging.getLogger(__name__)

@component(
    base_image="python:3.11-slim",
    packages_to_install=[
        "openai-whisper==20231117",
        "torch==2.1.0",
        "torchaudio==2.1.0",
        "transformers==4.36.0",
        "numpy==1.24.3",
        "librosa==0.10.1",
        "webrtcvad==2.0.10"
    ]
)
def transcribe_audio_op(
    audio: Input[Dataset],
    transcription: Output[Dataset],
    confidence_metrics: Output[Artifact],
    language_model: str = "whisper-large-v3",
    child_optimized: bool = True,
    target_language: str = "auto"
) -> Dict[str, any]:
    """
    ØªØ­ÙˆÙŠÙ„ Ø§Ù„ÙƒÙ„Ø§Ù… Ø¥Ù„Ù‰ Ù†Øµ Ù…Ø­Ø³Ù† Ù„Ù„Ø£Ø·ÙØ§Ù„
    """
    import whisper
    import torch
    import torchaudio
    import numpy as np
    from transformers import pipeline
    
    logger.info(f"Starting transcription with model: {language_model}")
    
    try:
        # ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Whisper Ù…Ø­Ø³Ù† Ù„Ù„Ø£Ø·ÙØ§Ù„
        model = whisper.load_model("large-v3")
        
        # ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙˆØª
        audio_data, sample_rate = torchaudio.load(audio.path)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø£ÙˆÙ„ÙŠØ© Ù„Ù„ØµÙˆØª
        if child_optimized:
            audio_data = optimize_for_child_speech(audio_data, sample_rate)
        
        # ÙƒØ´Ù Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ
        voice_segments = detect_voice_activity(audio_data, sample_rate)
        
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ numpy Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¹ Whisper
        audio_np = audio_data.squeeze().numpy()
        
        # Ø¥Ø¹Ø¯Ø§Ø¯ Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØªØ­ÙˆÙŠÙ„
        transcribe_options = {
            "language": None if target_language == "auto" else target_language,
            "task": "transcribe",
            "temperature": 0.0,  # Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ù…Ø­Ø¯Ø¯Ø©
            "best_of": 3,        # Ø¬Ø±Ø¨ Ø¹Ø¯Ø© Ù…Ø±Ø§Øª ÙˆØ§Ø®ØªØ± Ø§Ù„Ø£ÙØ¶Ù„
            "beam_size": 5,      # Ø¨Ø­Ø« Ø£ÙˆØ³Ø¹
            "patience": 1.0,
            "suppress_tokens": [-1],  # Ù„Ø§ ØªÙ‚Ù…Ø¹ Ø£ÙŠ Ø±Ù…ÙˆØ²
            "initial_prompt": "This is a child speaking clearly and naturally.",
        }
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­ÙˆÙŠÙ„
        result = model.transcribe(
            audio_np,
            **transcribe_options
        )
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        processed_result = process_transcription_result(
            result, 
            voice_segments,
            child_optimized
        )
        
        # ØªØ­Ø³ÙŠÙ†Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø£Ø·ÙØ§Ù„
        if child_optimized:
            processed_result = apply_child_speech_corrections(processed_result)
        
        # ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„
        quality_metrics = assess_transcription_quality(
            result, 
            audio_data, 
            sample_rate
        )
        
        # Ø­ÙØ¸ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
        final_result = {
            'text': processed_result['text'],
            'segments': processed_result['segments'],
            'language': result.get('language', 'unknown'),
            'confidence': quality_metrics['overall_confidence'],
            'processing_metadata': {
                'model_used': language_model,
                'child_optimized': child_optimized,
                'voice_segments_detected': len(voice_segments),
                'processing_timestamp': datetime.now().isoformat(),
                'audio_duration': float(len(audio_np) / sample_rate)
            }
        }
        
        with open(transcription.path, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=2, ensure_ascii=False)
        
        # Ø­ÙØ¸ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©
        with open(confidence_metrics.path, 'w') as f:
            json.dump(quality_metrics, f, indent=2)
        
        logger.info(f"Transcription completed: '{processed_result['text'][:50]}...'")
        return final_result
        
    except Exception as e:
        logger.error(f"Transcription failed: {str(e)}")
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø·ÙˆØ§Ø±Ø¦
        emergency_result = {
            'text': "",
            'confidence': 0.0,
            'error': str(e),
            'processing_timestamp': datetime.now().isoformat()
        }
        
        with open(transcription.path, 'w') as f:
            json.dump(emergency_result, f, indent=2)
        
        raise


def optimize_for_child_speech(audio: torch.Tensor, sample_rate: int) -> torch.Tensor:
    """ØªØ­Ø³ÙŠÙ† Ø§Ù„ØµÙˆØª Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø£Ø·ÙØ§Ù„"""
    
    # ØªØ·Ø¨ÙŠÙ‚ ÙÙ„ØªØ± Ù„ØªØ­Ø³ÙŠÙ† ØªØ±Ø¯Ø¯Ø§Øª Ø§Ù„ÙƒÙ„Ø§Ù… Ù„Ù„Ø£Ø·ÙØ§Ù„
    # Ø§Ù„Ø£Ø·ÙØ§Ù„ Ù„Ø¯ÙŠÙ‡Ù… ØªØ±Ø¯Ø¯Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ© Ø£Ø¹Ù„Ù‰ Ù…Ù† Ø§Ù„Ø¨Ø§Ù„ØºÙŠÙ†
    
    import torchaudio.transforms as T
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ±Ø¯Ø¯Ø§Øª Ø§Ù„Ù…ØªÙˆØ³Ø·Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© (300-3000 Hz)
    highpass = T.HighpassBiquad(sample_rate, cutoff_freq=200.0)
    lowpass = T.LowpassBiquad(sample_rate, cutoff_freq=4000.0)
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
    audio = highpass(audio)
    audio = lowpass(audio)
    
    # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„ØµÙˆØª
    audio = audio / torch.max(torch.abs(audio)) * 0.9
    
    return audio


def detect_voice_activity(audio: torch.Tensor, sample_rate: int) -> List[Tuple[float, float]]:
    """ÙƒØ´Ù ÙØªØ±Ø§Øª Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„ØµÙˆØªÙŠ"""
    import webrtcvad
    
    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù€ WebRTC VAD
    audio_np = audio.squeeze().numpy()
    
    # Ø¥Ø¹Ø§Ø¯Ø© Ø¹ÙŠÙ†Ø© Ø¥Ù„Ù‰ 16kHz Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
    if sample_rate != 16000:
        import librosa
        audio_np = librosa.resample(audio_np, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    
    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ 16-bit integers
    audio_int16 = (audio_np * 32767).astype(np.int16)
    
    # Ø¥Ø¹Ø¯Ø§Ø¯ VAD
    vad = webrtcvad.Vad(2)  # Ù…Ø³ØªÙˆÙ‰ Ø­Ø³Ø§Ø³ÙŠØ© Ù…ØªÙˆØ³Ø·
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØµÙˆØª ÙÙŠ Ø¥Ø·Ø§Ø±Ø§Øª 30ms
    frame_duration = 30  # ms
    frame_length = int(sample_rate * frame_duration / 1000)
    
    voice_segments = []
    current_segment_start = None
    
    for i in range(0, len(audio_int16) - frame_length, frame_length):
        frame = audio_int16[i:i + frame_length].tobytes()
        
        if vad.is_speech(frame, sample_rate):
            if current_segment_start is None:
                current_segment_start = i / sample_rate
        else:
            if current_segment_start is not None:
                voice_segments.append((current_segment_start, i / sample_rate))
                current_segment_start = None
    
    # Ø¥Ø¶Ø§ÙØ© Ø¢Ø®Ø± segment Ø¥Ø°Ø§ Ù„Ù… ÙŠÙ†ØªÙ‡
    if current_segment_start is not None:
        voice_segments.append((current_segment_start, len(audio_int16) / sample_rate))
    
    return voice_segments


def process_transcription_result(
    result: Dict, 
    voice_segments: List[Tuple[float, float]], 
    child_optimized: bool
) -> Dict:
    """Ù…Ø¹Ø§Ù„Ø¬Ø© Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­ÙˆÙŠÙ„"""
    
    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
    text = result['text'].strip()
    
    # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù‚Ø·Ø¹ (segments)
    segments = []
    for segment in result.get('segments', []):
        processed_segment = {
            'start': segment['start'],
            'end': segment['end'],
            'text': segment['text'].strip(),
            'confidence': segment.get('avg_logprob', 0.0),
            'words': segment.get('words', [])
        }
        segments.append(processed_segment)
    
    # ØªØ·Ø¨ÙŠÙ‚ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø®Ø§ØµØ© Ø¨Ø§Ù„Ø£Ø·ÙØ§Ù„
    if child_optimized:
        text = improve_child_transcription(text)
        segments = [improve_segment_transcription(seg) for seg in segments]
    
    return {
        'text': text,
        'segments': segments,
        'voice_activity_periods': voice_segments
    }


def improve_child_transcription(text: str) -> str:
    """ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø­ÙˆÙ„ Ù„Ù„Ø£Ø·ÙØ§Ù„"""
    
    # Ù‚Ø§Ù…ÙˆØ³ ØªØµØ­ÙŠØ­Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø£Ø·ÙØ§Ù„
    child_corrections = {
        # Ø£Ø®Ø·Ø§Ø¡ Ø§Ù„Ù†Ø·Ù‚ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©
        'wike': 'like',
        'pwease': 'please',
        'fwend': 'friend',
        'bwother': 'brother',
        'sistew': 'sister',
        'wove': 'love',
        'vewy': 'very',
        'wittle': 'little',
        'hewp': 'help',
        'wat': 'what',
        'wight': 'right',
        'wead': 'read',
        'wun': 'run',
        'wed': 'red',
        
        # ÙƒÙ„Ù…Ø§Øª Ù…Ø®ØªØµØ±Ø© Ø´Ø§Ø¦Ø¹Ø©
        'gonna': 'going to',
        'wanna': 'want to',
        'dunno': "don't know",
        'gimme': 'give me',
        'lemme': 'let me',
        
        # ØªØµØ­ÙŠØ­Ø§Øª Ù†Ø­ÙˆÙŠØ©
        'me want': 'I want',
        'me like': 'I like',
        'me go': 'I go',
        'me see': 'I see',
    }
    
    # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ØªØµØ­ÙŠØ­Ø§Øª
    corrected_text = text
    for wrong, correct in child_corrections.items():
        corrected_text = corrected_text.replace(wrong, correct)
    
    # ØªÙ†Ø¸ÙŠÙ Ø¥Ø¶Ø§ÙÙŠ
    corrected_text = ' '.join(corrected_text.split())  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
    
    return corrected_text


def improve_segment_transcription(segment: Dict) -> Dict:
    """ØªØ­Ø³ÙŠÙ† segment ÙˆØ§Ø­Ø¯"""
    segment['text'] = improve_child_transcription(segment['text'])
    return segment


def apply_child_speech_corrections(result: Dict) -> Dict:
    """ØªØ·Ø¨ÙŠÙ‚ ØªØµØ­ÙŠØ­Ø§Øª Ø´Ø§Ù…Ù„Ø© Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø£Ø·ÙØ§Ù„"""
    
    # ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†Øµ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ
    result['text'] = improve_child_transcription(result['text'])
    
    # ØªØ­Ø³ÙŠÙ† ÙƒÙ„ segment
    if 'segments' in result:
        result['segments'] = [
            improve_segment_transcription(seg) 
            for seg in result['segments']
        ]
    
    # Ø¥Ø¶Ø§ÙØ© Ø¹Ù„Ø§Ù…Ø§Øª Ø«Ù‚Ø© Ù…Ø­Ø³Ù†Ø© Ù„Ù„Ø£Ø·ÙØ§Ù„
    result['child_speech_confidence'] = calculate_child_speech_confidence(result)
    
    return result


def calculate_child_speech_confidence(result: Dict) -> float:
    """Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© ÙÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„ Ù„ÙƒÙ„Ø§Ù… Ø§Ù„Ø£Ø·ÙØ§Ù„"""
    
    base_confidence = 0.7  # Ø«Ù‚Ø© Ø£Ø³Ø§Ø³ÙŠØ©
    
    # ØªØ­Ù„ÙŠÙ„ Ø¹ÙˆØ§Ù…Ù„ Ø§Ù„Ø¬ÙˆØ¯Ø©
    text = result['text']
    
    # Ø·ÙˆÙ„ Ø§Ù„Ù†Øµ (Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø© Ù‚Ø¯ ØªÙƒÙˆÙ† Ø£Ù‚Ù„ Ø¯Ù‚Ø©)
    length_factor = min(1.0, len(text.split()) / 5.0)
    
    # ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ù„Ù„Ø£Ø·ÙØ§Ù„
    child_words = ['mama', 'papa', 'toy', 'play', 'fun', 'happy', 'want', 'like']
    child_word_count = sum(1 for word in child_words if word in text.lower())
    child_factor = min(1.0, child_word_count / 3.0)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    final_confidence = base_confidence * (0.4 + 0.3 * length_factor + 0.3 * child_factor)
    
    return max(0.1, min(1.0, final_confidence))


def assess_transcription_quality(
    result: Dict, 
    audio: torch.Tensor, 
    sample_rate: int
) -> Dict:
    """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„"""
    
    # Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¬ÙˆØ¯Ø©
    text = result['text']
    segments = result.get('segments', [])
    
    # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø£Ø³Ø§Ø³ÙŠØ©
    word_count = len(text.split())
    segment_count = len(segments)
    avg_confidence = np.mean([seg.get('avg_logprob', 0.0) for seg in segments]) if segments else 0.0
    
    # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØµÙ…Øª Ø¥Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…
    audio_duration = len(audio.squeeze()) / sample_rate
    speech_duration = sum(seg['end'] - seg['start'] for seg in segments)
    speech_ratio = speech_duration / audio_duration if audio_duration > 0 else 0.0
    
    # ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª
    audio_quality = assess_audio_quality(audio, sample_rate)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
    overall_confidence = calculate_overall_confidence(
        avg_confidence, speech_ratio, audio_quality, word_count
    )
    
    return {
        'overall_confidence': overall_confidence,
        'word_count': word_count,
        'segment_count': segment_count,
        'average_segment_confidence': avg_confidence,
        'speech_to_silence_ratio': speech_ratio,
        'audio_quality_score': audio_quality,
        'estimated_accuracy': estimate_transcription_accuracy(overall_confidence),
        'quality_indicators': {
            'sufficient_speech': speech_ratio > 0.3,
            'clear_audio': audio_quality > 0.7,
            'meaningful_length': word_count >= 3,
            'high_confidence': overall_confidence > 0.8
        }
    }


def assess_audio_quality(audio: torch.Tensor, sample_rate: int) -> float:
    """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØµÙˆØª"""
    
    # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø¶ÙˆØ¶Ø§Ø¡
    signal_power = torch.mean(audio ** 2)
    noise_estimate = torch.std(audio[-int(0.1 * sample_rate):])  # Ø¢Ø®Ø± 0.1 Ø«Ø§Ù†ÙŠØ©
    
    if noise_estimate > 0:
        snr = 10 * torch.log10(signal_power / (noise_estimate ** 2))
        snr_score = torch.clamp(snr / 30.0, 0.0, 1.0)  # ØªØ·Ø¨ÙŠØ¹ Ø¥Ù„Ù‰ 0-1
    else:
        snr_score = 0.5
    
    # ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·ÙŠÙÙŠ
    spectral_score = assess_spectral_quality(audio, sample_rate)
    
    # Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
    overall_quality = 0.6 * float(snr_score) + 0.4 * spectral_score
    
    return overall_quality


def assess_spectral_quality(audio: torch.Tensor, sample_rate: int) -> float:
    """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø·ÙŠÙÙŠ"""
    
    # ØªØ­Ù„ÙŠÙ„ FFT Ø¨Ø³ÙŠØ·
    fft = torch.fft.fft(audio.squeeze())
    magnitude = torch.abs(fft)
    
    # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø·Ø§Ù‚Ø© ÙÙŠ Ù†Ø·Ø§Ù‚ Ø§Ù„ÙƒÙ„Ø§Ù… (300-3400 Hz)
    freqs = torch.fft.fftfreq(len(fft), 1/sample_rate)
    speech_range_mask = (torch.abs(freqs) >= 300) & (torch.abs(freqs) <= 3400)
    
    speech_energy = torch.sum(magnitude[speech_range_mask])
    total_energy = torch.sum(magnitude)
    
    if total_energy > 0:
        speech_ratio = speech_energy / total_energy
        return float(speech_ratio)
    
    return 0.5


def calculate_overall_confidence(
    avg_confidence: float, 
    speech_ratio: float, 
    audio_quality: float, 
    word_count: int
) -> float:
    """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„ØªØ­ÙˆÙŠÙ„"""
    
    # ØªØ·Ø¨ÙŠØ¹ avg_confidence (Ù…Ù† log probability Ø¥Ù„Ù‰ 0-1)
    normalized_confidence = max(0.0, min(1.0, (avg_confidence + 1.0) / 1.0))
    
    # ØªØ·Ø¨ÙŠØ¹ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ù…Ø§Øª
    word_factor = min(1.0, word_count / 10.0)
    
    # Ø­Ø³Ø§Ø¨ Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ
    overall = (
        0.4 * normalized_confidence +
        0.2 * speech_ratio +
        0.3 * audio_quality +
        0.1 * word_factor
    )
    
    return max(0.1, min(1.0, overall))


def estimate_transcription_accuracy(confidence: float) -> str:
    """ØªÙ‚Ø¯ÙŠØ± Ø¯Ù‚Ø© Ø§Ù„ØªØ­ÙˆÙŠÙ„"""
    
    if confidence >= 0.9:
        return "excellent"
    elif confidence >= 0.8:
        return "very_good"
    elif confidence >= 0.7:
        return "good"
    elif confidence >= 0.6:
        return "fair"
    elif confidence >= 0.4:
        return "poor"
    else:
        return "very_poor" 