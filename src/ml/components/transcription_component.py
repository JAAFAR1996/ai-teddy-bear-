# ===================================================================
# ๐ค AI Teddy Bear - Speech Transcription Component
# Child-Optimized Speech Recognition
# AI Team Lead: Senior AI Engineer
# Date: January 2025
# ===================================================================

from kubeflow.dsl import component, Input, Output, Dataset, Artifact
import logging
from typing import Dict, List, Optional, Tuple
import json
from datetime import datetime

logger = logging.getLogger(__name__)

@component(
    base_image="python:3.11-slim",
    packages_to_install=[
        "openai-whisper==20231117",
        "torch==2.1.0",
        "torchaudio==2.1.0",
        "transformers==4.36.0",
        "numpy==1.24.3",
        "librosa==0.10.1",
        "webrtcvad==2.0.10"
    ]
)
def transcribe_audio_op(
    audio: Input[Dataset],
    transcription: Output[Dataset],
    confidence_metrics: Output[Artifact],
    language_model: str = "whisper-large-v3",
    child_optimized: bool = True,
    target_language: str = "auto"
) -> Dict[str, any]:
    """
    ุชุญููู ุงูููุงู ุฅูู ูุต ูุญุณู ููุฃุทูุงู
    """
    import whisper
    import torch
    import torchaudio
    import numpy as np
    from transformers import pipeline
    
    logger.info(f"Starting transcription with model: {language_model}")
    
    try:
        # ุชุญููู ูููุฐุฌ Whisper ูุญุณู ููุฃุทูุงู
        model = whisper.load_model("large-v3")
        
        # ุชุญููู ุงูุตูุช
        audio_data, sample_rate = torchaudio.load(audio.path)
        
        # ูุนุงูุฌุฉ ุฃูููุฉ ููุตูุช
        if child_optimized:
            audio_data = optimize_for_child_speech(audio_data, sample_rate)
        
        # ูุดู ุงููุดุงุท ุงูุตูุชู
        voice_segments = detect_voice_activity(audio_data, sample_rate)
        
        # ุชุญููู ุฅูู numpy ูููุนุงูุฌุฉ ูุน Whisper
        audio_np = audio_data.squeeze().numpy()
        
        # ุฅุนุฏุงุฏ ุฎูุงุฑุงุช ุงูุชุญููู
        transcribe_options = {
            "language": None if target_language == "auto" else target_language,
            "task": "transcribe",
            "temperature": 0.0,  # ููุญุตูู ุนูู ูุชุงุฆุฌ ูุญุฏุฏุฉ
            "best_of": 3,        # ุฌุฑุจ ุนุฏุฉ ูุฑุงุช ูุงุฎุชุฑ ุงูุฃูุถู
            "beam_size": 5,      # ุจุญุซ ุฃูุณุน
            "patience": 1.0,
            "suppress_tokens": [-1],  # ูุง ุชููุน ุฃู ุฑููุฒ
            "initial_prompt": "This is a child speaking clearly and naturally.",
        }
        
        # ุชูููุฐ ุงูุชุญููู
        result = model.transcribe(
            audio_np,
            **transcribe_options
        )
        
        # ูุนุงูุฌุฉ ุงููุชุงุฆุฌ
        processed_result = process_transcription_result(
            result, 
            voice_segments,
            child_optimized
        )
        
        # ุชุญุณููุงุช ุฎุงุตุฉ ุจุงูุฃุทูุงู
        if child_optimized:
            processed_result = apply_child_speech_corrections(processed_result)
        
        # ุชูููู ุฌูุฏุฉ ุงูุชุญููู
        quality_metrics = assess_transcription_quality(
            result, 
            audio_data, 
            sample_rate
        )
        
        # ุญูุธ ุงููุชูุฌุฉ ุงูููุงุฆูุฉ
        final_result = {
            'text': processed_result['text'],
            'segments': processed_result['segments'],
            'language': result.get('language', 'unknown'),
            'confidence': quality_metrics['overall_confidence'],
            'processing_metadata': {
                'model_used': language_model,
                'child_optimized': child_optimized,
                'voice_segments_detected': len(voice_segments),
                'processing_timestamp': datetime.now().isoformat(),
                'audio_duration': float(len(audio_np) / sample_rate)
            }
        }
        
        with open(transcription.path, 'w', encoding='utf-8') as f:
            json.dump(final_result, f, indent=2, ensure_ascii=False)
        
        # ุญูุธ ููุงููุณ ุงูุฌูุฏุฉ
        with open(confidence_metrics.path, 'w') as f:
            json.dump(quality_metrics, f, indent=2)
        
        logger.info(f"Transcription completed: '{processed_result['text'][:50]}...'")
        return final_result
        
    except Exception as e:
        logger.error(f"Transcription failed: {str(e)}")
        # ุฅูุดุงุก ูุชูุฌุฉ ุทูุงุฑุฆ
        emergency_result = {
            'text': "",
            'confidence': 0.0,
            'error': str(e),
            'processing_timestamp': datetime.now().isoformat()
        }
        
        with open(transcription.path, 'w') as f:
            json.dump(emergency_result, f, indent=2)
        
        raise


def optimize_for_child_speech(audio: torch.Tensor, sample_rate: int) -> torch.Tensor:
    """ุชุญุณูู ุงูุตูุช ูููุงู ุงูุฃุทูุงู"""
    
    # ุชุทุจูู ููุชุฑ ูุชุญุณูู ุชุฑุฏุฏุงุช ุงูููุงู ููุฃุทูุงู
    # ุงูุฃุทูุงู ูุฏููู ุชุฑุฏุฏุงุช ุฃุณุงุณูุฉ ุฃุนูู ูู ุงูุจุงูุบูู
    
    import torchaudio.transforms as T
    
    # ุชุญุณูู ุงูุชุฑุฏุฏุงุช ุงููุชูุณุทุฉ ุงูุนุงููุฉ (300-3000 Hz)
    highpass = T.HighpassBiquad(sample_rate, cutoff_freq=200.0)
    lowpass = T.LowpassBiquad(sample_rate, cutoff_freq=4000.0)
    
    # ุชุทุจูู ุงูููุงุชุฑ
    audio = highpass(audio)
    audio = lowpass(audio)
    
    # ุชุทุจูุน ุงูุตูุช
    audio = audio / torch.max(torch.abs(audio)) * 0.9
    
    return audio


def detect_voice_activity(audio: torch.Tensor, sample_rate: int) -> List[Tuple[float, float]]:
    """ูุดู ูุชุฑุงุช ุงููุดุงุท ุงูุตูุชู"""
    import webrtcvad
    
    # ุชุญููู ุฅูู ุชูุณูู ููุงุณุจ ูู WebRTC VAD
    audio_np = audio.squeeze().numpy()
    
    # ุฅุนุงุฏุฉ ุนููุฉ ุฅูู 16kHz ุฅุฐุง ูุฒู ุงูุฃูุฑ
    if sample_rate != 16000:
        import librosa
        audio_np = librosa.resample(audio_np, orig_sr=sample_rate, target_sr=16000)
        sample_rate = 16000
    
    # ุชุญููู ุฅูู 16-bit integers
    audio_int16 = (audio_np * 32767).astype(np.int16)
    
    # ุฅุนุฏุงุฏ VAD
    vad = webrtcvad.Vad(2)  # ูุณุชูู ุญุณุงุณูุฉ ูุชูุณุท
    
    # ุชุญููู ุงูุตูุช ูู ุฅุทุงุฑุงุช 30ms
    frame_duration = 30  # ms
    frame_length = int(sample_rate * frame_duration / 1000)
    
    voice_segments = []
    current_segment_start = None
    
    for i in range(0, len(audio_int16) - frame_length, frame_length):
        frame = audio_int16[i:i + frame_length].tobytes()
        
        if vad.is_speech(frame, sample_rate):
            if current_segment_start is None:
                current_segment_start = i / sample_rate
        else:
            if current_segment_start is not None:
                voice_segments.append((current_segment_start, i / sample_rate))
                current_segment_start = None
    
    # ุฅุถุงูุฉ ุขุฎุฑ segment ุฅุฐุง ูู ููุชู
    if current_segment_start is not None:
        voice_segments.append((current_segment_start, len(audio_int16) / sample_rate))
    
    return voice_segments


def process_transcription_result(
    result: Dict, 
    voice_segments: List[Tuple[float, float]], 
    child_optimized: bool
) -> Dict:
    """ูุนุงูุฌุฉ ูุชุงุฆุฌ ุงูุชุญููู"""
    
    # ุงุณุชุฎุฑุงุฌ ุงููุต ุงูุฃุณุงุณู
    text = result['text'].strip()
    
    # ูุนุงูุฌุฉ ุงููุทุน (segments)
    segments = []
    for segment in result.get('segments', []):
        processed_segment = {
            'start': segment['start'],
            'end': segment['end'],
            'text': segment['text'].strip(),
            'confidence': segment.get('avg_logprob', 0.0),
            'words': segment.get('words', [])
        }
        segments.append(processed_segment)
    
    # ุชุทุจูู ุชุญุณููุงุช ุฎุงุตุฉ ุจุงูุฃุทูุงู
    if child_optimized:
        text = improve_child_transcription(text)
        segments = [improve_segment_transcription(seg) for seg in segments]
    
    return {
        'text': text,
        'segments': segments,
        'voice_activity_periods': voice_segments
    }


def improve_child_transcription(text: str) -> str:
    """ุชุญุณูู ุงููุต ุงููุญูู ููุฃุทูุงู"""
    
    # ูุงููุณ ุชุตุญูุญุงุช ุดุงุฆุนุฉ ูููุงู ุงูุฃุทูุงู
    child_corrections = {
        # ุฃุฎุทุงุก ุงููุทู ุงูุดุงุฆุนุฉ
        'wike': 'like',
        'pwease': 'please',
        'fwend': 'friend',
        'bwother': 'brother',
        'sistew': 'sister',
        'wove': 'love',
        'vewy': 'very',
        'wittle': 'little',
        'hewp': 'help',
        'wat': 'what',
        'wight': 'right',
        'wead': 'read',
        'wun': 'run',
        'wed': 'red',
        
        # ูููุงุช ูุฎุชุตุฑุฉ ุดุงุฆุนุฉ
        'gonna': 'going to',
        'wanna': 'want to',
        'dunno': "don't know",
        'gimme': 'give me',
        'lemme': 'let me',
        
        # ุชุตุญูุญุงุช ูุญููุฉ
        'me want': 'I want',
        'me like': 'I like',
        'me go': 'I go',
        'me see': 'I see',
    }
    
    # ุชุทุจูู ุงูุชุตุญูุญุงุช
    corrected_text = text
    for wrong, correct in child_corrections.items():
        corrected_text = corrected_text.replace(wrong, correct)
    
    # ุชูุธูู ุฅุถุงูู
    corrected_text = ' '.join(corrected_text.split())  # ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ
    
    return corrected_text


def improve_segment_transcription(segment: Dict) -> Dict:
    """ุชุญุณูู segment ูุงุญุฏ"""
    segment['text'] = improve_child_transcription(segment['text'])
    return segment


def apply_child_speech_corrections(result: Dict) -> Dict:
    """ุชุทุจูู ุชุตุญูุญุงุช ุดุงููุฉ ูููุงู ุงูุฃุทูุงู"""
    
    # ุชุญุณูู ุงููุต ุงูุฑุฆูุณู
    result['text'] = improve_child_transcription(result['text'])
    
    # ุชุญุณูู ูู segment
    if 'segments' in result:
        result['segments'] = [
            improve_segment_transcription(seg) 
            for seg in result['segments']
        ]
    
    # ุฅุถุงูุฉ ุนูุงูุงุช ุซูุฉ ูุญุณูุฉ ููุฃุทูุงู
    result['child_speech_confidence'] = calculate_child_speech_confidence(result)
    
    return result


def calculate_child_speech_confidence(result: Dict) -> float:
    """ุญุณุงุจ ูุณุชูู ุงูุซูุฉ ูู ุงูุชุญููู ูููุงู ุงูุฃุทูุงู"""
    
    base_confidence = 0.7  # ุซูุฉ ุฃุณุงุณูุฉ
    
    # ุชุญููู ุนูุงูู ุงูุฌูุฏุฉ
    text = result['text']
    
    # ุทูู ุงููุต (ุงููุตูุต ุงููุตูุฑุฉ ูุฏ ุชููู ุฃูู ุฏูุฉ)
    length_factor = min(1.0, len(text.split()) / 5.0)
    
    # ูุฌูุฏ ูููุงุช ุดุงุฆุนุฉ ููุฃุทูุงู
    child_words = ['mama', 'papa', 'toy', 'play', 'fun', 'happy', 'want', 'like']
    child_word_count = sum(1 for word in child_words if word in text.lower())
    child_factor = min(1.0, child_word_count / 3.0)
    
    # ุญุณุงุจ ุงูุซูุฉ ุงูููุงุฆูุฉ
    final_confidence = base_confidence * (0.4 + 0.3 * length_factor + 0.3 * child_factor)
    
    return max(0.1, min(1.0, final_confidence))


def assess_transcription_quality(
    result: Dict, 
    audio: torch.Tensor, 
    sample_rate: int
) -> Dict:
    """ุชูููู ุฌูุฏุฉ ุงูุชุญููู"""
    
    # ุญุณุงุจ ููุงููุณ ุงูุฌูุฏุฉ
    text = result['text']
    segments = result.get('segments', [])
    
    # ููุงููุณ ุฃุณุงุณูุฉ
    word_count = len(text.split())
    segment_count = len(segments)
    avg_confidence = np.mean([seg.get('avg_logprob', 0.0) for seg in segments]) if segments else 0.0
    
    # ุญุณุงุจ ูุณุจุฉ ุงูุตูุช ุฅูู ุงูููุงู
    audio_duration = len(audio.squeeze()) / sample_rate
    speech_duration = sum(seg['end'] - seg['start'] for seg in segments)
    speech_ratio = speech_duration / audio_duration if audio_duration > 0 else 0.0
    
    # ุชูููู ุฌูุฏุฉ ุงูุตูุช
    audio_quality = assess_audio_quality(audio, sample_rate)
    
    # ุญุณุงุจ ุงูุซูุฉ ุงูุฅุฌูุงููุฉ
    overall_confidence = calculate_overall_confidence(
        avg_confidence, speech_ratio, audio_quality, word_count
    )
    
    return {
        'overall_confidence': overall_confidence,
        'word_count': word_count,
        'segment_count': segment_count,
        'average_segment_confidence': avg_confidence,
        'speech_to_silence_ratio': speech_ratio,
        'audio_quality_score': audio_quality,
        'estimated_accuracy': estimate_transcription_accuracy(overall_confidence),
        'quality_indicators': {
            'sufficient_speech': speech_ratio > 0.3,
            'clear_audio': audio_quality > 0.7,
            'meaningful_length': word_count >= 3,
            'high_confidence': overall_confidence > 0.8
        }
    }


def assess_audio_quality(audio: torch.Tensor, sample_rate: int) -> float:
    """ุชูููู ุฌูุฏุฉ ุงูุตูุช"""
    
    # ุญุณุงุจ ูุณุจุฉ ุงูุฅุดุงุฑุฉ ุฅูู ุงูุถูุถุงุก
    signal_power = torch.mean(audio ** 2)
    noise_estimate = torch.std(audio[-int(0.1 * sample_rate):])  # ุขุฎุฑ 0.1 ุซุงููุฉ
    
    if noise_estimate > 0:
        snr = 10 * torch.log10(signal_power / (noise_estimate ** 2))
        snr_score = torch.clamp(snr / 30.0, 0.0, 1.0)  # ุชุทุจูุน ุฅูู 0-1
    else:
        snr_score = 0.5
    
    # ุชูููู ุงูุชูุฒูุน ุงูุทููู
    spectral_score = assess_spectral_quality(audio, sample_rate)
    
    # ุงูุฌูุฏุฉ ุงูุฅุฌูุงููุฉ
    overall_quality = 0.6 * float(snr_score) + 0.4 * spectral_score
    
    return overall_quality


def assess_spectral_quality(audio: torch.Tensor, sample_rate: int) -> float:
    """ุชูููู ุฌูุฏุฉ ุงูุชูุฒูุน ุงูุทููู"""
    
    # ุชุญููู FFT ุจุณูุท
    fft = torch.fft.fft(audio.squeeze())
    magnitude = torch.abs(fft)
    
    # ุชุญูู ูู ูุฌูุฏ ุทุงูุฉ ูู ูุทุงู ุงูููุงู (300-3400 Hz)
    freqs = torch.fft.fftfreq(len(fft), 1/sample_rate)
    speech_range_mask = (torch.abs(freqs) >= 300) & (torch.abs(freqs) <= 3400)
    
    speech_energy = torch.sum(magnitude[speech_range_mask])
    total_energy = torch.sum(magnitude)
    
    if total_energy > 0:
        speech_ratio = speech_energy / total_energy
        return float(speech_ratio)
    
    return 0.5


def calculate_overall_confidence(
    avg_confidence: float, 
    speech_ratio: float, 
    audio_quality: float, 
    word_count: int
) -> float:
    """ุญุณุงุจ ุงูุซูุฉ ุงูุฅุฌูุงููุฉ ูู ุงูุชุญููู"""
    
    # ุชุทุจูุน avg_confidence (ูู log probability ุฅูู 0-1)
    normalized_confidence = max(0.0, min(1.0, (avg_confidence + 1.0) / 1.0))
    
    # ุชุทุจูุน ุนุฏุฏ ุงููููุงุช
    word_factor = min(1.0, word_count / 10.0)
    
    # ุญุณุงุจ ุงููุฒู ุงูุฅุฌูุงูู
    overall = (
        0.4 * normalized_confidence +
        0.2 * speech_ratio +
        0.3 * audio_quality +
        0.1 * word_factor
    )
    
    return max(0.1, min(1.0, overall))


def estimate_transcription_accuracy(confidence: float) -> str:
    """ุชูุฏูุฑ ุฏูุฉ ุงูุชุญููู"""
    
    if confidence >= 0.9:
        return "excellent"
    elif confidence >= 0.8:
        return "very_good"
    elif confidence >= 0.7:
        return "good"
    elif confidence >= 0.6:
        return "fair"
    elif confidence >= 0.4:
        return "poor"
    else:
        return "very_poor" 