# ===================================================================
# ðŸ§¸ AI Teddy Bear - Model Evaluation System
# Enterprise ML Model Performance Evaluation & Analysis
# ML Team Lead: Senior ML Engineer
# Date: January 2025
# ===================================================================

import asyncio
import logging
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Tuple, Optional

import numpy as np

logger = logging.getLogger(__name__)


class EvaluationMetric(Enum):
    """Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""

    ACCURACY = "accuracy"
    CHILD_SATISFACTION = "child_satisfaction"
    SAFETY_SCORE = "safety_score"
    RESPONSE_TIME = "response_time"
    ENGAGEMENT_RATE = "engagement_rate"
    LEARNING_EFFECTIVENESS = "learning_effectiveness"
    PARENT_APPROVAL = "parent_approval"
    COMPLIANCE_SCORE = "compliance_score"


@dataclass
class ModelEvaluationResult:
    """Ù†ØªÙŠØ¬Ø© ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""

    model_id: str
    model_type: str
    evaluation_date: datetime
    metrics: Dict[str, float]
    performance_grade: str
    strengths: List[str]
    weaknesses: List[str]
    recommendations: List[str]
    confidence_interval: Dict[str, Tuple[float, float]]
    sample_size: int


@dataclass
class PerformanceTrend:
    """Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ø£Ø¯Ø§Ø¡"""

    metric_name: str
    trend_direction: str  # 'improving', 'declining', 'stable'
    trend_strength: float  # 0-1
    time_period: str
    historical_values: List[float]
    current_value: float
    predicted_next_value: float


class ModelEvaluator:
    """Ù…Ù‚ÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø´Ø§Ù…Ù„"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.evaluation_history: List[ModelEvaluationResult] = []
        self.performance_thresholds = self._load_performance_thresholds()

        logger.info("ðŸ“Š Model Evaluator initialized")

    async def evaluate_current_models(self) -> Dict[str, Any]:
        """ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""

        logger.info("ðŸ” Starting comprehensive model evaluation...")

        # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠØ©
        current_models = await self._get_current_models()

        # ØªÙ‚ÙŠÙŠÙ… ÙƒÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø´ÙƒÙ„ Ù…ØªÙˆØ§Ø²ÙŠ
        evaluation_tasks = [
            self._evaluate_single_model(model_id, model_info)
            for model_id, model_info in current_models.items()
        ]

        model_evaluations = await asyncio.gather(
            *evaluation_tasks, return_exceptions=True
        )

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        evaluation_results = {}
        successful_evaluations = []

        for i, (model_id, _) in enumerate(current_models.items()):
            if not isinstance(model_evaluations[i], Exception):
                evaluation_results[model_id] = model_evaluations[i]
                successful_evaluations.append(model_evaluations[i])
            else:
                logger.error(
                    f"Failed to evaluate model {model_id}: {model_evaluations[i]}"
                )

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        overall_metrics = await self._calculate_overall_metrics(successful_evaluations)

        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§ØªØ¬Ø§Ù‡Ø§Øª
        performance_trends = await self._analyze_performance_trends(
            successful_evaluations
        )

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
        issues_and_recommendations = await self._identify_issues_and_recommendations(
            successful_evaluations
        )

        # Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…
        await self._save_evaluation_results(evaluation_results)

        return {
            "evaluation_date": datetime.utcnow(),
            "models_evaluated": len(successful_evaluations),
            "overall_metrics": overall_metrics,
            "individual_results": evaluation_results,
            "performance_trends": performance_trends,
            "issues_identified": issues_and_recommendations["issues"],
            "recommendations": issues_and_recommendations["recommendations"],
            "summary": await self._generate_evaluation_summary(
                overall_metrics, performance_trends
            ),
        }

    async def _get_current_models(self) -> Dict[str, Dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""

        # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø­Ø§Ù„ÙŠØ© ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬
        current_models = {
            "speech_recognition_model": {
                "type": "whisper_large_v3",
                "version": "2.1.0",
                "deployment_date": datetime.utcnow() - timedelta(days=30),
                "target_age_groups": ["3-6", "7-9", "10-12"],
                "languages": ["en", "ar"],
                "specialization": "child_speech_recognition",
            },
            "conversation_model": {
                "type": "gpt4_turbo_child_safe",
                "version": "1.8.2",
                "deployment_date": datetime.utcnow() - timedelta(days=15),
                "target_age_groups": ["3-6", "7-9", "10-12"],
                "languages": ["en", "ar"],
                "specialization": "child_conversation",
            },
            "emotion_analysis_model": {
                "type": "multimodal_emotion_detector",
                "version": "1.3.1",
                "deployment_date": datetime.utcnow() - timedelta(days=45),
                "target_age_groups": ["3-6", "7-9", "10-12"],
                "languages": ["en", "ar"],
                "specialization": "child_emotion_analysis",
            },
            "safety_classifier": {
                "type": "bert_safety_classifier",
                "version": "3.2.0",
                "deployment_date": datetime.utcnow() - timedelta(days=10),
                "target_age_groups": ["3-6", "7-9", "10-12"],
                "languages": ["en", "ar"],
                "specialization": "content_safety",
            },
            "learning_recommender": {
                "type": "collaborative_filtering_v2",
                "version": "2.0.3",
                "deployment_date": datetime.utcnow() - timedelta(days=20),
                "target_age_groups": ["3-6", "7-9", "10-12"],
                "languages": ["en", "ar"],
                "specialization": "personalized_learning",
            },
        }

        return current_models

    async def _evaluate_single_model(
        self, model_id: str, model_info: Dict[str, Any]
    ) -> ModelEvaluationResult:
        """ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­Ø¯"""

        logger.info(f"ðŸ”¬ Evaluating model: {model_id}")

        # Ù…Ø­Ø§ÙƒØ§Ø© Ù‚ÙŠØ§Ø³ Ø§Ù„Ø£Ø¯Ø§Ø¡
        metrics = await self._measure_model_performance(model_id, model_info)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ø±Ø¬Ø© Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        performance_grade = await self._calculate_performance_grade(metrics)

        # ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù
        strengths = await self._identify_model_strengths(metrics, model_info)
        weaknesses = await self._identify_model_weaknesses(metrics, model_info)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        recommendations = await self._generate_model_recommendations(
            metrics, strengths, weaknesses
        )

        # Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø©
        confidence_intervals = await self._calculate_confidence_intervals(metrics)

        return ModelEvaluationResult(
            model_id=model_id,
            model_type=model_info["type"],
            evaluation_date=datetime.utcnow(),
            metrics=metrics,
            performance_grade=performance_grade,
            strengths=strengths,
            weaknesses=weaknesses,
            recommendations=recommendations,
            confidence_interval=confidence_intervals,
            sample_size=1000,  # Ù…Ø­Ø§ÙƒØ§Ø© Ø­Ø¬Ù… Ø§Ù„Ø¹ÙŠÙ†Ø©
        )

    def _get_base_performance_metrics(self) -> Dict[str, float]:
        return {
            "accuracy": 0.85, "child_satisfaction": 0.82, "safety_score": 0.96,
            "response_time": 1.2, "engagement_rate": 0.78,
            "learning_effectiveness": 0.73, "parent_approval": 0.88,
            "compliance_score": 0.94,
        }

    def _adjust_performance_by_model_type(self, model_id: str, performance: Dict[str, float]):
        adjustments = {
            "speech_recognition": {"accuracy": np.random.beta(18, 3), "response_time": np.random.gamma(2, 0.3)},
            "conversation": {"child_satisfaction": np.random.beta(16, 3), "engagement_rate": np.random.beta(15, 4), "response_time": np.random.gamma(3, 0.4)},
            "emotion": {"accuracy": np.random.beta(12, 4), "learning_effectiveness": np.random.beta(14, 5)},
            "safety": {"safety_score": np.random.beta(25, 1), "compliance_score": np.random.beta(22, 2)},
            "learning": {"learning_effectiveness": np.random.beta(16, 4), "child_satisfaction": np.random.beta(14, 5)},
        }
        for type_key, values in adjustments.items():
            if type_key in model_id:
                performance.update(values)
                break

    def _apply_age_degradation(self, model_info: Dict[str, Any], performance: Dict[str, float]):
        days_since_deployment = (
            datetime.utcnow() - model_info["deployment_date"]).days
        degradation_factor = max(0.9, 1 - (days_since_deployment * 0.001))
        for metric in performance:
            if metric != "response_time":
                performance[metric] *= degradation_factor

    def _add_realistic_noise(self, performance: Dict[str, float]):
        for metric in performance:
            noise = np.random.normal(0, 0.02)
            performance[metric] = max(0, min(1, performance[metric] + noise))

    async def _measure_model_performance(
        self, model_id: str, model_info: Dict[str, Any]
    ) -> Dict[str, float]:
        """Ù‚ÙŠØ§Ø³ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        base_performance = self._get_base_performance_metrics()
        self._adjust_performance_by_model_type(model_id, base_performance)
        self._apply_age_degradation(model_info, base_performance)
        self._add_realistic_noise(base_performance)
        return base_performance

    async def _calculate_performance_grade(
            self, metrics: Dict[str, float]) -> str:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""

        # Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
        weights = {
            "safety_score": 0.25,  # Ø§Ù„Ø£Ù…Ø§Ù† Ø£Ù‡Ù… Ø´ÙŠØ¡
            "child_satisfaction": 0.20,
            "accuracy": 0.15,
            "parent_approval": 0.15,
            "learning_effectiveness": 0.10,
            "engagement_rate": 0.10,
            "compliance_score": 0.05,
        }

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ø§Ù„Ù…Ø±Ø¬Ø­
        weighted_score = sum(
            metrics.get(
                metric,
                0) * weight for metric,
            weight in weights.items())

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø¯Ø±Ø¬Ø©
        if weighted_score >= 0.90:
            return "A+"
        elif weighted_score >= 0.85:
            return "A"
        elif weighted_score >= 0.80:
            return "B+"
        elif weighted_score >= 0.75:
            return "B"
        elif weighted_score >= 0.70:
            return "C+"
        elif weighted_score >= 0.65:
            return "C"
        else:
            return "D"

    STRENGTH_CRITERIA = {
        "safety_score": {
            "threshold": 0.95,
            "condition": "gt",
            "message": "Excellent safety compliance",
        },
        "child_satisfaction": {
            "threshold": 0.85,
            "condition": "gt",
            "message": "High child satisfaction rates",
        },
        "accuracy": {
            "threshold": 0.88,
            "condition": "gt",
            "message": "Superior accuracy performance",
        },
        "parent_approval": {
            "threshold": 0.85,
            "condition": "gt",
            "message": "Strong parent approval",
        },
        "response_time": {
            "threshold": 1.0,
            "condition": "lt",
            "message": "Fast response times",
        },
        "learning_effectiveness": {
            "threshold": 0.80,
            "condition": "gt",
            "message": "Effective learning outcomes",
        },
        "engagement_rate": {
            "threshold": 0.80,
            "condition": "gt",
            "message": "High user engagement",
        },
        "compliance_score": {
            "threshold": 0.92,
            "condition": "gt",
            "message": "Excellent regulatory compliance",
        },
    }

    def _check_strength(self, metric_name: str, value: float) -> Optional[str]:
        """Check a single metric for strength based on predefined criteria."""
        criteria = self.STRENGTH_CRITERIA.get(metric_name)
        if not criteria:
            return None

        is_strong = False
        if criteria["condition"] == "gt" and value > criteria["threshold"]:
            is_strong = True
        elif criteria["condition"] == "lt" and value < criteria["threshold"]:
            is_strong = True

        return criteria["message"] if is_strong else None

    async def _identify_model_strengths(
        self, metrics: Dict[str, float], model_info: Dict[str, Any]
    ) -> List[str]:
        """Identify model strengths using a configuration-driven approach."""
        strengths = []

        for metric_name, value in metrics.items():
            strength_message = self._check_strength(metric_name, value)
            if strength_message:
                strengths.append(strength_message)

        # Add specialization-specific strengths
        if "speech_recognition" in model_info.get("specialization", ""):
            if metrics.get("accuracy", 0) > 0.85:
                strengths.append("Excellent speech recognition for children")

        return list(set(strengths))

    WEAKNESS_CRITERIA = {
        "safety_score": {"threshold": 0.95, "condition": "lt", "message": "Safety score below critical threshold"},
        "child_satisfaction": {"threshold": 0.75, "condition": "lt", "message": "Low child satisfaction rates"},
        "accuracy": {"threshold": 0.80, "condition": "lt", "message": "Accuracy below acceptable level"},
        "parent_approval": {"threshold": 0.75, "condition": "lt", "message": "Insufficient parent approval"},
        "response_time": {"threshold": 2.0, "condition": "gt", "message": "Slow response times"},
        "learning_effectiveness": {"threshold": 0.65, "condition": "lt", "message": "Poor learning outcomes"},
        "engagement_rate": {"threshold": 0.70, "condition": "lt", "message": "Low user engagement"},
        "compliance_score": {"threshold": 0.90, "condition": "lt", "message": "Compliance issues detected"},
    }

    def _check_weakness(self, metric_name: str, value: float) -> Optional[str]:
        """Check a single metric for weakness based on predefined criteria."""
        criteria = self.WEAKNESS_CRITERIA.get(metric_name)
        if not criteria:
            return None

        is_weak = False
        if criteria["condition"] == "gt" and value > criteria["threshold"]:
            is_weak = True
        elif criteria["condition"] == "lt" and value < criteria["threshold"]:
            is_weak = True

        return criteria["message"] if is_weak else None

    async def _identify_model_weaknesses(
        self, metrics: Dict[str, float], model_info: Dict[str, Any]
    ) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø¶Ø¹Ù Ø§Ù„Ù†Ù…ÙˆØ°Ø¬"""
        weaknesses = []
        for metric_name, value in metrics.items():
            weakness_message = self._check_weakness(metric_name, value)
            if weakness_message:
                weaknesses.append(weakness_message)

        days_since_deployment = (
            datetime.utcnow() - model_info["deployment_date"]).days
        if days_since_deployment > 60:
            weaknesses.append("Model may need updating due to age")

        return weaknesses

    RECOMMENDATION_MAPPING = {
        "Safety score below critical threshold": "Immediate safety model retraining required",
        "Low child satisfaction rates": "Enhance conversation flow and personalization",
        "Accuracy below acceptable level": "Increase training data quality and quantity",
        "Slow response times": "Optimize model inference speed",
        "Poor learning outcomes": "Improve educational content and delivery",
    }

    async def _generate_model_recommendations(
        self, metrics: Dict[str, float], strengths: List[str], weaknesses: List[str]
    ) -> List[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬"""
        recommendations = [self.RECOMMENDATION_MAPPING[w]
                           for w in weaknesses if w in self.RECOMMENDATION_MAPPING]

        if len(weaknesses) > 3:
            recommendations.append("Consider comprehensive model redesign")
        elif len(weaknesses) > 1:
            recommendations.append("Implement targeted improvements")

        if len(strengths) > 5:
            recommendations.append(
                "Model performing well - maintain current approach")

        return list(set(recommendations))

    async def _calculate_confidence_intervals(
        self, metrics: Dict[str, float]
    ) -> Dict[str, Tuple[float, float]]:
        """Ø­Ø³Ø§Ø¨ ÙØªØ±Ø§Øª Ø§Ù„Ø«Ù‚Ø© Ù„Ù„Ù…Ù‚Ø§ÙŠÙŠØ³"""

        confidence_intervals = {}

        for metric, value in metrics.items():
            # Ù…Ø­Ø§ÙƒØ§Ø© ÙØªØ±Ø© Ø«Ù‚Ø© 95%
            margin_of_error = 0.05  # 5% margin of error
            lower_bound = max(0, value - margin_of_error)
            upper_bound = min(1, value + margin_of_error)

            if metric == "response_time":
                # ÙˆÙ‚Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù„Ù‡ Ù†Ø·Ø§Ù‚ Ù…Ø®ØªÙ„Ù
                margin_of_error = 0.2
                lower_bound = max(0, value - margin_of_error)
                upper_bound = value + margin_of_error

            confidence_intervals[metric] = (lower_bound, upper_bound)

        return confidence_intervals

    async def _calculate_overall_metrics(
        self, evaluations: List[ModelEvaluationResult]
    ) -> Dict[str, float]:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©"""

        if not evaluations:
            return {}

        overall_metrics = {}

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªÙˆØ³Ø· Ù„ÙƒÙ„ Ù…Ù‚ÙŠØ§Ø³
        all_metrics = set()
        for eval_result in evaluations:
            all_metrics.update(eval_result.metrics.keys())

        for metric in all_metrics:
            values = [
                eval_result.metrics.get(
                    metric,
                    0) for eval_result in evaluations]
            overall_metrics[metric] = np.mean(values)

        # Ø¥Ø¶Ø§ÙØ© Ù…Ù‚Ø§ÙŠÙŠØ³ Ø¥Ø¶Ø§ÙÙŠØ©
        overall_metrics["models_above_threshold"] = sum(
            1
            for eval_result in evaluations
            if eval_result.performance_grade in ["A+", "A", "B+"]
        ) / len(evaluations)

        overall_metrics["average_grade_score"] = self._grade_to_score(
            [eval_result.performance_grade for eval_result in evaluations]
        )

        return overall_metrics

    def _grade_to_score(self, grades: List[str]) -> float:
        """ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¯Ø±Ø¬Ø§Øª Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø±Ù‚Ù…ÙŠØ©"""

        grade_mapping = {
            "A+": 4.0,
            "A": 3.7,
            "B+": 3.3,
            "B": 3.0,
            "C+": 2.3,
            "C": 2.0,
            "D": 1.0,
        }

        scores = [grade_mapping.get(grade, 0) for grade in grades]
        return np.mean(scores) if scores else 0

    async def _analyze_performance_trends(
        self, evaluations: List[ModelEvaluationResult]
    ) -> List[PerformanceTrend]:
        """ØªØ­Ù„ÙŠÙ„ Ø§ØªØ¬Ø§Ù‡Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""

        trends = []

        # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ§Ø±ÙŠØ®ÙŠØ©
        for metric in [
            "accuracy",
            "child_satisfaction",
            "safety_score",
            "response_time",
        ]:
            historical_values = [
                np.random.beta(
                    8,
                    2) +
                np.random.normal(
                    0,
                    0.02) for _ in range(30)]  # Ø¢Ø®Ø± 30 ÙŠÙˆÙ…

            current_value = np.mean([eval_result.metrics.get(
                metric, 0) for eval_result in evaluations])

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø§ØªØ¬Ø§Ù‡
            recent_trend = np.polyfit(
                range(len(historical_values)), historical_values, 1
            )[0]

            if recent_trend > 0.001:
                trend_direction = "improving"
                trend_strength = min(1.0, abs(recent_trend) * 100)
            elif recent_trend < -0.001:
                trend_direction = "declining"
                trend_strength = min(1.0, abs(recent_trend) * 100)
            else:
                trend_direction = "stable"
                trend_strength = 0.1

            # Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ©
            predicted_next = current_value + recent_trend

            trends.append(
                PerformanceTrend(
                    metric_name=metric,
                    trend_direction=trend_direction,
                    trend_strength=trend_strength,
                    time_period="30_days",
                    historical_values=historical_values,
                    current_value=current_value,
                    predicted_next_value=predicted_next,
                )
            )

        return trends

    def _collect_issues_and_recs(self, evaluations: List[ModelEvaluationResult]) -> Tuple[List[str], List[str]]:
        all_issues = []
        all_recommendations = []
        for eval_result in evaluations:
            all_issues.extend(eval_result.weaknesses)
            all_recommendations.extend(eval_result.recommendations)
        return all_issues, all_recommendations

    def _prioritize_issues(self, all_issues: List[str]) -> List[Tuple[str, int]]:
        issue_counts = {}
        for issue in all_issues:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
        return sorted(issue_counts.items(), key=lambda x: x[1], reverse=True)

    def _generate_system_recommendations(self, priority_issues: List[Tuple[str, int]]) -> List[str]:
        system_recommendations = []
        top_issues = [issue.lower() for issue, _ in priority_issues[:3]]
        if any("safety" in issue for issue in top_issues):
            system_recommendations.append(
                "Implement system-wide safety improvements")
        if any("satisfaction" in issue for issue in top_issues):
            system_recommendations.append(
                "Focus on user experience enhancements")
        if any("accuracy" in issue for issue in top_issues):
            system_recommendations.append(
                "Increase overall model training quality")
        return system_recommendations

    async def _identify_issues_and_recommendations(
        self, evaluations: List[ModelEvaluationResult]
    ) -> Dict[str, List[str]]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª"""
        all_issues, all_recommendations = self._collect_issues_and_recs(
            evaluations)
        priority_issues = self._prioritize_issues(all_issues)
        system_recommendations = self._generate_system_recommendations(
            priority_issues)

        return {
            "issues": [issue for issue, _ in priority_issues[:10]],
            "recommendations": list(set(all_recommendations + system_recommendations)),
        }

    def _load_performance_thresholds(self) -> Dict[str, float]:
        """ØªØ­Ù…ÙŠÙ„ Ø¹ØªØ¨Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"""

        return {
            "safety_score": 0.95,
            "accuracy": 0.80,
            "child_satisfaction": 0.75,
            "parent_approval": 0.75,
            "response_time": 2.0,
            "learning_effectiveness": 0.65,
            "engagement_rate": 0.70,
            "compliance_score": 0.90,
        }

    async def _save_evaluation_results(
        self, results: Dict[str, ModelEvaluationResult]
    ) -> None:
        """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªÙ‚ÙŠÙŠÙ…"""

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø¥Ù„Ù‰ Ø§Ù„ØªØ§Ø±ÙŠØ®
        for result in results.values():
            self.evaluation_history.append(result)

        # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø¢Ø®Ø± 100 ØªÙ‚ÙŠÙŠÙ… ÙÙ‚Ø·
        if len(self.evaluation_history) > 100:
            self.evaluation_history = self.evaluation_history[-100:]

        logger.info(f"ðŸ’¾ Saved evaluation results for {len(results)} models")

    def _determine_overall_health(
            self, overall_metrics: Dict[str, float]) -> str:
        """Determine the overall health status from metrics."""
        safety_score = overall_metrics.get("safety_score", 0)
        avg_satisfaction = overall_metrics.get("child_satisfaction", 0)

        if safety_score < 0.95:
            return "critical"
        elif avg_satisfaction < 0.70:
            return "poor"
        elif avg_satisfaction > 0.85 and safety_score > 0.96:
            return "excellent"
        return "good"

    def _generate_key_insights(
        self, overall_metrics: Dict[str, float], trends: List[PerformanceTrend]
    ) -> List[str]:
        """Generate key insights from the evaluation results."""
        insights = []
        improving_metrics = sum(
            1 for t in trends if t.trend_direction == "improving")
        declining_metrics = sum(
            1 for t in trends if t.trend_direction == "declining")

        if improving_metrics > declining_metrics:
            insights.append("Overall system performance is improving")

        if overall_metrics.get("safety_score", 0) > 0.96:
            insights.append("Safety systems performing excellently")

        if overall_metrics.get("models_above_threshold", 0) > 0.8:
            insights.append("Majority of models meeting performance standards")

        return insights

    async def _generate_evaluation_summary(
        self, overall_metrics: Dict[str, float], trends: List[PerformanceTrend]
    ) -> Dict[str, Any]:
        """Generate evaluation summary by orchestrating helper methods."""
        overall_health = self._determine_overall_health(overall_metrics)
        key_insights = self._generate_key_insights(overall_metrics, trends)

        improving_metrics = sum(
            1 for t in trends if t.trend_direction == "improving")
        declining_metrics = sum(
            1 for t in trends if t.trend_direction == "declining")
        stable_metrics = len(trends) - improving_metrics - declining_metrics

        return {
            "overall_health": overall_health,
            "critical_issues": 1 if overall_health == "critical" else 0,
            "improving_metrics": improving_metrics,
            "declining_metrics": declining_metrics,
            "stable_metrics": stable_metrics,
            "key_insights": key_insights,
        }
