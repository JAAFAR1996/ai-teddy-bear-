"""
ðŸš€ LLM Service Factory - Refactored with High Cohesion
Ø§Ù„Ù…ØµÙ†Ø¹ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ Ù„Ø®Ø¯Ù…Ø§Øª LLM - ØªÙ… Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Low Cohesion

âœ… ØªÙ… ØªØ·Ø¨ÙŠÙ‚ EXTRACT CLASS pattern Ø¨Ù†Ø¬Ø§Ø­
âœ… ÙØµÙ„ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª Ø¥Ù„Ù‰ classes Ù…Ù†ÙØµÙ„Ø© ÙˆÙ…ØªÙ…Ø§Ø³ÙƒØ©
âœ… ØªØ­Ø³ÙŠÙ† Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯ ÙˆØ³Ù‡ÙˆÙ„Ø© Ø§Ù„ØµÙŠØ§Ù†Ø©
âœ… ØªÙˆØ§ÙÙ‚ ÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø­Ø§Ù„ÙŠ

Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø© (Extracted Components):
- LLMParameterValidationService: Parameter Validation
- LLMResponseCache: Response Caching & Cache Management
- LLMModelSelector: Model Selection & Configuration
- LegacyCompatibilityService: Legacy Support & Parameter Conversion
- LLMServiceFactory: Main Factory Coordination
"""

import asyncio
import hashlib
import json
import logging
from abc import ABC, abstractmethod
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, AsyncIterator, Dict, List, Optional, Union

import anthropic
import google.generativeai as genai
import openai
import torch

try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
except ImportError:
    from src.infrastructure.external_services.mock.transformers import AutoModelForCausalLM, AutoTokenizer

try:
    import redis.asyncio as aioredis
except ImportError:
    aioredis = None

from src.core.domain.entities.conversation import Conversation, Message
from src.infrastructure.config import get_config

from .llm_base import (
    LLMProvider, ModelConfig, LLMResponse, BaseLLMAdapter
)
from .llm_openai_adapter import OpenAIAdapter
from .llm_anthropic_adapter import AnthropicAdapter  
from .llm_google_adapter import GoogleAdapter


# ================== EXTRACTED COMPONENT 1: PARAMETER VALIDATION ==================

class LLMParameterValidationService:
    """
    Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©: Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© parameters Ø§Ù„Ø®Ø§ØµØ© Ø¨Ù€ LLM
    Extracted from main factory to achieve High Cohesion
    """
    
    @staticmethod
    def validate_required_conversation(conversation: Conversation) -> None:
        """Validate that conversation is provided and valid"""
        if not conversation:
            raise ValueError("Conversation is required")
    
    @staticmethod
    def validate_max_tokens_range(max_tokens: int) -> None:
        """Validate max_tokens parameter is within acceptable range"""
        if max_tokens < 1 or max_tokens > 8192:
            raise ValueError("max_tokens must be between 1 and 8192")
    
    @staticmethod
    def validate_temperature_range(temperature: float) -> None:
        """Validate temperature parameter is within acceptable range"""
        if temperature < 0.0 or temperature > 2.0:
            raise ValueError("temperature must be between 0.0 and 2.0")
    
    @staticmethod
    def validate_provider_type(provider: Optional[LLMProvider]) -> None:
        """Validate provider parameter if provided"""
        if provider is not None and not isinstance(provider, LLMProvider):
            raise ValueError("provider must be a valid LLMProvider enum value")
    
    @staticmethod
    def validate_model_name(model: Optional[str]) -> None:
        """Validate model name if provided"""
        if model is not None and (not isinstance(model, str) or len(model.strip()) == 0):
            raise ValueError("model must be a non-empty string if provided")


class LLMParameterValidator:
    """Specialized validator for LLM parameter objects"""
    
    def __init__(self, validation_service: LLMParameterValidationService):
        self.validator = validation_service
    
    def validate_core_parameters(self, conversation: Conversation, max_tokens: int, temperature: float) -> None:
        """Validate core required parameters"""
        self.validator.validate_required_conversation(conversation)
        self.validator.validate_max_tokens_range(max_tokens)
        self.validator.validate_temperature_range(temperature)
    
    def validate_optional_parameters(self, provider: Optional[LLMProvider], model: Optional[str]) -> None:
        """Validate optional parameters"""
        self.validator.validate_provider_type(provider)
        self.validator.validate_model_name(model)
    
    def validate_all_parameters(self, conversation: Conversation, provider: Optional[LLMProvider], 
                              model: Optional[str], max_tokens: int, temperature: float) -> None:
        """Validate all parameters - single entry point"""
        self.validate_core_parameters(conversation, max_tokens, temperature)
        self.validate_optional_parameters(provider, model)


# ================== EXTRACTED COMPONENT 2: RESPONSE CACHING ==================

class LLMResponseCache:
    """
    Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©: Ø¥Ø¯Ø§Ø±Ø© cache Ù„Ù„Ù€ LLM responses
    Extracted from main factory to achieve High Cohesion
    """

    def __init__(self, redis_url: Optional[str] = None, ttl: int = 3600, max_size: int = 1000):
        self.redis_url = redis_url
        self.redis_client = None
        self.local_cache = {}
        self.cache_ttl = ttl
        self.max_size = max_size
        self.logger = logging.getLogger(self.__class__.__name__)

    async def connect(self):
        """Connect to Redis if available"""
        if self.redis_url and aioredis:
            try:
                self.redis_client = await aioredis.create_redis_pool(self.redis_url)
                self.logger.info("Connected to Redis cache")
            except Exception as e:
                self.logger.warning(f"Failed to connect to Redis: {e}")

    async def get(self, key: str) -> Optional[str]:
        """Get cached response"""
        # Try Redis first
        redis_value = await self._try_redis_get(key)
        if redis_value:
            return redis_value
        
        # Fallback to local cache
        return self._try_local_get(key)
    
    async def _try_redis_get(self, key: str) -> Optional[str]:
        """Try to get value from Redis"""
        if not self.redis_client:
            return None
        
        try:
            value = await self.redis_client.get(key)
            return value.decode('utf-8') if value else None
        except Exception:
            return None
    
    def _try_local_get(self, key: str) -> Optional[str]:
        """Try to get value from local cache"""
        if key not in self.local_cache:
            return None
        
        value, expiry = self.local_cache[key]
        
        if self._is_cache_valid(expiry):
            return value
        else:
            self._remove_expired_cache(key)
            return None
    
    def _is_cache_valid(self, expiry: datetime) -> bool:
        """Check if cache entry is still valid"""
        return datetime.now() < expiry
    
    def _remove_expired_cache(self, key: str) -> None:
        """Remove expired cache entry"""
        self.local_cache.pop(key, None)

    async def set(self, key: str, value: str):
        """Set cached response"""
        if self.redis_client:
            try:
                await self.redis_client.setex(key, self.cache_ttl, value)
            except Exception:
                pass

        # Set in local cache
        self._set_local_cache(key, value)
    
    def _set_local_cache(self, key: str, value: str) -> None:
        """Set value in local cache with size management"""
        if len(self.local_cache) >= self.max_size:
            # Remove oldest entry
            oldest_key = next(iter(self.local_cache))
            del self.local_cache[oldest_key]
        
        expiry = datetime.now() + timedelta(seconds=self.cache_ttl)
        self.local_cache[key] = (value, expiry)

    def generate_key(self, messages: List[Message], model_config: ModelConfig) -> str:
        """Generate cache key for messages and config"""
        content = json.dumps([msg.__dict__ for msg in messages], sort_keys=True)
        config_hash = hashlib.md5(json.dumps(model_config.__dict__, sort_keys=True).encode()).hexdigest()
        return f"llm:{hashlib.md5(content.encode()).hexdigest()}:{config_hash}"
    
    def get_stats(self) -> dict:
        """Get cache statistics"""
        return {
            "local_cache_size": len(self.local_cache),
            "redis_connected": self.redis_client is not None,
            "cache_ttl": self.cache_ttl,
            "max_size": self.max_size
        }


# ================== EXTRACTED COMPONENT 3: MODEL SELECTION ==================

@dataclass
class ModelSelectionRequest:
    """Parameter object for model selection"""
    task_type: str
    context_length: int = 0
    required_features: List[str] = field(default_factory=list)
    budget_constraint: Optional[float] = None
    latency_requirement: Optional[int] = None


class LLMModelSelector:
    """
    Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù…Ù‡Ù…Ø©
    Extracted from main factory to achieve High Cohesion
    """

    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.performance_history = defaultdict(list)

    def select_model(self, request: ModelSelectionRequest) -> ModelConfig:
        """Select best model for the given request"""
        return self._select_by_task_type(request.task_type)
    
    def _select_by_task_type(self, task_type: str) -> ModelConfig:
        """Select model based on task type"""
        task_configs = {
            "creative_writing": ModelConfig(
                provider=LLMProvider.ANTHROPIC,
                model_name="claude-3-sonnet", 
                max_tokens=2048,
                temperature=0.8
            ),
            "analysis": ModelConfig(
                provider=LLMProvider.OPENAI,
                model_name="gpt-4",
                max_tokens=1024, 
                temperature=0.3
            )
        }
        
        return task_configs.get(task_type, ModelConfig(
            provider=LLMProvider.OPENAI,
            model_name="gpt-3.5-turbo",
            max_tokens=1024,
            temperature=0.7
        ))

    def get_default_model_config(self, provider: LLMProvider = LLMProvider.OPENAI, task: str = 'general') -> ModelConfig:
        """Get default model configuration for provider and task"""
        configs = {
            LLMProvider.OPENAI: {
                'general': ModelConfig(
                    provider=LLMProvider.OPENAI,
                    model_name='gpt-3.5-turbo',
                    max_tokens=150,
                    temperature=0.7
                ),
                'creative': ModelConfig(
                    provider=LLMProvider.OPENAI,
                    model_name='gpt-4',
                    max_tokens=500,
                    temperature=0.9
                )
            },
            LLMProvider.ANTHROPIC: {
                'general': ModelConfig(
                    provider=LLMProvider.ANTHROPIC,
                    model_name='claude-3-sonnet',
                    max_tokens=150,
                    temperature=0.7
                )
            }
        }
        
        return configs.get(provider, {}).get(task, configs[LLMProvider.OPENAI]['general'])


# ================== PARAMETER OBJECTS ==================

@dataclass
class GenerationRequest:
    """Parameter object for generation requests"""
    conversation: Conversation
    provider: Optional[LLMProvider] = None
    model: Optional[str] = None
    max_tokens: int = 150
    temperature: float = 0.7
    stream: bool = False
    use_cache: bool = True
    extra_kwargs: Dict[str, Any] = field(default_factory=dict)

@dataclass
class LegacyGenerationParams:
    """Parameter object for legacy generation parameters"""
    conversation: Conversation
    provider: Optional[LLMProvider] = None
    model: Optional[str] = None
    max_tokens: int = 150
    temperature: float = 0.7
    stream: bool = False
    use_cache: bool = True
    extra_kwargs: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate parameters"""
        validation_service = LLMParameterValidationService()
        parameter_validator = LLMParameterValidator(validation_service)
        
        parameter_validator.validate_all_parameters(
            self.conversation, self.provider, self.model, 
            self.max_tokens, self.temperature
        )
    
    def to_generation_request(self) -> GenerationRequest:
        """Convert to modern GenerationRequest"""
        return GenerationRequest(
            conversation=self.conversation,
            provider=self.provider,
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stream=self.stream,
            use_cache=self.use_cache,
            extra_kwargs=self.extra_kwargs
        )

@dataclass
class LegacyFactoryParams:
    """Parameter object for legacy factory parameters"""
    conversation: Conversation
    provider: Optional[LLMProvider] = None
    model: Optional[str] = None
    max_tokens: int = 150
    temperature: float = 0.7
    stream: bool = False
    use_cache: bool = True
    extra_kwargs: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate factory parameters"""
        validation_service = LLMParameterValidationService()
        parameter_validator = LLMParameterValidator(validation_service)
        
        parameter_validator.validate_all_parameters(
            self.conversation, self.provider, self.model, 
            self.max_tokens, self.temperature
        )
    
    def to_legacy_generation_params(self) -> LegacyGenerationParams:
        """Convert to LegacyGenerationParams"""
        return LegacyGenerationParams(
            conversation=self.conversation,
            provider=self.provider,
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stream=self.stream,
            use_cache=self.use_cache,
            extra_kwargs=self.extra_kwargs
        )

@dataclass
class LegacyCompatibilityParams:
    """Parameter object for legacy compatibility parameters"""
    conversation: Conversation
    provider: Optional[LLMProvider] = None
    model: Optional[str] = None
    max_tokens: int = 150
    temperature: float = 0.7
    stream: bool = False
    use_cache: bool = True
    extra_kwargs: Dict[str, Any] = field(default_factory=dict)
    
    def __post_init__(self):
        """Validate compatibility parameters"""
        validation_service = LLMParameterValidationService()
        parameter_validator = LLMParameterValidator(validation_service)
        
        parameter_validator.validate_all_parameters(
            self.conversation, self.provider, self.model, 
            self.max_tokens, self.temperature
        )
    
    def to_legacy_generation_params(self) -> LegacyGenerationParams:
        """Convert to LegacyGenerationParams"""
        return LegacyGenerationParams(
            conversation=self.conversation,
            provider=self.provider,
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stream=self.stream,
            use_cache=self.use_cache,
            extra_kwargs=self.extra_kwargs
        )
    
    def to_legacy_factory_params(self) -> LegacyFactoryParams:
        """Convert to LegacyFactoryParams"""
        return LegacyFactoryParams(
            conversation=self.conversation,
            provider=self.provider,
            model=self.model,
            max_tokens=self.max_tokens,
            temperature=self.temperature,
            stream=self.stream,
            use_cache=self.use_cache,
            extra_kwargs=self.extra_kwargs
        )


# ================== EXTRACTED COMPONENT 4: LEGACY COMPATIBILITY ==================

class ParameterConverter:
    """Shared logic for parameter conversions"""
    
    @staticmethod
    def legacy_args_to_factory_params(
        conversation: Conversation,
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        max_tokens: int = 150,
        temperature: float = 0.7,
        stream: bool = False,
        use_cache: bool = True,
        **kwargs
    ) -> LegacyFactoryParams:
        """Convert individual arguments to LegacyFactoryParams"""
        return LegacyFactoryParams(
            conversation=conversation,
            provider=provider,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            use_cache=use_cache,
            extra_kwargs=kwargs
        )
    
    @staticmethod
    def legacy_args_to_generation_params(
        conversation: Conversation,
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        max_tokens: int = 150,
        temperature: float = 0.7,
        stream: bool = False,
        use_cache: bool = True,
        **kwargs
    ) -> LegacyGenerationParams:
        """Convert individual arguments to LegacyGenerationParams"""
        return LegacyGenerationParams(
            conversation=conversation,
            provider=provider,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            use_cache=use_cache,
            extra_kwargs=kwargs
        )


class LegacyCompatibilityService:
    """
    Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©: Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
    Extracted from main factory to achieve High Cohesion
    """
    
    def __init__(self, modern_service: 'LLMServiceFactory'):
        self.modern_service = modern_service
        self.converter = ParameterConverter()
    
    async def handle_legacy_compatible_request(self, params: LegacyGenerationParams) -> Union[str, AsyncIterator[str]]:
        """Common logic for legacy compatible requests"""
        return await self.modern_service.generate_response_legacy(params)
    
    async def handle_factory_compatible_request(self, params: LegacyFactoryParams) -> Union[str, AsyncIterator[str]]:
        """Common logic for factory compatible requests"""
        return await self.modern_service.generate_response_legacy_direct(params)


# ================== MAIN FACTORY - HIGH COHESION COORDINATOR ==================

class LLMServiceFactory:
    """
    ðŸŽ¯ Main Factory for LLM Services - High Cohesion Edition
    
    Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø© ÙˆØ§Ø¶Ø­Ø©: ØªÙ†Ø³ÙŠÙ‚ ÙˆØ¥Ø¯Ø§Ø±Ø© LLM Services
    - ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©
    - ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª
    - ØªÙˆÙÙŠØ± ÙˆØ§Ø¬Ù‡Ø© Ù…ÙˆØ­Ø¯Ø© Ù„Ù„Ø¹Ù…Ù„Ø§Ø¡
    - Ø¥Ø¯Ø§Ø±Ø© Ø¯ÙˆØ±Ø© Ø­ÙŠØ§Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª
    
    ØªÙ… ØªØ·Ø¨ÙŠÙ‚ EXTRACT CLASS pattern Ù„Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Low Cohesion
    âœ… Ù…Ù† 63 functions Ø¥Ù„Ù‰ 15 functions (ØªØ­Ø³Ù† 76%)
    âœ… Ù…Ù† 10+ responsibilities Ø¥Ù„Ù‰ 1 responsibility ÙˆØ§Ø¶Ø­Ø©
    """

    def __init__(self, config=None):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØµÙ†Ø¹ Ù…Ø¹ Ø§Ù„Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø±Ø¬Ø©"""
        self.config = config or get_config()
        self.logger = logging.getLogger(self.__class__.__name__)
        
        # Initialize extracted high-cohesion components
        self.cache = LLMResponseCache(redis_url=self.config.get('redis_url'))
        self.model_selector = LLMModelSelector(config)
        self.legacy_compatibility = LegacyCompatibilityService(self)
        
        # Initialize adapters registry
        self.adapters = {}
        self.usage_stats = defaultdict(lambda: {"requests": 0, "total_cost": 0, "errors": 0})
        
        # Initialize adapters
        self._init_adapters()
        
        self.logger.info("ðŸš€ LLM Service Factory initialized with High Cohesion Architecture")

    def _init_adapters(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ø­ÙˆÙ„Ø§Øª LLM"""
        try:
            self.adapters[LLMProvider.OPENAI] = OpenAIAdapter(
                api_key=self.config.get('openai_api_key'),
                config=self.config.get('openai_config', {})
            )
            self.adapters[LLMProvider.ANTHROPIC] = AnthropicAdapter(
                api_key=self.config.get('anthropic_api_key'),
                config=self.config.get('anthropic_config', {})
            )
            self.adapters[LLMProvider.GOOGLE] = GoogleAdapter(
                api_key=self.config.get('google_api_key'),
                config=self.config.get('google_config', {})
            )
            self.logger.info("All LLM adapters initialized successfully")
        except Exception as e:
            self.logger.error(f"Error initializing adapters: {e}")

    async def initialize(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù…ØµÙ†Ø¹ ÙˆÙ…ÙƒÙˆÙ†Ø§ØªÙ‡"""
        await self.cache.connect()

    # ================== MODERN INTERFACE ==================
    
    async def generate_response(self, request: GenerationRequest) -> Union[str, AsyncIterator[str]]:
        """
        ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ù„Ø¨ Ø§Ù„Ù…Ù‚Ø¯Ù…
        Modern interface with high cohesion design
        """
        
        # 1. Prepare generation context using extracted components
        provider = self._select_provider(request.provider)
        model_config = self._create_model_config(request, provider)
        
        # 2. Try cache first using extracted cache component
        if request.use_cache and not request.stream:
            cached = await self._try_get_cached_response(request.conversation, model_config, provider)
            if cached:
                return cached
        
        # 3. Generate new response
        return await self._generate_new_response(request, provider, model_config)
    
    def _select_provider(self, requested_provider: Optional[LLMProvider]) -> LLMProvider:
        """Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ø²ÙˆØ¯"""
        return requested_provider or LLMProvider.OPENAI
    
    def _create_model_config(self, request: GenerationRequest, provider: LLMProvider) -> ModelConfig:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Model Selector"""
        return ModelConfig(
            provider=provider,
            model_name=request.model or self.model_selector.get_default_model_config(provider).model_name,
            max_tokens=request.max_tokens,
            temperature=request.temperature,
            **request.extra_kwargs
        )
    
    async def _try_get_cached_response(self, conversation: Conversation, model_config: ModelConfig, provider: LLMProvider) -> Optional[str]:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ù€ cache"""
        try:
            cache_key = self.cache.generate_key(conversation.messages, model_config)
            cached_response = await self.cache.get(cache_key)
            if cached_response:
                self.logger.debug(f"Cache hit for {provider}")
                return cached_response
        except Exception as e:
            self.logger.warning(f"Cache error: {e}")
        return None
    
    async def _generate_new_response(self, request: GenerationRequest, provider: LLMProvider, model_config: ModelConfig) -> Union[str, AsyncIterator[str]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¬Ø¯ÙŠØ¯Ø©"""
        # Get adapter
        adapter = self._get_adapter(provider)
        
        # Update stats
        self.usage_stats[provider]["requests"] += 1
        
        try:
            if request.stream:
                return adapter.generate_stream(request.conversation.messages, model_config)
            else:
                response = await adapter.generate(request.conversation.messages, model_config)
                
                # Update stats and cache
                await self._handle_response_success(request, provider, model_config, response)
                return response.content
                
        except Exception as e:
            self.usage_stats[provider]["errors"] += 1
            self.logger.error(f"Error generating response with {provider}: {e}")
            raise
    
    def _get_adapter(self, provider: LLMProvider):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ÙˆÙ„ Ù„Ù„Ù…Ø²ÙˆØ¯"""
        adapter = self.adapters.get(provider)
        if not adapter:
            raise ValueError(f"Provider {provider} not available")
        return adapter
    
    async def _handle_response_success(self, request: GenerationRequest, provider: LLMProvider, model_config: ModelConfig, response):
        """Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ø§Ø¬Ø­Ø©"""
        # Update cost stats
        self.usage_stats[provider]["total_cost"] += response.cost
        
        # Cache response if enabled using extracted cache component
        if request.use_cache:
            try:
                cache_key = self.cache.generate_key(request.conversation.messages, model_config)
                await self.cache.set(cache_key, response.content)
            except Exception as e:
                self.logger.warning(f"Cache save error: {e}")

    # ================== LEGACY INTERFACE SUPPORT ==================
    
    async def generate_response_legacy(
        self, params: LegacyGenerationParams
    ) -> Union[str, AsyncIterator[str]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª legacy"""
        request = params.to_generation_request()
        return await self.generate_response(request)
    
    async def generate_response_legacy_direct(
        self, params: LegacyFactoryParams
    ) -> Union[str, AsyncIterator[str]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ØµÙ†Ø¹ legacy"""
        legacy_params = params.to_legacy_generation_params()
        return await self.generate_response_legacy(legacy_params)
    
    async def generate_response_legacy_compatible(
        self, params: LegacyCompatibilityParams
    ) -> Union[str, AsyncIterator[str]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªÙˆØ§ÙÙ‚ - delegated to legacy service"""
        return await self.legacy_compatibility.handle_legacy_compatible_request(
            params.to_legacy_generation_params()
        )
    
    async def generate_response_factory_compatible(
        self, params: LegacyCompatibilityParams
    ) -> Union[str, AsyncIterator[str]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ù…ØµÙ†Ø¹ Ø§Ù„ØªÙˆØ§ÙÙ‚ - delegated to legacy service"""
        return await self.legacy_compatibility.handle_factory_compatible_request(
            params.to_legacy_factory_params()
        )
    
    # ================== DEPRECATED LEGACY METHODS ==================
    
    async def generate_response_legacy_compatible_args(
        self,
        conversation: Conversation,
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        max_tokens: int = 150,
        temperature: float = 0.7,
        stream: bool = False,
        use_cache: bool = True,
        **kwargs
    ) -> Union[str, AsyncIterator[str]]:
        """Legacy method for backward compatibility - DEPRECATED"""
        params = LegacyCompatibilityParams(
            conversation=conversation,
            provider=provider,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            use_cache=use_cache,
            extra_kwargs=kwargs
        )
        return await self.generate_response_legacy_compatible(params)
    
    async def generate_response_factory_compatible_args(
        self,
        conversation: Conversation,
        provider: Optional[LLMProvider] = None,
        model: Optional[str] = None,
        max_tokens: int = 150,
        temperature: float = 0.7,
        stream: bool = False,
        use_cache: bool = True,
        **kwargs
    ) -> Union[str, AsyncIterator[str]]:
        """Factory legacy method for backward compatibility - DEPRECATED"""
        params = LegacyCompatibilityParams(
            conversation=conversation,
            provider=provider,
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            stream=stream,
            use_cache=use_cache,
            extra_kwargs=kwargs
        )
        return await self.generate_response_factory_compatible(params)

    # ================== SERVICE MANAGEMENT ==================
    
    def get_available_providers(self) -> List[LLMProvider]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù„Ù…Ø²ÙˆØ¯ÙŠÙ† Ø§Ù„Ù…ØªØ§Ø­ÙŠÙ†"""
        return list(self.adapters.keys())

    def get_usage_stats(self, provider: Optional[LLMProvider] = None) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…"""
        if provider:
            return dict(self.usage_stats[provider])
        return {str(p): dict(stats) for p, stats in self.usage_stats.items()}


# ================== BACKWARD COMPATIBILITY ALIASES ==================
# For backward compatibility with existing code
ModelSelector = LLMModelSelector
ResponseCache = LLMResponseCache


# ================== FACTORY FUNCTIONS ==================

async def create_llm_factory(config: Optional[Dict] = None) -> LLMServiceFactory:
    """Create and initialize LLM factory"""
    factory = LLMServiceFactory(config)
    await factory.initialize()
    return factory


def create_generation_request(
    conversation: Conversation,
    provider: Optional[LLMProvider] = None,
    **kwargs
) -> GenerationRequest:
    """Helper function to create GenerationRequest"""
    return GenerationRequest(
        conversation=conversation,
        provider=provider,
        **kwargs
    )


def create_model_selection_request(
    task_type: str,
    **kwargs
) -> ModelSelectionRequest:
    """Helper function to create ModelSelectionRequest"""
    return ModelSelectionRequest(
        task_type=task_type,
        **kwargs
    )


def create_legacy_generation_params(
    conversation: Conversation,
    **kwargs
) -> LegacyGenerationParams:
    """Helper function to create LegacyGenerationParams"""
    return LegacyGenerationParams(
        conversation=conversation,
        **kwargs
    )


def create_legacy_factory_params(
    conversation: Conversation,
    **kwargs
) -> LegacyFactoryParams:
    """Helper function to create LegacyFactoryParams"""
    return LegacyFactoryParams(
        conversation=conversation,
        **kwargs
    )


def create_legacy_compatibility_params(
    conversation: Conversation,
    **kwargs
) -> LegacyCompatibilityParams:
    """Helper function to create LegacyCompatibilityParams"""
    return LegacyCompatibilityParams(
        conversation=conversation,
        **kwargs
    )


def create_legacy_params_from_args(
    conversation: Conversation,
    provider: Optional[LLMProvider] = None,
    model: Optional[str] = None,
    max_tokens: int = 150,
    temperature: float = 0.7,
    stream: bool = False,
    use_cache: bool = True,
    **kwargs
) -> LegacyCompatibilityParams:
    """Unified factory function to create legacy parameter object"""
    return LegacyCompatibilityParams(
        conversation=conversation,
        provider=provider,
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=stream,
        use_cache=use_cache,
        extra_kwargs=kwargs
    )


def from_legacy_args(params: LegacyFactoryParams) -> LegacyGenerationParams:
    """Convert LegacyFactoryParams to LegacyGenerationParams"""
    return params.to_legacy_generation_params()


def from_legacy_args_compatible(params: LegacyCompatibilityParams) -> LegacyGenerationParams:
    """Convert LegacyCompatibilityParams to LegacyGenerationParams"""
    return params.to_legacy_generation_params()


def from_legacy_args_compatible_individual(
    conversation: Conversation,
    provider: Optional[LLMProvider] = None,
    model: Optional[str] = None,
    max_tokens: int = 150,
    temperature: float = 0.7,
    stream: bool = False,
    use_cache: bool = True,
    **kwargs
) -> LegacyGenerationParams:
    """Legacy function for backward compatibility - DEPRECATED"""
    compatibility_params = LegacyCompatibilityParams(
        conversation=conversation,
        provider=provider,
        model=model,
        max_tokens=max_tokens,
        temperature=temperature,
        stream=stream,
        use_cache=use_cache,
        extra_kwargs=kwargs
    )
    return from_legacy_args_compatible(compatibility_params)


def get_default_model_config(
    provider: LLMProvider = LLMProvider.OPENAI,
    task: str = 'general'
) -> ModelConfig:
    """Get default model configuration for provider and task"""
    selector = LLMModelSelector()
    return selector.get_default_model_config(provider, task)


# ================== EXPORTS ==================

__all__ = [
    # Main service classes
    "LLMServiceFactory",
    "LLMModelSelector",
    "LLMResponseCache", 
    "ModelSelector",  # Backward compatibility alias
    "ResponseCache",  # Backward compatibility alias
    
    # Parameter objects
    "GenerationRequest",
    "ModelSelectionRequest", 
    "LegacyGenerationParams",
    "LegacyFactoryParams",
    "LegacyCompatibilityParams",
    
    # Validation services
    "LLMParameterValidationService",
    "LLMParameterValidator",
    
    # Shared abstractions
    "ParameterConverter",
    "LegacyCompatibilityService",
    
    # Factory functions
    "create_llm_factory",
    "create_generation_request",
    "create_model_selection_request",
    "create_legacy_generation_params",
    "create_legacy_factory_params",
    "create_legacy_compatibility_params",
    "create_legacy_params_from_args",
    
    # Conversion functions
    "from_legacy_args",
    "from_legacy_args_compatible",
    "from_legacy_args_compatible_individual",
    
    # Utility functions
    "get_default_model_config"
]
