#!/usr/bin/env python3
"""
ðŸš€ Enhanced LLM Service Factory
Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ "Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ÙˆØ¹Ø±Ø©" ÙˆØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¯ÙˆØ±ÙŠ

Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©:
âœ… Parameter Objects Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 7+ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
âœ… ØªÙ‚Ø³ÙŠÙ… generate_response Ù…Ù† 60+ Ø³Ø·Ø± Ø¥Ù„Ù‰ Ø¯ÙˆØ§Ù„ ØµØºÙŠØ±Ø©  
âœ… ØªØ¨Ø³ÙŠØ· ResponseCache.get Ù…Ù† Ù…Ù†Ø·Ù‚ Ù…Ø¹Ù‚Ø¯ Ø¥Ù„Ù‰ strategy pattern
âœ… ÙØµÙ„ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª - ÙƒÙ„ ÙƒÙ„Ø§Ø³ Ù„Ù‡ Ù…Ù‡Ù…Ø© ÙˆØ§Ø­Ø¯Ø©
âœ… Strategy Pattern Ù„Ù„cache
âœ… Service Layer Ù„Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
"""

import asyncio
import hashlib
import json
import logging
from abc import ABC, abstractmethod
from collections import defaultdict
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, AsyncIterator, Dict, List, Optional, Union


# ================== PARAMETER OBJECTS ==================

@dataclass
class GenerationRequest:
    """ðŸ“¦ Parameter Object - Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© 7+ Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
    conversation: Any  # Mock conversation type
    provider: Optional[str] = None
    model: Optional[str] = None
    max_tokens: int = 150
    temperature: float = 0.7
    stream: bool = False
    use_cache: bool = True
    task_type: str = "general"


@dataclass
class CacheRequest:
    """ðŸ“¦ Parameter Object Ù„Ù„cache operations"""
    key: str
    value: Optional[str] = None
    ttl: int = 3600


@dataclass
class GenerationContext:
    """ðŸ“¦ Context object Ù„Ù„Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
    request: GenerationRequest
    model_config: dict
    cache_key: Optional[str] = None
    start_time: datetime = field(default_factory=datetime.now)


# ================== CACHE STRATEGIES ==================

class CacheStrategy(ABC):
    """ðŸ—ï¸ Strategy Pattern Ù„Ù„cache - Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© ResponseCache.get Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©"""
    
    @abstractmethod
    async def get(self, request: CacheRequest) -> Optional[str]:
        pass
    
    @abstractmethod
    async def set(self, request: CacheRequest) -> bool:
        pass


class LocalCacheStrategy(CacheStrategy):
    """ðŸ’¾ Local cache strategy"""
    
    def __init__(self, max_size: int = 1000):
        self.cache: Dict[str, tuple] = {}
        self.max_size = max_size
    
    async def get(self, request: CacheRequest) -> Optional[str]:
        """ðŸ” Get from local cache - Ù…Ù†Ø·Ù‚ Ù…Ø¨Ø³Ø·"""
        if request.key not in self.cache:
            return None
        
        value, expiry = self.cache[request.key]
        
        if self._is_expired(expiry):
            self._remove_expired_entry(request.key)
            return None
        
        return value
    
    async def set(self, request: CacheRequest) -> bool:
        """ðŸ’¾ Set in local cache"""
        if len(self.cache) >= self.max_size:
            self._cleanup_old_entries()
        
        expiry = datetime.now() + timedelta(seconds=request.ttl)
        self.cache[request.key] = (request.value, expiry)
        return True
    
    def _is_expired(self, expiry: datetime) -> bool:
        """â° Check if expired - single responsibility"""
        return datetime.now() >= expiry
    
    def _remove_expired_entry(self, key: str) -> None:
        """ðŸ—‘ï¸ Remove expired entry - single responsibility"""
        self.cache.pop(key, None)
    
    def _cleanup_old_entries(self) -> None:
        """ðŸ§¹ Cleanup old entries - single responsibility"""
        items_to_remove = len(self.cache) // 5  # Remove 20%
        oldest_keys = sorted(
            self.cache.keys(),
            key=lambda k: self.cache[k][1]
        )[:items_to_remove]
        
        for key in oldest_keys:
            self.cache.pop(key, None)


class HybridCacheStrategy(CacheStrategy):
    """ðŸ”„ Hybrid strategy with fallback"""
    
    def __init__(self):
        self.primary = LocalCacheStrategy()
        self.fallback = LocalCacheStrategy(max_size=500)
    
    async def get(self, request: CacheRequest) -> Optional[str]:
        """ðŸ” Get with fallback - Ù…Ø¨Ø³Ø·"""
        # Try primary first
        result = await self.primary.get(request)
        if result:
            return result
        
        # Fallback
        return await self.fallback.get(request)
    
    async def set(self, request: CacheRequest) -> bool:
        """ðŸ’¾ Set in both caches"""
        primary_success = await self.primary.set(request)
        fallback_success = await self.fallback.set(request)
        return primary_success or fallback_success


# ================== ENHANCED CACHE MANAGER ==================

class EnhancedResponseCache:
    """ðŸ“¦ Cache manager Ù…Ø¹ strategy pattern - Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ÙˆØ¹Ø±Ø©"""
    
    def __init__(self, redis_url: Optional[str] = None):
        self.strategy = HybridCacheStrategy()
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def get(self, key: str) -> Optional[str]:
        """ðŸ” Get response - Ù…Ø¨Ø³Ø· Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù…Ù†Ø·Ù‚ Ù…Ø¹Ù‚Ø¯"""
        request = CacheRequest(key=key)
        return await self.strategy.get(request)
    
    async def set(self, key: str, value: str, ttl: int = 3600):
        """ðŸ’¾ Set response - Ù…Ø¨Ø³Ø·"""
        request = CacheRequest(key=key, value=value, ttl=ttl)
        return await self.strategy.set(request)
    
    def generate_key(self, messages: List, model_config: dict) -> str:
        """ðŸ”‘ Generate cache key - Ù…Ù†Ø·Ù‚ Ù…Ø¨Ø³Ø·"""
        try:
            content_hash = hashlib.md5(str(messages).encode()).hexdigest()
            config_hash = hashlib.md5(str(model_config).encode()).hexdigest()
            return f"llm:{content_hash}:{config_hash}"
        except Exception:
            return f"llm:fallback:{hash(str(messages))}"


# ================== MODEL SELECTION SERVICE ==================

class ModelSelectionService:
    """ðŸŽ¯ Model selection service - Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ù…Ù†ÙØµÙ„Ø©"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def select_optimal_model(self, request: GenerationRequest) -> dict:
        """ðŸŽ¯ Select best model - Ù…Ù†Ø·Ù‚ Ù…Ø¨Ø³Ø·"""
        if request.provider:
            return self._create_config_for_provider(request)
        
        return self._auto_select_by_task(request)
    
    def _create_config_for_provider(self, request: GenerationRequest) -> dict:
        """ðŸ”§ Create config for specific provider"""
        return {
            "provider": request.provider,
            "model_name": request.model or "default",
            "max_tokens": request.max_tokens,
            "temperature": request.temperature
        }
    
    def _auto_select_by_task(self, request: GenerationRequest) -> dict:
        """ðŸ¤– Auto-select by task type"""
        task_configs = {
            "creative": {"provider": "anthropic", "model": "claude-3", "temperature": 0.8},
            "analysis": {"provider": "openai", "model": "gpt-4", "temperature": 0.3},
            "general": {"provider": "openai", "model": "gpt-3.5-turbo", "temperature": 0.7}
        }
        
        config = task_configs.get(request.task_type, task_configs["general"])
        config.update({
            "max_tokens": request.max_tokens,
            "temperature": request.temperature
        })
        
        return config


# ================== USAGE STATISTICS SERVICE ==================

class UsageStatsService:
    """ðŸ“Š Statistics service - Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ù…Ù†ÙØµÙ„Ø©"""
    
    def __init__(self):
        self.stats = defaultdict(lambda: {
            "requests": 0,
            "total_cost": 0.0,
            "errors": 0,
            "cache_hits": 0,
            "avg_response_time": 0.0
        })
        self.response_times = defaultdict(list)
    
    def record_request(self, provider: str):
        """ðŸ“ Record request start"""
        self.stats[provider]["requests"] += 1
    
    def record_response(self, provider: str, response_time: float, cost: float = 0.0):
        """ðŸ“ˆ Record successful response"""
        self.stats[provider]["total_cost"] += cost
        self.response_times[provider].append(response_time)
        
        # Update average
        times = self.response_times[provider]
        self.stats[provider]["avg_response_time"] = sum(times) / len(times)
    
    def record_error(self, provider: str):
        """âŒ Record error"""
        self.stats[provider]["errors"] += 1
    
    def record_cache_hit(self, provider: str):
        """ðŸ’¾ Record cache hit"""
        self.stats[provider]["cache_hits"] += 1
    
    def get_stats(self, provider: Optional[str] = None) -> Dict[str, Any]:
        """ðŸ“Š Get statistics"""
        if provider:
            return dict(self.stats[provider])
        return {p: dict(stats) for p, stats in self.stats.items()}


# ================== RESPONSE GENERATION SERVICE ==================

class ResponseGenerationService:
    """ðŸ¤– Response generation service - Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© Ù…Ù†ÙØµÙ„Ø©"""
    
    def __init__(self, adapters: Dict):
        self.adapters = adapters
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def generate_streaming_response(self, context: GenerationContext) -> AsyncIterator[str]:
        """ðŸŒŠ Generate streaming response"""
        # Mock streaming
        response_parts = ["Ù…Ø±Ø­Ø¨Ø§! ", "ÙƒÙŠÙ ", "ÙŠÙ…ÙƒÙ†Ù†ÙŠ ", "Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"]
        for part in response_parts:
            yield part
            await asyncio.sleep(0.1)
    
    async def generate_standard_response(self, context: GenerationContext) -> tuple:
        """ðŸ“ Generate standard response"""
        # Mock generation
        response_content = "Ù…Ø±Ø­Ø¨Ø§! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ"
        cost = 0.001
        
        return response_content, cost
    
    def get_adapter(self, provider: str):
        """ðŸ”Œ Get adapter for provider"""
        adapter = self.adapters.get(provider)
        if not adapter:
            raise ValueError(f"Provider {provider} not available")
        return adapter


# ================== ENHANCED LLM SERVICE FACTORY ==================

class EnhancedLLMServiceFactory:
    """
    ðŸš€ Enhanced LLM Service Factory
    
    Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ "Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ÙˆØ¹Ø±Ø©":
    âœ… ØªÙ‚Ø³ÙŠÙ… generate_response Ù…Ù† 60+ Ø³Ø·Ø± Ø¥Ù„Ù‰ Ø¯ÙˆØ§Ù„ ØµØºÙŠØ±Ø©
    âœ… Parameter Objects Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† 7+ Ù…Ø¹Ø§Ù…Ù„Ø§Øª
    âœ… Strategy Pattern Ù„Ù„cache
    âœ… ÙØµÙ„ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª Ø¥Ù„Ù‰ services Ù…Ù†ÙØµÙ„Ø©
    âœ… ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø´Ø±Ø·ÙŠ
    """
    
    def __init__(self, config=None):
        self.config = config or {}
        self.adapters = {}
        
        # Inject dependencies - Dependency Injection Pattern
        self.cache = EnhancedResponseCache(config.get('redis_url') if config else None)
        self.model_selector = ModelSelectionService(config)
        self.stats_service = UsageStatsService()
        self.response_service = ResponseGenerationService(self.adapters)
        
        self.logger = logging.getLogger(self.__class__.__name__)
    
    async def initialize(self):
        """ðŸš€ Initialize factory"""
        self.logger.info("Enhanced LLM Factory initialized")
    
    async def generate_response(self, request: GenerationRequest) -> Union[str, AsyncIterator[str]]:
        """
        ðŸŽ¯ Generate response - Ù…Ø¨Ø³Ø· ÙˆÙ…Ø­Ø³Ù†
        
        Ù‚Ø¨Ù„ Ø§Ù„ØªØ­Ø³ÙŠÙ†: 60+ Ø³Ø·Ø± Ù…Ø¹ Ù…Ù†Ø·Ù‚ Ù…Ø¹Ù‚Ø¯
        Ø¨Ø¹Ø¯ Ø§Ù„ØªØ­Ø³ÙŠÙ†: Ø¯ÙˆØ§Ù„ ØµØºÙŠØ±Ø© Ù…Ø¹ Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª ÙˆØ§Ø¶Ø­Ø©
        """
        
        # 1. Create context
        context = await self._create_generation_context(request)
        
        # 2. Try cache first
        if request.use_cache and not request.stream:
            cached_response = await self._try_get_cached_response(context)
            if cached_response:
                return cached_response
        
        # 3. Generate new response
        return await self._generate_new_response(context)
    
    async def _create_generation_context(self, request: GenerationRequest) -> GenerationContext:
        """ðŸ“‹ Create generation context - Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©"""
        model_config = self.model_selector.select_optimal_model(request)
        
        cache_key = None
        if request.use_cache:
            cache_key = self.cache.generate_key(
                request.conversation or [],
                model_config
            )
        
        return GenerationContext(
            request=request,
            model_config=model_config,
            cache_key=cache_key
        )
    
    async def _try_get_cached_response(self, context: GenerationContext) -> Optional[str]:
        """ðŸ’¾ Try to get cached response - Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©"""
        if not context.cache_key:
            return None
        
        try:
            cached_response = await self.cache.get(context.cache_key)
            if cached_response:
                provider = context.model_config.get("provider", "unknown")
                self.stats_service.record_cache_hit(provider)
                self.logger.debug(f"Cache hit for {provider}")
                return cached_response
        except Exception as e:
            self.logger.warning(f"Cache retrieval error: {e}")
        
        return None
    
    async def _generate_new_response(self, context: GenerationContext) -> Union[str, AsyncIterator[str]]:
        """ðŸ¤– Generate new response - Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© ÙˆØ§Ø­Ø¯Ø©"""
        provider = context.model_config.get("provider", "unknown")
        
        # Record request start
        self.stats_service.record_request(provider)
        
        try:
            if context.request.stream:
                return await self.response_service.generate_streaming_response(context)
            else:
                return await self._generate_and_cache_response(context)
                
        except Exception as e:
            self.stats_service.record_error(provider)
            self.logger.error(f"Error generating response: {e}")
            raise
    
    async def _generate_and_cache_response(self, context: GenerationContext) -> str:
        """ðŸ“ Generate and cache standard response"""
        provider = context.model_config.get("provider", "unknown")
        
        # Generate response
        response_content, cost = await self.response_service.generate_standard_response(context)
        
        # Record stats
        response_time = (datetime.now() - context.start_time).total_seconds()
        self.stats_service.record_response(provider, response_time, cost)
        
        # Cache response
        if context.request.use_cache and context.cache_key:
            await self._cache_response(context.cache_key, response_content)
        
        return response_content
    
    async def _cache_response(self, cache_key: str, response: str):
        """ðŸ’¾ Cache response safely"""
        try:
            await self.cache.set(cache_key, response)
        except Exception as e:
            self.logger.warning(f"Cache save error: {e}")
    
    def get_available_providers(self) -> List[str]:
        """ðŸ“‹ Get available providers"""
        return list(self.adapters.keys()) or ["openai", "anthropic", "google"]
    
    def get_usage_stats(self, provider: Optional[str] = None) -> Dict[str, Any]:
        """ðŸ“Š Get usage statistics"""
        return self.stats_service.get_stats(provider)


# ================== FACTORY FUNCTIONS ==================

async def create_enhanced_llm_factory(config: Optional[Dict] = None) -> EnhancedLLMServiceFactory:
    """ðŸ­ Factory function"""
    factory = EnhancedLLMServiceFactory(config)
    await factory.initialize()
    return factory


def create_generation_request(
    conversation: Any,
    provider: Optional[str] = None,
    **kwargs
) -> GenerationRequest:
    """ðŸ“¦ Helper function to create GenerationRequest"""
    return GenerationRequest(
        conversation=conversation,
        provider=provider,
        **kwargs
    )


# ================== BACKWARD COMPATIBILITY ==================

# Aliases for backward compatibility
LLMServiceFactory = EnhancedLLMServiceFactory
create_llm_factory = create_enhanced_llm_factory
ResponseCache = EnhancedResponseCache 