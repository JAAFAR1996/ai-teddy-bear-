"""
ðŸ¤– OpenAI Service - Enterprise 2025 Implementation
Modern OpenAI integration with advanced features and error handling
"""

import asyncio
import hashlib
import json
import logging
import time
from datetime import datetime
from functools import lru_cache
from typing import Any, Dict, List, Optional

from openai import APIError, APITimeoutError, AsyncOpenAI, RateLimitError
from openai.types.chat import ChatCompletion

from src.application.services.ai.analyzers.emotion_analyzer_service import \
    EmotionAnalyzerService
from src.application.services.ai.fallback_response_service import \
    FallbackResponseService
from src.application.services.ai.core import \
    IAIService
from src.application.services.ai.models.ai_response_models import \
    AIResponseModel
from src.core.domain.entities.child import Child
from src.infrastructure.caching.simple_cache_service import CacheService
from src.infrastructure.config import Settings

logger = logging.getLogger(__name__)


class ModernOpenAIService(IAIService):
    """
    ðŸš€ Modern OpenAI implementation with 2025 enterprise features:
    - Advanced caching with LRU + TTL
    - Comprehensive error handling
    - Active emotion analysis
    - Performance monitoring
    - Circuit breaker pattern
    """

    def __init__(
        self,
        settings: Settings,
        cache_service: CacheService,
        emotion_analyzer: EmotionAnalyzerService,
        fallback_service: FallbackResponseService,
    ):
        self.settings = settings
        self.cache = cache_service
        self.emotion_analyzer = emotion_analyzer
        self.fallback_service = fallback_service
        self.client = None

        # Enhanced caching
        self.memory_cache: Dict[str, tuple] = {}
        self.cache_ttl = 3600  # 1 hour
        self.max_cache_size = 1000

        # Performance tracking
        self.request_count = 0
        self.total_processing_time = 0
        self.error_count = 0
        self.rate_limit_count = 0

        # Conversation management
        self.conversation_history: Dict[str, List[Dict]] = {}
        self.max_history_length = 10

        self._initialize_client()
        logger.info("âœ… Modern OpenAI Service initialized with enhanced features")

    def _initialize_client(self) -> None:
        """Initialize OpenAI client with comprehensive error handling"""
        try:
            api_key = self.settings.openai_api_key
            if not api_key:
                logger.error("ðŸš« OpenAI API key not configured")
                raise ValueError("OpenAI API key is required for AI service")

            self.client = AsyncOpenAI(api_key=api_key, timeout=30.0, max_retries=3)
            logger.info("âœ… OpenAI client initialized successfully")

        except Exception as e:
            logger.error(
                f"âŒ Failed to initialize OpenAI client: {str(e)}", exc_info=True
            )
            raise

    @lru_cache(maxsize=1000)
    def _get_cache_key(self, text: str, context: str, child_profile: str) -> str:
        """Generate optimized cache key with LRU caching"""
        combined = f"{text}:{context}:{child_profile}"
        return hashlib.md5(combined.encode()).hexdigest()

    def _get_child_profile_key(self, child: Child) -> str:
        """Generate child profile key for caching"""
        return f"{child.name}:{child.age}:{getattr(child, 'learning_level', 'basic')}"

    def _check_memory_cache(self, cache_key: str) -> Optional[AIResponseModel]:
        """Check memory cache with TTL validation"""
        if cache_key in self.memory_cache:
            response_dict, timestamp = self.memory_cache[cache_key]
            if time.time() - timestamp < self.cache_ttl:
                logger.debug(f"ðŸŽ¯ Memory cache hit for key: {cache_key[:8]}...")
                response = AIResponseModel(**response_dict)
                response.cached = True
                return response
            else:
                # Remove expired entry
                del self.memory_cache[cache_key]
                logger.debug(f"ðŸ§¹ Expired cache entry removed: {cache_key[:8]}...")

        return None

    def _store_in_memory_cache(self, cache_key: str, response: AIResponseModel) -> None:
        """Store response in memory cache with size management"""
        # Clean old entries if cache is full
        if len(self.memory_cache) >= self.max_cache_size:
            # Remove 10% oldest entries
            sorted_entries = sorted(
                self.memory_cache.items(), key=lambda x: x[1][1]
            )  # Sort by timestamp
            entries_to_remove = int(self.max_cache_size * 0.1)
            for key, _ in sorted_entries[:entries_to_remove]:
                del self.memory_cache[key]
            logger.debug(f"ðŸ§¹ Cleaned {entries_to_remove} old cache entries")

        # Store new entry
        response_dict = response.to_dict()
        response_dict["cached"] = False  # Don't store cached flag
        self.memory_cache[cache_key] = (response_dict, time.time())
        logger.debug(f"ðŸ’¾ Stored in memory cache: {cache_key[:8]}...")

    async def generate_response(
        self,
        message: str,
        child: Child,
        session_id: Optional[str] = None,
        context: Optional[Dict[str, Any]] = None,
    ) -> AIResponseModel:
        """ðŸš€ Enhanced response generation with modern 2025 features"""
        start_time = datetime.utcnow()
        self.request_count += 1

        try:
            # Generate session ID if not provided
            if not session_id:
                device_id = getattr(child, "device_id", "unknown")
                session_id = f"session_{device_id}_{int(datetime.utcnow().timestamp())}"

            # Check for wake words (fast path)
            if self._is_wake_word_only(message):
                return await self._create_wake_word_response(child, session_id)

            # Enhanced caching strategy
            child_profile = self._get_child_profile_key(child)
            context_str = json.dumps(context or {}, sort_keys=True)
            cache_key = self._get_cache_key(message, context_str, child_profile)

            # Check memory cache first (fastest)
            cached_response = self._check_memory_cache(cache_key)
            if cached_response:
                logger.info("ðŸŽ¯ Memory cache hit - response time: <1ms")
                return cached_response

            # Check persistent cache
            persistent_cached = await self.cache.get(f"ai_response_{cache_key}")
            if persistent_cached:
                logger.info("ðŸŽ¯ Persistent cache hit")
                response = AIResponseModel(**json.loads(persistent_cached))
                response.cached = True
                # Store in memory for next time
                self._store_in_memory_cache(cache_key, response)
                return response

            # ðŸŽ­ Activate emotion analyzer (parallel processing)
            emotion_task = asyncio.create_task(self._enhanced_emotion_analysis(message))
            category_task = asyncio.create_task(self.categorize_message(message))

            # Get conversation history
            device_id = getattr(child, "device_id", "unknown")
            history = self._get_conversation_history(device_id)

            # Build enhanced system prompt with emotion context
            system_prompt = await self._build_enhanced_system_prompt(child, context)

            # ðŸ¤– Call OpenAI API with comprehensive error handling
            response = await self._enhanced_openai_call(
                message=message,
                system_prompt=system_prompt,
                history=history,
                emotion_context=await emotion_task,
            )

            # Extract response data
            response_text = response.choices[0].message.content.strip()

            # Wait for parallel tasks
            emotion, category = await asyncio.gather(emotion_task, category_task)
            learning_points = await self._extract_learning_points(
                message, response_text
            )

            # Calculate processing time
            processing_time = int(
                (datetime.utcnow() - start_time).total_seconds() * 1000
            )
            self.total_processing_time += processing_time

            # Create enhanced response model
            ai_response = AIResponseModel(
                text=response_text,
                emotion=emotion,
                category=category,
                learning_points=learning_points,
                session_id=session_id,
                confidence=0.95,
                processing_time_ms=processing_time,
                cached=False,
                model_used=response.model,
                usage=response.usage.model_dump() if response.usage else {},
            )

            # Update conversation history
            self._update_conversation_history(
                device_id=device_id,
                message=message,
                response=response_text,
                emotion=emotion,
            )

            # Store in both caches
            self._store_in_memory_cache(cache_key, ai_response)
            await self.cache.set(
                f"ai_response_{cache_key}",
                json.dumps(ai_response.to_dict()),
                ttl=self.cache_ttl,
            )

            logger.info(
                f"âœ… AI response generated in {processing_time}ms (model: {response.model})"
            )
            return ai_response

        except RateLimitError:
            self.rate_limit_count += 1
            logger.warning(f"âš ï¸ OpenAI rate limit hit (#{self.rate_limit_count})")
            return await self.fallback_service.create_rate_limit_fallback(
                message, child, session_id
            )

        except APITimeoutError as e:
            self.error_count += 1
            logger.error(f"â° OpenAI API timeout: {str(e)}")
            return await self.fallback_service.create_timeout_fallback(
                message, child, session_id
            )

        except APIError as e:
            self.error_count += 1
            logger.error(f"ðŸš« OpenAI API error: {str(e)}", exc_info=True)
            return await self.fallback_service.create_api_error_fallback(
                message, child, session_id, str(e)
            )

        except Exception as e:
            self.error_count += 1
            logger.error(f"ðŸ’¥ Unexpected AI service error: {str(e)}", exc_info=True)
            return await self.fallback_service.create_generic_fallback(
                message, child, session_id, str(e)
            )

    async def _enhanced_openai_call(
        self,
        message: str,
        system_prompt: str,
        history: List[Dict],
        emotion_context: str,
    ) -> ChatCompletion:
        """Enhanced OpenAI API call with emotion context"""
        messages = [{"role": "system", "content": system_prompt}]

        # Add conversation history
        messages.extend(history[-self.max_history_length :])

        # Add emotion context to message
        enhanced_message = (
            f"{message}\n[Context: Child's emotion appears to be {emotion_context}]"
        )
        messages.append({"role": "user", "content": enhanced_message})

        try:
            async with asyncio.timeout(25):
                response = await self.client.chat.completions.create(
                    model=self.settings.openai_model or "gpt-4-turbo-preview",
                    messages=messages,
                    max_tokens=200,
                    temperature=0.7,  # Slightly more deterministic
                    presence_penalty=0.3,
                    frequency_penalty=0.3,
                    top_p=0.9,
                )
                return response

        except asyncio.TimeoutError:
            logger.error("â° OpenAI API call timed out after 25 seconds")
            raise APITimeoutError("API call timed out")

    async def _enhanced_emotion_analysis(self, message: str) -> str:
        """ðŸŽ­ Enhanced emotion analysis with fallback"""
        try:
            emotion_result = await self.emotion_analyzer.analyze_text_emotion(message)
            return emotion_result.primary_emotion

        except Exception as e:
            logger.warning(f"âš ï¸ Emotion analyzer failed, using fallback: {str(e)}")
            return self._basic_emotion_detection(message)

    def _basic_emotion_detection(self, message: str) -> str:
        """Basic rule-based emotion detection"""
        message_lower = message.lower()

        if any(word in message_lower for word in ["Ø³Ø¹ÙŠØ¯", "happy", "ÙØ±Ø­", "Ù…Ø¨Ø³ÙˆØ·"]):
            return "joy"
        elif any(word in message_lower for word in ["Ø­Ø²ÙŠÙ†", "sad", "Ø¨ÙƒÙŠ", "Ø²Ø¹Ù„Ø§Ù†"]):
            return "sadness"
        elif any(word in message_lower for word in ["ØºØ¶Ø¨", "angry", "Ø²Ø¹Ù„", "Ø¹ØµØ¨ÙŠ"]):
            return "anger"
        elif any(word in message_lower for word in ["Ø®ÙˆÙ", "scared", "afraid", "Ø®Ø§Ø¦Ù"]):
            return "fear"
        elif any(word in message_lower for word in ["Ø­Ø¨", "love", "Ø£Ø­Ø¨"]):
            return "love"
        elif any(word in message_lower for word in ["Ù…ØªØ­Ù…Ø³", "excited", "Ø±Ø§Ø¦Ø¹", "ÙˆØ§Ùˆ"]):
            return "excitement"

        return "neutral"

    async def analyze_emotion(self, message: str) -> str:
        """Analyze emotion from message"""
        return await self._enhanced_emotion_analysis(message)

    async def categorize_message(self, message: str) -> str:
        """Enhanced message categorization"""
        message_lower = message.lower()

        if any(word in message_lower for word in ["Ù‚ØµØ©", "story", "Ø­ÙƒØ§ÙŠØ©", "Ø§Ø­ÙƒÙŠ"]):
            return "story_request"
        elif any(word in message_lower for word in ["Ù„Ø¹Ø¨", "play", "game", "Ù†Ù„Ø¹Ø¨"]):
            return "play_request"
        elif any(word in message_lower for word in ["ØªØ¹Ù„Ù…", "learn", "Ø¯Ø±Ø³", "Ø¹Ù„Ù…Ù†ÙŠ"]):
            return "learning_inquiry"
        elif any(word in message_lower for word in ["ØºÙ†Ø§Ø¡", "sing", "Ø£ØºÙ†ÙŠØ©", "ØºÙ†ÙŠ"]):
            return "music_request"
        elif any(word in message_lower for word in ["?", "ØŸ", "ÙƒÙŠÙ", "Ù„Ù…Ø§Ø°Ø§", "Ù…ØªÙ‰"]):
            return "question"
        elif any(
            word in message_lower for word in ["Ù…Ø±Ø­Ø¨Ø§", "hello", "Ø£Ù‡Ù„Ø§", "Ø§Ù„Ø³Ù„Ø§Ù…"]
        ):
            return "greeting"

        return "general_conversation"

    async def _build_enhanced_system_prompt(
        self, child: Child, context: Optional[Dict[str, Any]] = None
    ) -> str:
        """Build enhanced system prompt with context awareness"""
        base_prompt = f"""Ø£Ù†Øª Ø¯Ø¨Ø¯ÙˆØ¨ØŒ Ø¯Ø¨ Ù…Ø­Ø¨ÙˆØ¨ ÙˆØ°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ø£Ø·ÙØ§Ù„ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©.

Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·ÙÙ„:
- Ø§Ù„Ø§Ø³Ù…: {child.name}
- Ø§Ù„Ø¹Ù…Ø±: {child.age} Ø³Ù†ÙˆØ§Øª
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù„Ù…: {getattr(child, 'learning_level', 'Ù…ØªÙˆØ³Ø·')}
- Ø§Ù„Ø¬Ù‡Ø§Ø²: {getattr(child, 'device_id', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯')}

Ø´Ø®ØµÙŠØªÙƒ Ø§Ù„Ù…Ø­Ø¯Ø«Ø© 2025:
- Ù…Ø­Ø¨ÙˆØ¨ ÙˆÙˆØ¯ÙˆØ¯ ÙˆÙ…Ø±Ø­ ÙˆØ°ÙƒÙŠ
- ØªØªÙƒÙŠÙ Ù…Ø¹ Ù…Ø´Ø§Ø¹Ø± Ø§Ù„Ø·ÙÙ„ ÙˆØ­Ø§Ù„ØªÙ‡ Ø§Ù„Ù†ÙØ³ÙŠØ©
- ØªØ³ØªØ®Ø¯Ù… ØªÙ‚Ù†ÙŠØ§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø­Ø¯ÙŠØ«Ø© ÙˆØ§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ø¥ÙŠØ¬Ø§Ø¨ÙŠ
- ØªØ´Ø¬Ø¹ Ø§Ù„ÙØ¶ÙˆÙ„ ÙˆØ§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ ÙˆØ§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ
- ØªÙ‚Ø¯Ù… Ù…Ø­ØªÙˆÙ‰ ØªØ¹Ù„ÙŠÙ…ÙŠ Ù…Ù…ØªØ¹ ÙˆÙ…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ù…Ø±

Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„ØªÙØ§Ø¹Ù„ Ø§Ù„Ù…Ø­Ø¯Ø«Ø©:
- Ø§Ø¬Ø¹Ù„ Ø§Ù„Ø±Ø¯ÙˆØ¯ Ù‚ØµÙŠØ±Ø© ÙˆÙ…ÙÙŠØ¯Ø© (2-3 Ø¬Ù…Ù„ ÙƒØ­Ø¯ Ø£Ù‚ØµÙ‰)
- Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ù…Ø¨Ø³Ø·Ø©
- Ø£Ø¶Ù Ù„Ù…Ø³Ø© Ù…Ù† Ø§Ù„Ø¯Ø¹Ø§Ø¨Ø© ÙˆØ§Ù„Ù…Ø±Ø­ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
- Ø´Ø¬Ø¹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ§Ù„Ø§Ø³ØªÙƒØ´Ø§Ù
- ÙƒÙ† ØµØ¨ÙˆØ±Ø§Ù‹ ÙˆÙ…ØªÙÙ‡Ù…Ø§Ù‹ ÙˆÙ…Ø­Ø¨Ø§Ù‹"""

        # Add context-specific instructions
        if context:
            if context.get("time_of_day"):
                base_prompt += f"\n- ÙˆÙ‚Øª Ø§Ù„ØªÙØ§Ø¹Ù„: {context['time_of_day']}"
            if context.get("activity"):
                base_prompt += f"\n- Ø§Ù„Ù†Ø´Ø§Ø· Ø§Ù„Ø­Ø§Ù„ÙŠ: {context['activity']}"
            if context.get("mood"):
                base_prompt += f"\n- Ù…Ø²Ø§Ø¬ Ø§Ù„Ø·ÙÙ„: {context['mood']}"

        return base_prompt

    def _is_wake_word_only(self, message: str) -> bool:
        """Enhanced wake word detection"""
        wake_patterns = [
            "ÙŠØ§ Ø¯Ø¨Ø¯ÙˆØ¨",
            "Ø¯Ø¨Ø¯ÙˆØ¨",
            "hey teddy",
            "hello teddy",
            "Ù…Ø±Ø­Ø¨Ø§ Ø¯Ø¨Ø¯ÙˆØ¨",
            "Ø£Ù‡Ù„Ø§ Ø¯Ø¨Ø¯ÙˆØ¨",
            "Ø³Ù„Ø§Ù… Ø¯Ø¨Ø¯ÙˆØ¨",
        ]
        message_lower = message.lower().strip()

        # Check if message is primarily a wake word
        for pattern in wake_patterns:
            if pattern in message_lower and len(message_lower.split()) <= 4:
                return True
        return False

    async def _create_wake_word_response(
        self, child: Child, session_id: str
    ) -> AIResponseModel:
        """Enhanced wake word response with variety"""
        import random

        responses = [
            f"Ù†Ø¹Ù… {child.name}ØŸ Ø£Ù†Ø§ Ù‡Ù†Ø§! ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒØŸ ðŸ§¸âœ¨",
            f"Ù…Ø±Ø­Ø¨Ø§Ù‹ {child.name}! Ø£Ø³Ø¹Ø¯ Ø¨Ø³Ù…Ø§Ø¹ ØµÙˆØªÙƒ! Ø¨Ù…Ø§Ø°Ø§ ØªÙÙƒØ±ØŸ ðŸŒŸðŸ˜Š",
            f"Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹ {child.name}! Ø£Ù†Ø§ Ù…Ø³ØªØ¹Ø¯ Ù„Ù„Ø­Ø¯ÙŠØ«! Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ±ÙŠØ¯ Ø£Ù† Ù†ÙØ¹Ù„Ù‡ØŸ ðŸŽ‰ðŸ§¸",
        ]

        return AIResponseModel(
            text=random.choice(responses),
            emotion="happy",
            category="greeting",
            learning_points=["social_interaction", "communication"],
            session_id=session_id,
            confidence=1.0,
            processing_time_ms=8,
        )

    async def _extract_learning_points(self, message: str, response: str) -> List[str]:
        """Enhanced learning points extraction"""
        points = []
        combined_text = f"{message} {response}".lower()

        learning_patterns = {
            "emotional_intelligence": ["Ù…Ø´Ø§Ø¹Ø±", "Ø­Ø²ÙŠÙ†", "Ø³Ø¹ÙŠØ¯", "Ø®Ø§Ø¦Ù"],
            "language_development": ["ÙƒÙ„Ù…Ø©", "Ø¬Ù…Ù„Ø©", "Ù‚Ø±Ø§Ø¡Ø©", "ÙƒØªØ§Ø¨Ø©"],
            "mathematical_thinking": ["Ø±Ù‚Ù…", "Ø¹Ø¯Ø¯", "Ø­Ø³Ø§Ø¨", "Ø¬Ù…Ø¹"],
            "scientific_curiosity": ["Ù„Ù…Ø§Ø°Ø§", "ÙƒÙŠÙ", "ØªØ¬Ø±Ø¨Ø©", "Ø§ÙƒØªØ´Ø§Ù"],
            "social_skills": ["ØµØ¯ÙŠÙ‚", "Ø´ÙƒØ±Ø§Ù‹", "Ù…Ù† ÙØ¶Ù„Ùƒ", "Ø¢Ø³Ù"],
            "creative_expression": ["Ø±Ø³Ù…", "Ù‚ØµØ©", "Ø¥Ø¨Ø¯Ø§Ø¹", "Ø®ÙŠØ§Ù„"],
            "cultural_awareness": ["ØªÙ‚Ø§Ù„ÙŠØ¯", "Ø¹Ø§Ø¯Ø§Øª", "Ø«Ù‚Ø§ÙØ©"],
            "problem_solving": ["Ø­Ù„", "Ù…Ø´ÙƒÙ„Ø©", "ÙÙƒØ±", "Ø·Ø±ÙŠÙ‚Ø©"],
        }

        for skill, keywords in learning_patterns.items():
            if any(keyword in combined_text for keyword in keywords):
                points.append(skill)

        return points if points else ["general_communication"]

    def _update_conversation_history(
        self, device_id: str, message: str, response: str, emotion: str
    ):
        """Enhanced conversation history with emotion tracking"""
        if device_id not in self.conversation_history:
            self.conversation_history[device_id] = []

        history = self.conversation_history[device_id]

        # Add message with emotion context
        history.append(
            {
                "role": "user",
                "content": message,
                "emotion": emotion,
                "timestamp": datetime.utcnow().isoformat(),
            }
        )
        history.append(
            {
                "role": "assistant",
                "content": response,
                "timestamp": datetime.utcnow().isoformat(),
            }
        )

        # Keep only recent history
        if len(history) > self.max_history_length * 2:
            self.conversation_history[device_id] = history[
                -self.max_history_length * 2 :
            ]

    def _get_conversation_history(self, device_id: str) -> List[Dict]:
        """Get conversation history for device"""
        return self.conversation_history.get(device_id, [])

    async def get_performance_metrics(self) -> Dict[str, Any]:
        """Get comprehensive performance metrics"""
        avg_processing_time = (
            self.total_processing_time / self.request_count
            if self.request_count > 0
            else 0
        )

        return {
            "total_requests": self.request_count,
            "total_errors": self.error_count,
            "rate_limit_hits": self.rate_limit_count,
            "error_rate": (
                self.error_count / self.request_count if self.request_count > 0 else 0
            ),
            "average_processing_time_ms": avg_processing_time,
            "cache_size": len(self.memory_cache),
            "active_conversations": len(self.conversation_history),
        }
