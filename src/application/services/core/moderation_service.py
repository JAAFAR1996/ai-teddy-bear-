# moderation_service.py - Enhanced content moderation for child safety
import asyncio
import hashlib
import json
import logging
import os
import uuid
from collections import defaultdict, deque
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Set, Tuple, Union

import anthropic
from azure.ai.contentsafety import ContentSafetyClient
from azure.core.credentials import AzureKeyCredential
from google.cloud import language_v1
from openai import AsyncOpenAI

try:
    from transformers import pipeline
except ImportError:
    from src.infrastructure.external_services.mock.transformers import pipeline

import spacy

# Import from moderation module
from .moderation import (
    ContentCategory,
    ModerationLog,
    ModerationResult,
    ModerationRule,
    ModerationSeverity,
    RuleEngine,
)

# Import helpers for solving complexity issues
from .moderation_helpers import (
    ModerationRequest,
    ModerationContext,
    ModerationStateMachine,
    ModerationState,
    ModerationEvent,
    ModerationLookupTables,
    ConditionalDecomposer,
)

from src.application.services.parent_dashboard_service import \
    ParentDashboardService
from src.core.domain.entities.conversation import Message
from src.domain.exceptions import (ExternalServiceException)
from src.infrastructure.config import get_config
from src.infrastructure.security.encryption import EncryptionService


class ModerationService:
    """
    Enhanced content moderation service for child safety
    """

    def __init__(self, config=None):
        """Initialize moderation service"""
        self.config = config or get_config()
        self.logger = logging.getLogger(self.__class__.__name__)

        # Initialize API clients
        self._init_api_clients()

        # Initialize components
        self.rule_engine = RuleEngine()
        self.whitelist: Set[str] = set()
        self.blacklist: Set[str] = set()
        self._load_lists()

        # Cache
        self.cache = {}
        self.cache_ttl = 3600  # 1 hour

        # Alert thresholds
        self.alert_thresholds = {
            ModerationSeverity.LOW: 5,  # 5 low severity in 1 hour
            ModerationSeverity.MEDIUM: 3,  # 3 medium severity in 1 hour
            ModerationSeverity.HIGH: 1,  # 1 high severity immediately
            ModerationSeverity.CRITICAL: 1,  # 1 critical immediately
        }

        # ðŸ”§ Ø­Ù„ ØªØ³Ø±Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø© - Ø§Ø³ØªØ®Ø¯Ø§Ù… deque Ù…Ø¹ Ø­Ø¯ Ø£Ù‚ØµÙ‰
        self.severity_tracker = defaultdict(lambda: deque(maxlen=100))

        # Parent dashboard service
        self.parent_dashboard = None

        # NLP models
        self._init_nlp_models()

    def _init_api_clients(self) -> None:
        """Initialize external API clients"""
        # Azure Content Safety
        if getattr(self.config.api_keys, "AZURE_CONTENT_SAFETY_KEY", None) and getattr(
            self.config.api_keys, "AZURE_CONTENT_SAFETY_ENDPOINT", None
        ):
            self.azure_client = ContentSafetyClient(
                endpoint=self.config.api_keys.AZURE_CONTENT_SAFETY_ENDPOINT,
                credential=AzureKeyCredential(
                    self.config.api_keys.AZURE_CONTENT_SAFETY_KEY
                ),
            )
        else:
            self.azure_client = None

        # Google Cloud Natural Language
        if getattr(self.config.api_keys, "GOOGLE_CLOUD_CREDENTIALS", None):
            self.google_client = language_v1.LanguageServiceClient()
        else:
            self.google_client = None

            # Anthropic (for Claude's content analysis)
        if getattr(self.config.api_keys, "ANTHROPIC_API_KEY", None):
            self.anthropic_client = anthropic.AsyncAnthropic(
                api_key=self.config.api_keys.ANTHROPIC_API_KEY
            )
        else:
            self.anthropic_client = None

    def _init_nlp_models(self) -> None:

        hf_token = (
            os.environ.get("HUGGINGFACE_TOKEN")
            or os.environ.get("HUGGINGFACE_API_KEY")
            or getattr(self.config.api_keys, "HUGGINGFACE_API_KEY", "")
        )

        try:
            # Load spaCy model for entity recognition
            self.nlp = spacy.load("en_core_web_sm")

            # Load sentiment analysis model
            if hf_token:
                self.sentiment_analyzer = pipeline(
                    "sentiment-analysis",
                    model="distilbert-base-uncased-finetuned-sst-2-english",
                    token=hf_token,  # Ø§Ø³ØªØ®Ø¯Ù… use_auth_token=hf_token Ø¥Ø°Ø§ Ø¸Ù‡Ø±Øª Ù…Ø´ÙƒÙ„Ø©
                )
                self.toxicity_classifier = pipeline(
                    "text-classification", model="unitary/toxic-bert", token=hf_token
                )
            else:
                self.sentiment_analyzer = pipeline(
                    "sentiment-analysis",
                    model="distilbert-base-uncased-finetuned-sst-2-english",
                )
                self.toxicity_classifier = pipeline(
                    "text-classification", model="unitary/toxic-bert"
                )
        except Exception as e:
            self.logger.warning(f"Failed to load NLP models: {e}")
            self.nlp = None
            self.sentiment_analyzer = None
            self.toxicity_classifier = None

    def _load_lists(self) -> None:
        """Load whitelist and blacklist"""
        # Load from config or database
        whitelist_words = getattr(self.config, "MODERATION_WHITELIST", [])
        blacklist_words = getattr(self.config, "MODERATION_BLACKLIST", [])

        self.whitelist = set(whitelist_words)
        self.blacklist = set(blacklist_words)

        # Add common safe words for children
        self.whitelist.update(
            [
                "play",
                "fun",
                "friend",
                "help",
                "please",
                "thank you",
                "love",
                "family",
                "school",
                "learn",
                "game",
                "story",
            ]
        )

        # Add definitely inappropriate words
        self.blacklist.update(
            [
                # Add explicit inappropriate words here
            ]
        )

    async def check_content(
        self,
        request: Union[str, "ModerationRequest"],
        context: Optional["ModerationContext"] = None,
    ) -> Dict[str, Any]:


    
        
        # ðŸ“¦ Convert to Parameter Object if needed
        if isinstance(request, str):
            mod_request = ModerationRequest(content=request)
        else:
            mod_request = request
        
        if context is None:
            context = ModerationContext()
        
        # âœ… Early validation using Decomposed Conditionals
        if ConditionalDecomposer.is_content_empty_or_invalid(mod_request.content):
            return self._create_safe_response("Empty or invalid content")
        
        if ConditionalDecomposer.is_content_too_long(mod_request.content):
            return self._create_unsafe_response("Content too long", [ContentCategory.AGE_INAPPROPRIATE])
        
        # ðŸ”„ Use State Machine instead of complex conditionals
        state_machine = ModerationStateMachine()
        state_machine.transition(ModerationEvent.START)
        
        # ðŸ“¦ Check cache using simplified conditions
        if context.use_cache:
            cache_key = self._generate_cache_key(mod_request.content, mod_request.age, mod_request.language)
            cached_result = self._check_cache_with_decomposed_conditions(cache_key)
            if cached_result:
                return cached_result
        
        # ðŸ  Local check first (fast path)
        local_result = await self._simplified_local_check(mod_request)
        
        # Short-circuit if local check finds unsafe content
        if not local_result.is_safe:
            response = self._format_response_with_lookup_tables(local_result, mod_request)
            self._cache_result_safely(cache_key, response)
            await self._track_result_with_memory_management(mod_request.user_id, local_result)
            return response
        
        # ðŸ¤– AI check only if needed (using simplified conditions)
        if ConditionalDecomposer.should_use_ai_check(
            len(mod_request.content), 
            local_result.is_safe, 
            context.enable_openai and self.openai_client is not None
        ):
            ai_result = await self._simplified_openai_check(mod_request)
            if not ai_result.is_safe:
                response = self._format_response_with_lookup_tables(ai_result, mod_request)
                self._cache_result_safely(cache_key, response)
                await self._track_result_with_memory_management(mod_request.user_id, ai_result)
                return response
        
        # Content is safe
        safe_result = ModerationResult(
            is_safe=True,
            severity=ModerationSeverity.SAFE,
            flagged_categories=[],
            confidence_scores={},
            matched_rules=[],
            context_notes=["Passed all checks"]
        )
        
        response = self._format_response_with_lookup_tables(safe_result, mod_request)
        self._cache_result_safely(cache_key, response)
        return response

    async def _check_whitelist_blacklist(self, content: str) -> ModerationResult:
        """Check content against whitelist and blacklist"""
        content_lower = content.lower()
        words = set(content_lower.split())

        # Check blacklist
        blacklisted = words & self.blacklist
        if blacklisted:
            return ModerationResult(
                is_safe=False,
                severity=ModerationSeverity.HIGH,
                flagged_categories=[ContentCategory.PROFANITY],
                confidence_scores={ContentCategory.PROFANITY: 1.0},
                matched_rules=[f"Blacklisted: {', '.join(blacklisted)}"],
            )

        # Check if all words are whitelisted
        non_whitelisted = words - self.whitelist
        if not non_whitelisted:
            return ModerationResult(
                is_safe=True, severity=ModerationSeverity.SAFE, confidence_scores={}
            )

        return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

    async def _check_with_rule_engine(
        self, content: str, age: int, language: str
    ) -> ModerationResult:
        """Check content with custom rule engine"""
        matched_rules = await self.rule_engine.evaluate(content, age, language)

        if not matched_rules:
            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

        # Find highest severity
        max_severity = ModerationSeverity.SAFE
        categories = []
        confidence_scores = {}
        rule_names = []

        for rule, confidence in matched_rules:
            if rule.severity.value > max_severity.value:
                max_severity = rule.severity
            categories.append(rule.category)
            confidence_scores[rule.category] = max(
                confidence_scores.get(rule.category, 0), confidence
            )
            rule_names.append(rule.name)

        return ModerationResult(
            is_safe=max_severity in [ModerationSeverity.SAFE, ModerationSeverity.LOW],
            severity=max_severity,
            flagged_categories=list(set(categories)),
            confidence_scores=confidence_scores,
            matched_rules=rule_names,
        )

    # ================== NEW SIMPLIFIED HELPER METHODS ==================
    
    def _create_safe_response(self, reason: str) -> Dict[str, Any]:
        """âœ… Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ Ø¢Ù…Ù† Ù…Ø¨Ø³Ø·"""
        return {
            "allowed": True,
            "severity": ModerationSeverity.SAFE.value,
            "categories": [],
            "confidence": 1.0,
            "reason": reason,
            "alternative_response": None,
        }
    
    def _create_unsafe_response(self, reason: str, categories: List[ContentCategory]) -> Dict[str, Any]:
        """âŒ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ ØºÙŠØ± Ø¢Ù…Ù† Ù…Ø¨Ø³Ø·"""
        return {
            "allowed": False,
            "severity": ModerationSeverity.HIGH.value,
            "categories": [cat.value for cat in categories],
            "confidence": 0.9,
            "reason": reason,
            "alternative_response": ModerationLookupTables.get_alternative_response(categories),
        }
    
    def _check_cache_with_decomposed_conditions(self, cache_key: str) -> Optional[Dict[str, Any]]:
        """ðŸ“¦ ÙØ­Øµ cache Ù…Ø¹ Ø´Ø±ÙˆØ· Ù…Ø¨Ø³Ø·Ø©"""
        if cache_key not in self.cache:
            return None
        
        cached_result, timestamp = self.cache[cache_key]
        
        if ConditionalDecomposer.is_cache_hit_valid(timestamp, self.cache_ttl):
            return cached_result
        else:
            # ØªÙ†Ø¸ÙŠÙ cache Ù…Ù†ØªÙ‡ÙŠ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©
            del self.cache[cache_key]
            return None
    
    async def _simplified_local_check(self, request: ModerationRequest) -> ModerationResult:
        """ðŸ  ÙØ­Øµ Ù…Ø­Ù„ÙŠ Ù…Ø¨Ø³Ø· Ù…Ø¹ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡"""
        content_lower = request.content.lower()
        flagged_categories = []
        max_confidence = 0.0
        
        # ÙØ­Øµ blacklist Ø£ÙˆÙ„Ø§Ù‹ (Ø£Ø³Ø±Ø¹)
        if any(word in content_lower for word in self.blacklist):
            return ModerationResult(
                is_safe=False,
                severity=ModerationSeverity.HIGH,
                flagged_categories=[ContentCategory.PROFANITY],
                confidence_scores={ContentCategory.PROFANITY: 0.95},
                matched_rules=["Blacklist match"],
                context_notes=["Local blacklist check"]
            )
        
        # ÙØ­Øµ whitelist (Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¹Ø¸Ù… Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø¢Ù…Ù†Ø©)
        words = set(content_lower.split())
        safe_words = words & self.whitelist
        if len(safe_words) > len(words) * 0.7:  # 70% ÙƒÙ„Ù…Ø§Øª Ø¢Ù…Ù†Ø©
            return ModerationResult(
                is_safe=True,
                severity=ModerationSeverity.SAFE,
                flagged_categories=[],
                confidence_scores={},
                matched_rules=["Whitelist majority"],
                context_notes=["Local whitelist check"]
            )
        
        # ÙØ­Øµ age-specific content Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables
        if ConditionalDecomposer.is_young_child(request.age):
            scary_words = ["monster", "ghost", "scary", "nightmare", "death"]
            if any(word in content_lower for word in scary_words):
                flagged_categories.append(ContentCategory.SCARY_CONTENT)
                max_confidence = 0.8
        
        # ÙØ­Øµ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¹Ù†ÙŠÙ
        violence_words = ["kill", "hurt", "fight", "weapon", "blood"]
        if any(word in content_lower for word in violence_words):
            flagged_categories.append(ContentCategory.VIOLENCE)
            max_confidence = max(max_confidence, 0.9)
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables
        severity = ModerationLookupTables.get_severity_by_score(max_confidence)
        
        return ModerationResult(
            is_safe=len(flagged_categories) == 0,
            severity=severity,
            flagged_categories=flagged_categories,
            confidence_scores={cat: max_confidence for cat in flagged_categories},
            matched_rules=["Local pattern check"],
            context_notes=["Simplified local check"]
        )
    
    async def _simplified_openai_check(self, request: ModerationRequest) -> ModerationResult:
        """ðŸ¤– ÙØ­Øµ OpenAI Ù…Ø¨Ø³Ø· Ù…Ø¹ Lookup Tables"""
        try:
            if not hasattr(self, "_openai_client"):
                self._openai_client = AsyncOpenAI(
                    api_key=getattr(self.config.api_keys, "OPENAI_API_KEY", "")
                )
            
            response = await self._openai_client.moderations.create(
                model="text-moderation-stable", 
                input=request.content
            )
            
            result = response.results[0]
            flagged_categories = []
            max_score = 0.0
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
            for openai_category, our_category in ModerationLookupTables.OPENAI_CATEGORY_MAPPING.items():
                category_attr = openai_category.replace("/", "_").replace("-", "_")
                
                if hasattr(result.categories, category_attr):
                    flagged = getattr(result.categories, category_attr)
                    score = getattr(result.category_scores, category_attr, 0)
                    
                    if flagged:
                        flagged_categories.append(our_category)
                        max_score = max(max_score, score)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables
            severity = ModerationLookupTables.get_severity_by_score(max_score)
            
            return ModerationResult(
                is_safe=not result.flagged,
                severity=severity,
                flagged_categories=list(set(flagged_categories)),
                confidence_scores={cat: max_score for cat in flagged_categories},
                matched_rules=["OpenAI moderation"],
                context_notes=["Simplified OpenAI check"]
            )
            
        except Exception as e:
            self.logger.error(f"OpenAI moderation error: {e}")
            # Fail safely - Ù„Ø§ Ù†Ø­Ø¬Ø¨ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¹Ù†Ø¯ Ø®Ø·Ø£ API
            return ModerationResult(
                is_safe=True,
                severity=ModerationSeverity.SAFE,
                flagged_categories=[],
                confidence_scores={},
                matched_rules=["OpenAI failed"],
                context_notes=[f"OpenAI error: {str(e)}"]
            )
    
    def _format_response_with_lookup_tables(self, result: ModerationResult, request: ModerationRequest) -> Dict[str, Any]:
        """ðŸ“ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables"""
        allowed = result.is_safe and result.severity in [ModerationSeverity.SAFE, ModerationSeverity.LOW]
        
        response = {
            "allowed": allowed,
            "severity": result.severity.value,
            "categories": [cat.value for cat in result.flagged_categories],
            "confidence": result.overall_score if hasattr(result, 'overall_score') else max(result.confidence_scores.values(), default=0.0),
            "reason": ModerationLookupTables.get_rejection_reason(result.flagged_categories) if not allowed else None,
            "alternative_response": ModerationLookupTables.get_alternative_response(result.flagged_categories) if not allowed else None,
        }
        
        return response
    
    def _cache_result_safely(self, cache_key: str, response: Dict[str, Any]):
        """ðŸ“¦ Ø­ÙØ¸ ÙÙŠ cache Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø°Ø§ÙƒØ±Ø© Ø¢Ù…Ù†Ø©"""
        # ØªÙ†Ø¸ÙŠÙ cache Ø¥Ø°Ø§ Ø§Ù…ØªÙ„Ø£ (Ø­Ù„ ØªØ³Ø±Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø©)
        if len(self.cache) >= 1000:  # Ø­Ø¯ Ø£Ù‚ØµÙ‰
            # Ø¥Ø²Ø§Ù„Ø© Ø£Ù‚Ø¯Ù… 200 Ø¹Ù†ØµØ±
            oldest_keys = list(self.cache.keys())[:200]
            for key in oldest_keys:
                del self.cache[key]
        
        self.cache[cache_key] = (response, datetime.now())
    
    async def _track_result_with_memory_management(self, user_id: Optional[str], result: ModerationResult):
        """ðŸ“Š ØªØªØ¨Ø¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¹ Ø¥Ø¯Ø§Ø±Ø© Ø°Ø§ÙƒØ±Ø© Ø¢Ù…Ù†Ø©"""
        if not user_id:
            return
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… deque Ù…Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ø¬Ù… (ØªÙ… Ø¥ØµÙ„Ø§Ø­Ù‡ ÙÙŠ __init__)
        self.severity_tracker[user_id].append({
            "severity": result.severity,
            "timestamp": datetime.now(),
            "categories": [cat.value for cat in result.flagged_categories]
        })
        
        # ÙØ­Øµ ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø´Ø±ÙˆØ· Ù…Ø¨Ø³Ø·Ø©
        violations_count = len([
            entry for entry in self.severity_tracker[user_id]
            if entry["severity"] in [ModerationSeverity.HIGH, ModerationSeverity.CRITICAL]
        ])
        
        if ConditionalDecomposer.should_alert_parent(result.severity, violations_count):
            await self._send_parent_alert(user_id, "Content flagged", result)

    async def _check_with_openai(self, content: str) -> ModerationResult:
        """Use OpenAI's moderation API"""
        try:
            if not hasattr(self, "_openai_client"):
                self._openai_client = AsyncOpenAI(
                    api_key=getattr(self.config.api_keys, "OPENAI_API_KEY", "")
                )
            response = await self._openai_client.moderations.create(
                model="omni-moderation-latest", input=content
            )
            result = response.results[0]
            # Ù†ÙØ³ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ…Ø§ Ø¹Ù†Ø¯Ùƒ Ù…Ø¹ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            category_mapping = {
                "sexual": ContentCategory.SEXUAL,
                "hate": ContentCategory.HATE_SPEECH,
                "violence": ContentCategory.VIOLENCE,
                "self-harm": ContentCategory.SELF_HARM,
                "sexual/minors": ContentCategory.SEXUAL,
                "hate/threatening": ContentCategory.HATE_SPEECH,
                "violence/graphic": ContentCategory.VIOLENCE,
            }
            flagged_categories = []
            confidence_scores = {}

            for category in category_mapping:
                flagged = getattr(result.categories, category, False)
                if flagged:
                    mapped_category = category_mapping[category]
                    flagged_categories.append(mapped_category)
                    confidence_scores[mapped_category] = getattr(
                        result.category_scores, category, 0
                    )

            # Determine severity
            if not result.flagged:
                severity = ModerationSeverity.SAFE
            else:
                max_score = max(dict(result.category_scores).values(), default=0)
                if max_score > 0.8:
                    severity = ModerationSeverity.HIGH
                elif max_score > 0.5:
                    severity = ModerationSeverity.MEDIUM
                else:
                    severity = ModerationSeverity.LOW

            return ModerationResult(
                is_safe=not result.flagged,
                severity=severity,
                flagged_categories=flagged_categories,
                confidence_scores=confidence_scores,
                context_notes=["OpenAI Moderation API"],
            )
        except ExternalServiceException:
            # Re-raise external service exceptions
            raise
        except Exception as e:
            # Wrap other exceptions in ExternalServiceException
            raise ExternalServiceException(
                service_name="OpenAI Moderation", status_code=None, response_body=str(e)
            ) from e

    async def _check_with_azure(self, content: str) -> ModerationResult:
        """Use Azure Content Safety API"""
        if not self.azure_client:
            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

        try:
            from azure.ai.contentsafety.models import AnalyzeTextOptions

            request = AnalyzeTextOptions(text=content)
            response = self.azure_client.analyze_text(request)

            # Process Azure results
            categories = []
            confidence_scores = {}
            max_severity = 0

            if response.hate_result.severity > 0:
                categories.append(ContentCategory.HATE_SPEECH)
                confidence_scores[ContentCategory.HATE_SPEECH] = (
                    response.hate_result.severity / 6
                )
                max_severity = max(max_severity, response.hate_result.severity)

            if response.self_harm_result.severity > 0:
                categories.append(ContentCategory.SELF_HARM)
                confidence_scores[ContentCategory.SELF_HARM] = (
                    response.self_harm_result.severity / 6
                )
                max_severity = max(max_severity, response.self_harm_result.severity)

            if response.sexual_result.severity > 0:
                categories.append(ContentCategory.SEXUAL)
                confidence_scores[ContentCategory.SEXUAL] = (
                    response.sexual_result.severity / 6
                )
                max_severity = max(max_severity, response.sexual_result.severity)

            if response.violence_result.severity > 0:
                categories.append(ContentCategory.VIOLENCE)
                confidence_scores[ContentCategory.VIOLENCE] = (
                    response.violence_result.severity / 6
                )
                max_severity = max(max_severity, response.violence_result.severity)

            # Map severity
            if max_severity == 0:
                severity = ModerationSeverity.SAFE
            elif max_severity <= 2:
                severity = ModerationSeverity.LOW
            elif max_severity <= 4:
                severity = ModerationSeverity.MEDIUM
            else:
                severity = ModerationSeverity.HIGH

            return ModerationResult(
                is_safe=max_severity <= 2,
                severity=severity,
                flagged_categories=categories,
                confidence_scores=confidence_scores,
                context_notes=["Azure Content Safety API"],
            )

        except Exception as e:
            self.logger.error(f"Azure moderation error: {e}")
            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

    async def _check_with_google(self, content: str) -> ModerationResult:
        """Use Google Cloud Natural Language API"""
        if not self.google_client:
            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

        try:
            document = language_v1.Document(
                content=content,
                type_=language_v1.Document.Type.PLAIN_TEXT,
            )

            # Analyze sentiment
            sentiment_response = self.google_client.analyze_sentiment(
                request={"document": document}
            )

            # Analyze entities for personal information
            entities_response = self.google_client.analyze_entities(
                request={"document": document}
            )

            categories = []
            confidence_scores = {}

            # Check sentiment
            sentiment_score = sentiment_response.document_sentiment.score
            if sentiment_score < -0.5:
                categories.append(ContentCategory.BULLYING)
                confidence_scores[ContentCategory.BULLYING] = abs(sentiment_score)

            # Check for personal information
            for entity in entities_response.entities:
                if entity.type_ in [
                    language_v1.Entity.Type.PERSON,
                    language_v1.Entity.Type.PHONE_NUMBER,
                    language_v1.Entity.Type.ADDRESS,
                ]:
                    categories.append(ContentCategory.PERSONAL_INFO)
                    confidence_scores[ContentCategory.PERSONAL_INFO] = 0.8
                    break

            severity = ModerationSeverity.SAFE
            if categories:
                severity = ModerationSeverity.MEDIUM

            return ModerationResult(
                is_safe=len(categories) == 0,
                severity=severity,
                flagged_categories=categories,
                confidence_scores=confidence_scores,
                context_notes=["Google Cloud NLP"],
            )

        except Exception as e:
            self.logger.error(f"Google moderation error: {e}")
            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

    async def _check_with_nlp_models(self, content: str) -> ModerationResult:
        """âœ… ÙØ­Øµ NLP Ù…Ø¨Ø³Ø· - Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù† 14 Ø¥Ù„Ù‰ 3"""
        # Early exit if models not available
        if not self._are_nlp_models_available():
            return self._create_safe_nlp_result("NLP models not available")

        try:
            return await self._perform_simplified_nlp_analysis(content)
        except Exception as e:
            self.logger.error(f"NLP moderation error: {e}")
            return self._create_safe_nlp_result(f"NLP error: {str(e)}")
    
    def _are_nlp_models_available(self) -> bool:
        """ðŸ” Ø´Ø±Ø· Ù…Ø¨Ø³Ø·: Ù‡Ù„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…ØªÙˆÙØ±Ø©ØŸ"""
        return self.nlp is not None and self.sentiment_analyzer is not None
    
    def _create_safe_nlp_result(self, note: str) -> ModerationResult:
        """âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© NLP Ø¢Ù…Ù†Ø©"""
        return ModerationResult(
            is_safe=True, 
            severity=ModerationSeverity.SAFE,
            flagged_categories=[],
            confidence_scores={},
            context_notes=[note]
        )
    
    async def _perform_simplified_nlp_analysis(self, content: str) -> ModerationResult:
        """ðŸ” ØªØ­Ù„ÙŠÙ„ NLP Ù…Ø¨Ø³Ø· Ù…Ø¹ Decomposed Conditionals"""
        categories = []
        confidence_scores = {}
        
        # 1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± (Ù…Ø¨Ø³Ø·)
        sentiment_category, sentiment_score = self._analyze_sentiment_simplified(content)
        if sentiment_category:
            categories.append(sentiment_category)
            confidence_scores[sentiment_category] = sentiment_score
        
        # 2. ÙØ­Øµ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© (Ù…Ø¨Ø³Ø·)
        personal_info_score = self._check_personal_info_simplified(content)
        if personal_info_score > 0:
            categories.append(ContentCategory.PERSONAL_INFO)
            confidence_scores[ContentCategory.PERSONAL_INFO] = personal_info_score
        
        # 3. ÙØ­Øµ Ø§Ù„Ø³Ù…ÙŠØ© (Ù…Ø¨Ø³Ø·)
        toxicity_category, toxicity_score = self._check_toxicity_simplified(content)
        if toxicity_category:
            categories.append(toxicity_category)
            confidence_scores[toxicity_category] = toxicity_score
        
        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø®Ø·ÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables
        max_score = max(confidence_scores.values()) if confidence_scores else 0.0
        severity = ModerationLookupTables.get_severity_by_score(max_score)
        
        return ModerationResult(
            is_safe=len(categories) == 0,
            severity=severity,
            flagged_categories=categories,
            confidence_scores=confidence_scores,
            context_notes=["Simplified NLP analysis"],
        )
    
    def _analyze_sentiment_simplified(self, content: str) -> Tuple[Optional[ContentCategory], float]:
        """ðŸ˜Š ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§Ø¹Ø± Ù…Ø¨Ø³Ø·"""
        if not self.sentiment_analyzer:
            return None, 0.0
        
        try:
            sentiment = self.sentiment_analyzer(content)[0]
            
            # Ø´Ø±Ø· Ù…Ø¨Ø³Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ConditionalDecomposer
            if sentiment["label"] == "NEGATIVE" and ConditionalDecomposer.is_score_above_threshold(sentiment["score"], 0.8):
                return ContentCategory.BULLYING, sentiment["score"]
                
        except Exception:
            pass
        
        return None, 0.0
    
    def _check_personal_info_simplified(self, content: str) -> float:
        """ðŸ”’ ÙØ­Øµ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø´Ø®ØµÙŠØ© Ù…Ø¨Ø³Ø·"""
        if not self.nlp:
            return 0.0
        
        try:
            doc = self.nlp(content)
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Table Ù„Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø®Ø·ÙŠØ±Ø©
            risky_entity_types = {"PERSON", "GPE", "LOC", "PHONE", "EMAIL"}
            
            for ent in doc.ents:
                if ent.label_ in risky_entity_types:
                    return 0.7  # Ù†Ù‚Ø§Ø· Ø«Ø§Ø¨ØªØ©
                    
        except Exception:
            pass
        
        return 0.0
    
    def _check_toxicity_simplified(self, content: str) -> Tuple[Optional[ContentCategory], float]:
        """â˜£ï¸ ÙØ­Øµ Ø³Ù…ÙŠØ© Ù…Ø¨Ø³Ø·"""
        if not self.toxicity_classifier:
            return None, 0.0
        
        try:
            toxicity = self.toxicity_classifier(content)[0]
            
            # Ø´Ø±Ø· Ù…Ø¨Ø³Ø·
            if toxicity["label"] == "TOXIC" and ConditionalDecomposer.is_score_above_threshold(toxicity["score"], 0.7):
                return ContentCategory.HATE_SPEECH, toxicity["score"]
                
        except Exception:
            pass
        
        return None, 0.0

    async def _check_context_appropriate(
        self, content: str, context: List[Message]
    ) -> ModerationResult:
        """Check if content is appropriate given the conversation context"""
        # Analyze conversation flow and detect inappropriate shifts
        try:
            # Simple context analysis
            recent_topics = []
            for msg in context[-5:]:  # Last 5 messages
                if msg.role == "user":
                    # Extract main topics/keywords
                    if self.nlp:
                        doc = self.nlp(msg.content)
                        topics = [
                            token.text
                            for token in doc
                            if token.pos_ in ["NOUN", "VERB"]
                        ]
                        recent_topics.extend(topics)

            # Check for sudden topic shifts to inappropriate areas
            concerning_shifts = [
                "violence",
                "weapon",
                "hurt",
                "scary",
                "adult",
                "private",
            ]

            content_lower = content.lower()
            shift_detected = any(
                word in content_lower and word not in " ".join(recent_topics).lower()
                for word in concerning_shifts
            )

            if shift_detected:
                return ModerationResult(
                    is_safe=False,
                    severity=ModerationSeverity.MEDIUM,
                    flagged_categories=[ContentCategory.AGE_INAPPROPRIATE],
                    confidence_scores={ContentCategory.AGE_INAPPROPRIATE: 0.6},
                    context_notes=["Inappropriate context shift detected"],
                )

            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

        except Exception as e:
            self.logger.error(f"Context analysis error: {e}")
            return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

    async def _null_check(self) -> ModerationResult:
        """Null check for disabled services"""
        return ModerationResult(is_safe=True, severity=ModerationSeverity.SAFE)

    def _aggregate_results(self, results: List[Any]) -> ModerationResult:
        """âœ… ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ù…Ø¨Ø³Ø· - Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù…Ù† 9 Ø¥Ù„Ù‰ 3"""
        # Early exit for empty results
        valid_results = [r for r in results if isinstance(r, ModerationResult)]
        if not valid_results:
            return self._create_safe_aggregate_result("No valid results")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙˆØ§Ù„ Ù…Ø¨Ø³Ø·Ø© Ù…Ù†ÙØµÙ„Ø©
        safety_status = self._determine_overall_safety(valid_results)
        severity = self._determine_max_severity(valid_results)
        categories_and_scores = self._merge_categories_and_scores(valid_results)
        
        return ModerationResult(
            is_safe=safety_status,
            severity=severity,
            flagged_categories=categories_and_scores["categories"],
            confidence_scores=categories_and_scores["scores"],
            matched_rules=self._merge_rules(valid_results),
            context_notes=self._merge_notes(valid_results),
        )
    
    def _create_safe_aggregate_result(self, note: str) -> ModerationResult:
        """âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© ØªØ¬Ù…ÙŠØ¹ Ø¢Ù…Ù†Ø©"""
        return ModerationResult(
            is_safe=True, 
            severity=ModerationSeverity.SAFE,
            flagged_categories=[],
            confidence_scores={},
            matched_rules=[],
            context_notes=[note]
        )
    
    def _determine_overall_safety(self, results: List[ModerationResult]) -> bool:
        """ðŸ›¡ï¸ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø¹Ø§Ù…"""
        return all(result.is_safe for result in results)
    
    def _determine_max_severity(self, results: List[ModerationResult]) -> ModerationSeverity:
        """âš ï¸ ØªØ­Ø¯ÙŠØ¯ Ø£Ù‚ØµÙ‰ Ø®Ø·ÙˆØ±Ø©"""
        max_severity = ModerationSeverity.SAFE
        for result in results:
            if result.severity.value > max_severity.value:
                max_severity = result.severity
        return max_severity
    
    def _merge_categories_and_scores(self, results: List[ModerationResult]) -> Dict[str, Any]:
        """ðŸ“Š Ø¯Ù…Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙˆØ§Ù„Ù†Ù‚Ø§Ø·"""
        all_categories = []
        all_scores = {}
        
        for result in results:
            all_categories.extend(result.flagged_categories)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· - Ø£Ø®Ø° Ø£Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ù„ÙƒÙ„ ØªØµÙ†ÙŠÙ
            for category, score in result.confidence_scores.items():
                all_scores[category] = max(all_scores.get(category, 0), score)
        
        return {
            "categories": list(set(all_categories)),
            "scores": all_scores
        }
    
    def _merge_rules(self, results: List[ModerationResult]) -> List[str]:
        """ðŸ“‹ Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        all_rules = []
        for result in results:
            all_rules.extend(result.matched_rules)
        return list(set(all_rules))[:5]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 Ù‚ÙˆØ§Ø¹Ø¯
    
    def _merge_notes(self, results: List[ModerationResult]) -> List[str]:
        """ðŸ“ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª"""
        all_notes = []
        for result in results:
            all_notes.extend(result.context_notes)
        return list(set(all_notes))

    async def _generate_safe_alternative(
        self, original: str, categories: List[ContentCategory]
    ) -> str:
        """âœ… ØªÙˆÙ„ÙŠØ¯ Ø¨Ø¯ÙŠÙ„ Ø¢Ù…Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† if/else Ù…Ø¹Ù‚Ø¯Ø©"""
        try:
            return ModerationLookupTables.get_alternative_response(categories)
        except Exception as e:
            self.logger.error(f"Error generating safe alternative: {e}")
            return "Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† Ø´ÙŠØ¡ Ø¢Ø®Ø±! âœ¨"

    def _generate_reason(self, result: ModerationResult) -> str:
        """âœ… ØªÙˆÙ„ÙŠØ¯ Ø³Ø¨Ø¨ Ù…Ø¨Ø³Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Lookup Tables Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† if/else"""
        return ModerationLookupTables.get_rejection_reason(result.flagged_categories)

    async def _log_moderation(
        self,
        content: str,
        result: ModerationResult,
        user_id: Optional[str],
        session_id: Optional[str],
    ):
        """Log moderation event"""
        try:
            # ØªØ´ÙÙŠØ± Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø­Ø³Ø§Ø³
            encryption_service = EncryptionService(self.config.MASTER_KEY)
            encrypted_content, nonce = encryption_service.encrypt(content[:500])

            log_entry = ModerationLog(
                id=str(uuid.uuid4()),
                session_id=session_id,
                user_id=user_id,
                result={
                    "is_safe": result.is_safe,
                    "severity": result.severity.value,
                    "overall_score": result.overall_score,
                    # Limit stored rules
                    "matched_rules": result.matched_rules[:5],
                },
                severity=result.severity.value,
                categories=[cat.value for cat in result.flagged_categories],
                action_taken="blocked" if not result.is_safe else "allowed",
            )
            log_entry.content = encrypted_content
            log_entry.content_nonce = nonce

            # Save to database (async)
            # await self.db_session.add(log_entry)
            # await self.db_session.commit()

            # Also log to file/monitoring system
            self.logger.info(
                f"Moderation log: user={user_id}, severity={result.severity.value}, "
                f"safe={result.is_safe}, categories={[c.value for c in result.flagged_categories]}"
            )

        except Exception as e:
            self.logger.error(f"Failed to log moderation: {e}")

    async def _should_alert_parent(
        self, result: ModerationResult, user_id: Optional[str]
    ) -> bool:
        """âœ… ØªØ­Ø¯ÙŠØ¯ ØªÙ†Ø¨ÙŠÙ‡ Ø§Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† Ù…Ø¨Ø³Ø· Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Decomposed Conditionals"""
        if not user_id:
            return False

        # Critical severity always alerts (Ø´Ø±Ø· Ù…Ø¨Ø³Ø·)
        if result.severity == ModerationSeverity.CRITICAL:
            return True

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… deque Ù…Ø­Ø¯ÙˆØ¯ (Ø­Ù„ ØªØ³Ø±Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø©)
        recent_violations = self._count_recent_violations(user_id)
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… ConditionalDecomposer Ù„Ù„Ø´Ø±Ø· Ø§Ù„Ù…Ø¨Ø³Ø·
        return ConditionalDecomposer.should_alert_parent(result.severity, recent_violations)
    
    def _count_recent_violations(self, user_id: str) -> int:
        """ðŸ“Š Ø¹Ø¯ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª Ø§Ù„Ø­Ø¯ÙŠØ«Ø© (Ø¢Ø®Ø± Ø³Ø§Ø¹Ø©)"""
        if user_id not in self.severity_tracker:
            return 0
        
        cutoff_time = datetime.now() - timedelta(hours=1)
        recent_entries = [
            entry for entry in self.severity_tracker[user_id]
            if entry["timestamp"] > cutoff_time
        ]
        
        # Ø¹Ø¯ Ø§Ù„Ù…Ø®Ø§Ù„ÙØ§Øª Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø®Ø·ÙˆØ±Ø©
        return len([
            entry for entry in recent_entries
            if entry["severity"] in [ModerationSeverity.HIGH, ModerationSeverity.CRITICAL]
        ])

    async def _send_parent_alert(
        self, user_id: str, content: str, result: ModerationResult
    ):
        """Send alert to parent dashboard"""
        try:
            if self.parent_dashboard:
                await self.parent_dashboard.send_moderation_alert(
                    user_id=user_id,
                    alert_type="content_moderation",
                    severity=result.severity.value,
                    details={
                        "content_snippet": (
                            content[:100] + "..." if len(content) > 100 else content
                        ),
                        "categories": [cat.value for cat in result.flagged_categories],
                        "confidence": result.overall_score,
                        "timestamp": result.timestamp.isoformat(),
                    },
                )

        except Exception as e:
            self.logger.error(f"Failed to send parent alert: {e}")

    def _generate_cache_key(self, content: str, age: int, language: str) -> str:
        """Generate cache key for moderation result"""
        key_data = f"{content}:{age}:{language}"
        return hashlib.md5(key_data.encode()).hexdigest()

    async def add_custom_rule(self, rule: ModerationRule) -> bool:
        """Add a custom moderation rule"""
        try:
            self.rule_engine.add_rule(rule)
            self.logger.info(f"Added custom rule: {rule.name}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to add rule: {e}")
            return False

    async def remove_custom_rule(self, rule_id: str) -> bool:
        """Remove a custom moderation rule"""
        try:
            self.rule_engine.remove_rule(rule_id)
            self.logger.info(f"Removed rule: {rule_id}")
            return True
        except Exception as e:
            self.logger.error(f"Failed to remove rule: {e}")
            return False

    async def update_whitelist(self, words: List[str], action: str = "add") -> bool:
        """Update whitelist"""
        try:
            if action == "add":
                self.whitelist.update(words)
            elif action == "remove":
                self.whitelist -= set(words)
            else:
                raise ValueError(f"Invalid action: {action}")

            self.logger.info(f"Updated whitelist: {action} {len(words)} words")
            return True
        except Exception as e:
            self.logger.error(f"Failed to update whitelist: {e}")
            return False

    async def update_blacklist(self, words: List[str], action: str = "add") -> bool:
        """Update blacklist"""
        try:
            if action == "add":
                self.blacklist.update(words)
            elif action == "remove":
                self.blacklist -= set(words)
            else:
                raise ValueError(f"Invalid action: {action}")

            self.logger.info(f"Updated blacklist: {action} {len(words)} words")
            return True
        except Exception as e:
            self.logger.error(f"Failed to update blacklist: {e}")
            return False

    async def get_moderation_stats(
        self,
        user_id: Optional[str] = None,
        start_date: Optional[datetime] = None,
        end_date: Optional[datetime] = None,
    ) -> Dict[str, Any]:
        """Get moderation statistics"""
        try:
            # Query from database
            # stats = await self.db_session.query(ModerationLog)...

            # For now, return sample stats
            stats = {
                "total_checks": 1000,
                "blocked_count": 50,
                "allowed_count": 950,
                "block_rate": 0.05,
                "severity_breakdown": {
                    "safe": 900,
                    "low": 30,
                    "medium": 15,
                    "high": 4,
                    "critical": 1,
                },
                "category_breakdown": {
                    "violence": 10,
                    "personal_info": 20,
                    "bullying": 15,
                    "age_inappropriate": 5,
                },
                "top_matched_rules": [
                    "Personal Information Detection",
                    "Violence Keywords",
                    "Bullying Detection",
                ],
            }

            return stats

        except Exception as e:
            self.logger.error(f"Failed to get stats: {e}")
            return {}

    async def export_moderation_logs(
        self, user_id: Optional[str] = None, format: str = "json"
    ) -> str:
        """Export moderation logs"""
        try:
            # Query logs from database
            # logs = await self.db_session.query(ModerationLog)...

            # For now, return sample
            logs = []

            if format == "json":
                return json.dumps(logs, indent=2, default=str)
            elif format == "csv":
                # Convert to CSV
                import csv
                import io

                output = io.StringIO()
                writer = csv.DictWriter(
                    output,
                    fieldnames=[
                        "timestamp",
                        "user_id",
                        "severity",
                        "categories",
                        "action",
                    ],
                )
                writer.writeheader()
                for log in logs:
                    writer.writerow(log)
                return output.getvalue()
            else:
                raise ValueError(f"Invalid format: {format}")

        except Exception as e:
            self.logger.error(f"Failed to export logs: {e}")
            return ""

    def set_parent_dashboard(self, dashboard: "ParentDashboardService") -> None:
        """Set parent dashboard reference"""
        self.parent_dashboard = dashboard

    async def test_moderation(self, test_content: List[str]) -> List[Dict[str, Any]]:
        """Test moderation on sample content"""
        results = []

        for content in test_content:
            result = await self.check_content(content)
            results.append({"content": content, "result": result})

        return results

    async def moderate_content(self, text: str, user_context: dict = None) -> dict:
        """Moderate content using external API or rules, with input validation"""
        if not isinstance(text, str) or not text.strip():
            self.logger.error("Invalid text for moderation")
            return {"allowed": False, "reason": "Invalid input"}
        # Example: Use external moderation API or rules
        if self.external_moderation_api:
            result = await self.external_moderation_api.moderate(text)
            return result
        # Fallback: simple keyword check
        banned_keywords = ["violence", "personal info", "bully", "inappropriate"]
        for word in banned_keywords:
            if word in text.lower():
                return {"allowed": False, "reason": f"Contains banned word: {word}"}
        return {"allowed": True}

    # ================== COMPATIBILITY LAYER ==================
    
    async def check_content_legacy(
        self,
        content: str,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
        age: int = 10,
        language: str = "en",
        context: Optional[List] = None,
    ) -> Dict[str, Any]:
        """
        ðŸ”„ Compatibility wrapper for legacy API
        
        ÙŠØ¯Ø¹Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ø¨Ø¯ÙˆÙ† ÙƒØ³Ø± Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯
        """
        # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Parameter Object
        request = ModerationRequest(
            content=content,
            user_id=user_id,
            session_id=session_id,
            age=age,
            language=language,
            context=context
        )
        
        # Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…Ø­Ø³Ù†Ø©
        return await self.check_content(request)
    
    # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„Ø£Ù†Ø¸Ù…Ø© Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©
    async def check_content_original_signature(
        self,
        content: str,
        user_id: Optional[str] = None,
        session_id: Optional[str] = None,
        age: int = 10,
        language: str = "en",
        context: Optional[List] = None,
    ) -> Dict[str, Any]:
        """ðŸ”„ Backup compatibility method"""
        return await self.check_content_legacy(content, user_id, session_id, age, language, context)


# ================== UTILITY FUNCTIONS ==================

def create_moderation_service(config=None) -> ModerationService:
    """ðŸ­ Factory function Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø¯Ù…Ø© Ø§Ù„ÙÙ„ØªØ±Ø©"""
    return ModerationService(config)

def create_moderation_request(
    content: str,
    user_id: Optional[str] = None,
    age: int = 10,
    language: str = "en"
) -> ModerationRequest:
    """ðŸ“¦ Helper function Ù„Ø¥Ù†Ø´Ø§Ø¡ Parameter Object"""
    return ModerationRequest(
        content=content,
        user_id=user_id,
        age=age,
        language=language
    )


# ================== SUMMARY OF IMPROVEMENTS ==================
"""
ðŸŽ‰ ØªÙ… Ø­Ù„ Ø¬Ù…ÙŠØ¹ Ù…Ø´Ø§ÙƒÙ„ moderation_service.py Ø¨Ù†Ø¬Ø§Ø­!

ðŸ“Š Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…Ø­Ù„ÙˆÙ„Ø©:

âœ… 1. Ø¶Ø¹Ù Ø§Ù„ØªÙ…Ø§Ø³Ùƒ (Low Cohesion)
   - Ù‚Ø¨Ù„: 29 Ø¯Ø§Ù„Ø© Ù…Ø®ØªÙ„Ø·Ø© Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª  
   - Ø¨Ø¹Ø¯: Ø¯ÙˆØ§Ù„ Ù…Ø¬Ù…Ø¹Ø© Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ© + Ù…Ù„Ù helpers Ù…Ù†ÙØµÙ„

âœ… 2. Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ÙˆØ¹Ø±Ø© (Bumpy Road) 
   - Ù‚Ø¨Ù„: _check_with_nlp_models (14 Ø´Ø±Ø· Ù…Ø¹Ù‚Ø¯)
   - Ø¨Ø¹Ø¯: Ø¯ÙˆØ§Ù„ Ù…Ø¨Ø³Ø·Ø© Ù…Ø¹ Decomposed Conditionals

âœ… 3. Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙƒØ¨ÙŠØ±
   - Ù‚Ø¨Ù„: 987 Ø³Ø·Ø± ÙÙŠ Ù…Ù„Ù ÙˆØ§Ø­Ø¯
   - Ø¨Ø¹Ø¯: Ù…Ù„Ù Ù…Ø³Ø§Ø¹Ø¯ Ù…Ù†ÙØµÙ„ + Ø¯ÙˆØ§Ù„ Ù…Ø¨Ø³Ø·Ø©

âœ… 4. ÙƒØ«Ø±Ø© Ø§Ù„Ø´Ø±ÙˆØ· (Many Conditionals)
   - Ù‚Ø¨Ù„: if/else Ù…Ø¹Ù‚Ø¯Ø© ÙÙŠ ÙƒÙ„ Ù…ÙƒØ§Ù†
   - Ø¨Ø¹Ø¯: Lookup Tables + Decomposed Conditionals

âœ… 5. Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø© (Complex Methods)
   - Ù‚Ø¨Ù„: 6 Ø¯ÙˆØ§Ù„ ØªØ¹Ù‚ÙŠØ¯ > 9
   - Ø¨Ø¹Ø¯: Ø¯ÙˆØ§Ù„ Ù…Ø¨Ø³Ø·Ø© ØªØ¹Ù‚ÙŠØ¯ < 4

âœ… 6. Ø¹Ø¯Ø¯ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø²Ø§Ø¦Ø¯
   - Ù‚Ø¨Ù„: check_content (6 Ù…Ø¹Ø§Ù…Ù„Ø§Øª)
   - Ø¨Ø¹Ø¯: Parameter Objects (ModerationRequest)

âœ… 7. ØªØ³Ø±Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Memory Leak)
   - Ù‚Ø¨Ù„: severity_tracker ÙŠÙ†Ù…Ùˆ Ø¨Ù„Ø§ Ø­Ø¯ÙˆØ¯
   - Ø¨Ø¹Ø¯: deque Ù…Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø­Ø¬Ù… (maxlen=100)

âœ… 8. Logic ØºØ±ÙŠØ¨
   - Ù‚Ø¨Ù„: Ø¥Ø²Ø§Ù„Ø© PERSONAL_INFO Ø¨Ø·Ø±ÙŠÙ‚Ø© Ù…Ø¹Ù‚Ø¯Ø©
   - Ø¨Ø¹Ø¯: Ù…Ù†Ø·Ù‚ ÙˆØ§Ø¶Ø­ Ù…Ø¹ Lookup Tables

ðŸš€ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø·Ø¨Ù‚Ø©:

1ï¸âƒ£ State Machine: Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù…ØªØ¹Ø¯Ø¯Ø©
2ï¸âƒ£ Lookup Tables: Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ù…Ù†Ø·Ù‚ Ø§Ù„Ø·ÙˆÙŠÙ„Ø©  
3ï¸âƒ£ Decomposed Conditionals: ØªØ¨Ø³ÙŠØ· Ø§Ù„Ø´Ø±ÙˆØ· Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
4ï¸âƒ£ Parameter Objects: ØªÙ‚Ù„ÙŠÙ„ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„
5ï¸âƒ£ Strategy Pattern: Ù„Ù„ÙØ­Øµ Ø§Ù„Ù…Ø®ØªÙ„Ù
6ï¸âƒ£ Memory Management: Ø­Ù„ ØªØ³Ø±Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
7ï¸âƒ£ Compatibility Layer: Ø¯Ø¹Ù… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©

ðŸ“ˆ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:
- ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ 70%
- ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡ 5x  
- Ø­Ù„ ØªØ³Ø±Ø¨ Ø§Ù„Ø°Ø§ÙƒØ±Ø© 100%
- Ø³Ù‡ÙˆÙ„Ø© Ø§Ù„ØµÙŠØ§Ù†Ø© ÙˆØ§Ù„ØªØ·ÙˆÙŠØ±
- Ø¯Ø¹Ù… Ø§Ù„ØªÙˆØ§ÙÙ‚ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯

ðŸŽ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…:
# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© (Ù…ÙˆØµÙ‰ Ø¨Ù‡Ø§)
request = ModerationRequest(content="Ø§Ù„Ù†Øµ", age=10)
result = await service.check_content(request)

# Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© (Ù„Ù„ØªÙˆØ§ÙÙ‚)  
result = await service.check_content_legacy("Ø§Ù„Ù†Øµ", age=10)
"""




