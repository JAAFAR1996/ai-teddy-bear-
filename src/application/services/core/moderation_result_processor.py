#!/usr/bin/env python3
"""
ğŸ“Š Moderation Result Processor
Ù…Ø¹Ø§Ù„Ø¬Ø© ÙˆØªØ¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ­Øµ

Ø§Ù„Ù…Ø³Ø¤ÙˆÙ„ÙŠØ§Øª:
- ØªØ¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø©
- ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
- ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨Ø¯Ø§Ø¦Ù„ Ø§Ù„Ø¢Ù…Ù†Ø©
- Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø© ÙˆØ§Ù„Ø®Ø·ÙˆØ±Ø©
"""

import logging
from typing import Any, Dict, List, Optional

from .moderation import ContentCategory, ModerationResult, ModerationSeverity
from .moderation_helpers import ModerationRequest, ModerationLookupTables


class ModerationResultProcessor:
    """ğŸ“Š Ù…Ø¹Ø§Ù„Ø¬ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ÙØ­Øµ"""
    
    def __init__(self):
        self.logger = logging.getLogger(self.__class__.__name__)
    
    def aggregate_results(self, results: List[ModerationResult]) -> ModerationResult:
        """ğŸ“‹ ØªØ¬Ù…ÙŠØ¹ Ù†ØªØ§Ø¦Ø¬ Ù…ØªØ¹Ø¯Ø¯Ø©"""
        if not results:
            return self._create_safe_result("No results to aggregate")
        
        # ØªØµÙÙŠØ© Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØµØ§Ù„Ø­Ø©
        valid_results = [r for r in results if isinstance(r, ModerationResult)]
        if not valid_results:
            return self._create_safe_result("No valid results")
        
        # ØªØ¬Ù…ÙŠØ¹ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯ÙˆØ§Ù„ Ù…Ø¨Ø³Ø·Ø©
        safety_status = self._determine_overall_safety(valid_results)
        severity = self._determine_max_severity(valid_results)
        categories_and_scores = self._merge_categories_and_scores(valid_results)
        
        return ModerationResult(
            is_safe=safety_status,
            severity=severity,
            flagged_categories=categories_and_scores["categories"],
            confidence_scores=categories_and_scores["scores"],
            matched_rules=self._merge_rules(valid_results),
            context_notes=self._merge_notes(valid_results),
        )
    
    def format_response(self, result: ModerationResult, request: ModerationRequest) -> Dict[str, Any]:
        """ğŸ“ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©"""
        allowed = result.is_safe and result.severity in [
            ModerationSeverity.SAFE, 
            ModerationSeverity.LOW
        ]
        
        response = {
            "allowed": allowed,
            "severity": result.severity.value,
            "categories": [cat.value for cat in result.flagged_categories],
            "confidence": self._calculate_confidence(result),
            "reason": self._get_rejection_reason(result) if not allowed else None,
            "alternative_response": self._get_alternative_response(result) if not allowed else None,
            "timestamp": result.timestamp.isoformat() if hasattr(result, 'timestamp') else None,
        }
        
        return response
    
    def _determine_overall_safety(self, results: List[ModerationResult]) -> bool:
        """ğŸ›¡ï¸ ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø¹Ø§Ù…"""
        return all(result.is_safe for result in results)
    
    def _determine_max_severity(self, results: List[ModerationResult]) -> ModerationSeverity:
        """âš ï¸ ØªØ­Ø¯ÙŠØ¯ Ø£Ù‚ØµÙ‰ Ø®Ø·ÙˆØ±Ø©"""
        max_severity = ModerationSeverity.SAFE
        for result in results:
            if result.severity.value > max_severity.value:
                max_severity = result.severity
        return max_severity
    
    def _merge_categories_and_scores(self, results: List[ModerationResult]) -> Dict[str, Any]:
        """ğŸ”— Ø¯Ù…Ø¬ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙˆØ§Ù„Ù†Ù‚Ø§Ø·"""
        all_categories = []
        all_scores = {}
        
        for result in results:
            all_categories.extend(result.flagged_categories)
            
            # Ø¯Ù…Ø¬ Ø§Ù„Ù†Ù‚Ø§Ø· - Ø£Ø®Ø° Ø£Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ù„ÙƒÙ„ ØªØµÙ†ÙŠÙ
            for category, score in result.confidence_scores.items():
                all_scores[category] = max(all_scores.get(category, 0), score)
        
        return {
            "categories": list(set(all_categories)),
            "scores": all_scores
        }
    
    def _merge_rules(self, results: List[ModerationResult]) -> List[str]:
        """ğŸ“‹ Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø©"""
        all_rules = []
        for result in results:
            all_rules.extend(result.matched_rules)
        return list(set(all_rules))[:5]  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 5 Ù‚ÙˆØ§Ø¹Ø¯
    
    def _merge_notes(self, results: List[ModerationResult]) -> List[str]:
        """ğŸ“ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª"""
        all_notes = []
        for result in results:
            all_notes.extend(result.context_notes)
        return list(set(all_notes))
    
    def _calculate_confidence(self, result: ModerationResult) -> float:
        """ğŸ¯ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø«Ù‚Ø©"""
        if hasattr(result, 'overall_score') and result.overall_score:
            return result.overall_score
        
        if result.confidence_scores:
            return max(result.confidence_scores.values())
        
        return 1.0 if result.is_safe else 0.0
    
    def _get_rejection_reason(self, result: ModerationResult) -> str:
        """ğŸ“ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¨Ø¨ Ø§Ù„Ø±ÙØ¶"""
        try:
            return ModerationLookupTables.get_rejection_reason(result.flagged_categories)
        except Exception as e:
            self.logger.error(f"Error getting rejection reason: {e}")
            return "Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù‚Ø¯ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…ÙˆØ§Ø¯ ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨Ø©"
    
    def _get_alternative_response(self, result: ModerationResult) -> str:
        """ğŸ’¬ Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø±Ø¯ Ø¨Ø¯ÙŠÙ„"""
        try:
            return ModerationLookupTables.get_alternative_response(result.flagged_categories)
        except Exception as e:
            self.logger.error(f"Error getting alternative response: {e}")
            return "Ø¯Ø¹Ù†Ø§ Ù†ØªØ­Ø¯Ø« Ø¹Ù† Ø´ÙŠØ¡ Ø¢Ø®Ø±! âœ¨"
    
    def _create_safe_result(self, note: str) -> ModerationResult:
        """âœ… Ø¥Ù†Ø´Ø§Ø¡ Ù†ØªÙŠØ¬Ø© Ø¢Ù…Ù†Ø©"""
        return ModerationResult(
            is_safe=True,
            severity=ModerationSeverity.SAFE,
            flagged_categories=[],
            confidence_scores={},
            matched_rules=[],
            context_notes=[note]
        )
    
    def create_unsafe_response(self, reason: str, categories: List[ContentCategory]) -> Dict[str, Any]:
        """âŒ Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ ØºÙŠØ± Ø¢Ù…Ù†"""
        return {
            "allowed": False,
            "severity": ModerationSeverity.HIGH.value,
            "categories": [cat.value for cat in categories],
            "confidence": 0.9,
            "reason": reason,
            "alternative_response": ModerationLookupTables.get_alternative_response(categories),
        }
    
    def create_safe_response(self, reason: str) -> Dict[str, Any]:
        """âœ… Ø¥Ù†Ø´Ø§Ø¡ Ø±Ø¯ Ø¢Ù…Ù†"""
        return {
            "allowed": True,
            "severity": ModerationSeverity.SAFE.value,
            "categories": [],
            "confidence": 1.0,
            "reason": reason,
            "alternative_response": None,
        }


def create_result_processor() -> ModerationResultProcessor:
    """ğŸ­ Factory function"""
    return ModerationResultProcessor() 