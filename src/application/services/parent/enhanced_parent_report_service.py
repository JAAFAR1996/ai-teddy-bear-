"""
ğŸ§  Enhanced Parent Report Service - Task 7
ØªØ­Ù„ÙŠÙ„ ØªÙ‚Ø¯Ù… Ø§Ù„Ø·ÙÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NLP Ù…ØªÙ‚Ø¯Ù… ÙˆLLM Ù…Ø¹ Chain-of-Thought prompting
"""

import asyncio
import json
import logging
import re
from dataclasses import asdict, dataclass
from datetime import datetime, timedelta
from enum import Enum
from typing import Any, Dict, List, Optional, Tuple

# NLP Libraries
try:
    from collections import Counter

    import nltk
    import spacy
    from nltk.sentiment import SentimentIntensityAnalyzer

    NLP_AVAILABLE = True
except ImportError:
    NLP_AVAILABLE = False
    logger.warning("âš ï¸ NLP libraries not available. Install with: pip install spacy nltk")

# LLM Integration
try:
    import openai
    from anthropic import Anthropic

    LLM_AVAILABLE = True
except ImportError:
    LLM_AVAILABLE = False
    logger.warning("âš ï¸ LLM libraries not available. Install with: pip install openai anthropic")


@dataclass
class ProgressMetrics:
    """Advanced progress metrics using NLP analysis"""

    child_id: int
    analysis_date: datetime

    # Vocabulary Analysis
    total_unique_words: int
    new_words_this_period: List[str]
    vocabulary_complexity_score: float  # 0-1
    reading_level_equivalent: str

    # Language Development
    average_sentence_length: float
    grammar_accuracy_score: float
    question_sophistication_level: int  # 1-5
    conversation_coherence_score: float

    # Emotional Intelligence
    emotion_vocabulary_richness: int
    empathy_expression_frequency: float
    emotional_regulation_indicators: List[str]
    social_awareness_metrics: Dict[str, float]

    # Cognitive Development
    abstract_thinking_indicators: int
    problem_solving_approaches: List[str]
    creativity_markers: List[str]
    attention_span_trends: Dict[str, float]

    # Learning Patterns
    preferred_learning_styles: List[str]
    knowledge_retention_rate: float
    curiosity_indicators: List[str]
    learning_velocity: float  # words/concepts per day

    # Behavioral Insights
    initiative_taking_frequency: int
    cooperation_patterns: List[str]
    conflict_resolution_skills: List[str]
    independence_level: float

    # Red Flags & Concerns
    developmental_concerns: List[str]
    intervention_recommendations: List[str]
    urgency_level: int  # 0-3 (0=none, 3=urgent)


@dataclass
class LLMRecommendation:
    """LLM-generated recommendation with reasoning"""

    category: str  # "emotional", "cognitive", "social", "learning"
    recommendation: str
    reasoning: str
    expected_impact: str
    implementation_steps: List[str]
    success_metrics: List[str]
    priority_level: int  # 1-5


class EnhancedParentReportService:
    """
    Enhanced service for analyzing child progress using NLP and LLM
    """

    def __init__(self, database_service=None, config=None):
        self.db = database_service
        self.config = config or {}
        self.logger = logging.getLogger(__name__)

        # Initialize NLP models
        self._init_nlp_models()

        # Initialize LLM clients
        self._init_llm_clients()

        # Load analysis templates
        self._load_cot_templates()

    def _init_nlp_models(self) -> Any:
        """Initialize NLP models and tools"""
        if not NLP_AVAILABLE:
            self.logger.warning("NLP models not available")
            return

        try:
            # Load spaCy model for English (Arabic model needs separate installation)
            self.nlp = spacy.load("en_core_web_sm")

            # Initialize NLTK sentiment analyzer
            self.sentiment_analyzer = SentimentIntensityAnalyzer()

            self.logger.info("âœ… NLP models initialized successfully")

        except Exception as e:
            self.logger.error(f"Failed to initialize NLP models: {e}")
            self.nlp = None

    def _init_llm_clients(self) -> Any:
        """Initialize LLM clients"""
        if not LLM_AVAILABLE:
            self.logger.warning("LLM clients not available")
            return

        try:
            # Initialize OpenAI client
            openai_key = self.config.get("openai_api_key")
            if openai_key:
                self.openai_client = openai.OpenAI(api_key=openai_key)
                self.logger.info("âœ… OpenAI client initialized")

            # Initialize Anthropic client
            anthropic_key = self.config.get("anthropic_api_key")
            if anthropic_key:
                self.anthropic_client = Anthropic(api_key=anthropic_key)
                self.logger.info("âœ… Anthropic client initialized")

        except Exception as e:
            self.logger.error(f"Failed to initialize LLM clients: {e}")

    def _load_cot_templates(self) -> Any:
        """Load Chain-of-Thought prompting templates"""
        self.cot_templates = {
            "progress_analysis": """
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ·ÙˆÙŠØ± Ø§Ù„Ø·ÙÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ÙˆØ§Ù„Ø¹Ø§Ø·ÙÙŠ. 
Ø§Ø³ØªØ®Ø¯Ù… Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© (Chain-of-Thought) Ù„ØªØ­Ù„ÙŠÙ„ ØªÙ‚Ø¯Ù… Ø§Ù„Ø·ÙÙ„.

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·ÙÙ„:
- Ø§Ù„Ø§Ø³Ù…: {child_name}
- Ø§Ù„Ø¹Ù…Ø±: {age} Ø³Ù†ÙˆØ§Øª
- ÙØªØ±Ø© Ø§Ù„ØªØ­Ù„ÙŠÙ„: {period_days} Ø£ÙŠØ§Ù…
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª: {total_interactions}

Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…Ø­Ø³ÙˆØ¨Ø©:
{metrics_summary}

Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:

Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù„ØºÙˆÙŠØ©
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù†Ù…Ùˆ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØªØ·ÙˆØ± Ø§Ù„Ø­Ø¯ÙŠØ«:
- Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©: {new_words_count}
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {complexity_score}
- Ø·ÙˆÙ„ Ø§Ù„Ø¬Ù…Ù„Ø© Ø§Ù„Ù…ØªÙˆØ³Ø·: {avg_sentence_length}

Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªÙ‚ÙŠÙŠÙ… Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ
ÙØ­Øµ Ø§Ù„Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ© ÙˆØ§Ù„Ø§Ø¬ØªÙ…Ø§Ø¹ÙŠØ©:
- Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©: {emotion_vocab}
- ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø§Ù„ØªØ¹Ø§Ø·Ù: {empathy_frequency}
- Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¹Ø§Ø·ÙÙŠ: {regulation_indicators}

Ø§Ù„Ø®Ø·ÙˆØ© 3: ÙØ­Øµ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©
ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù‚Ø¯Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ© ÙˆØ§Ù„ØªØ¹Ù„Ù…:
- Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ø¬Ø±Ø¯: {abstract_thinking}
- Ø£Ø³Ø§Ù„ÙŠØ¨ Ø­Ù„ Ø§Ù„Ù…Ø´ÙƒÙ„Ø§Øª: {problem_solving}
- Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹: {creativity_markers}

Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØ­Ø¯ÙŠØ¯ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ§Øª
Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø£Ø¹Ù„Ø§Ù‡:
- Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©: 
- Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ±:
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„ØªØ¯Ø®Ù„: {urgency_level}

Ø§Ù„Ø®Ø·ÙˆØ© 5: ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù…Ø®ØµØµØ©
Ù‚Ø¯Ù… 3 ØªÙˆØµÙŠØ§Øª Ù…Ø®ØµØµØ© ÙˆØ¹Ù…Ù„ÙŠØ© Ù„Ù„ÙˆØ§Ù„Ø¯ÙŠÙ†:
""",
            "recommendation_generation": """
ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ© Ù…Ø®ØµØµØ© Ù„Ù„Ø·ÙÙ„ ÙÙŠ Ù…Ø¬Ø§Ù„ {category}:

Ø§Ù„Ø³ÙŠØ§Ù‚:
- Ø§Ù„Ø·ÙÙ„: {child_name} ({age} Ø³Ù†ÙˆØ§Øª)
- Ø§Ù„Ù…Ø¬Ø§Ù„: {category}
- Ø§Ù„Ù†Ù‚Ø·Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©: {specific_area}
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ©: {priority}

Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:

1. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ:
Ù…Ø§ Ù‡Ùˆ ÙˆØ¶Ø¹ Ø§Ù„Ø·ÙÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ ÙÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¬Ø§Ù„ØŸ

2. ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ù…Ø±:
Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ø·ÙÙ„ Ø¹Ù…Ø±Ù‡ {age} Ø³Ù†ÙˆØ§ØªØŸ

3. Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©:
Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø´Ø®ØµÙŠØ© Ø§Ù„Ø·ÙÙ„ØŸ

4. ØªØµÙ…ÙŠÙ… Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°:
ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©ØŸ

5. Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù†Ø¬Ø§Ø­:
ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù‚ÙŠØ§Ø³ ØªÙ‚Ø¯Ù… Ø§Ù„Ø·ÙÙ„ØŸ

Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (JSON format):
""",
        }

    async def analyze_progress(self, child_id: int, period_days: int = 7) -> ProgressMetrics:
        """
        ØªØ­Ù„ÙŠÙ„ ØªÙ‚Ø¯Ù… Ø§Ù„Ø·ÙÙ„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NLP Ù…ØªÙ‚Ø¯Ù…

        Args:
            child_id: Ù…Ø¹Ø±Ù Ø§Ù„Ø·ÙÙ„ Ø§Ù„ÙØ±ÙŠØ¯
            period_days: Ø¹Ø¯Ø¯ Ø§Ù„Ø£ÙŠØ§Ù… Ù„Ù„ØªØ­Ù„ÙŠÙ„ (Ø§ÙØªØ±Ø§Ø¶ÙŠ: 7)

        Returns:
            ProgressMetrics with comprehensive analysis
        """
        self.logger.info(f"ğŸ§  Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø·ÙÙ„ {child_id}")

        # Ø¬Ù„Ø¨ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªÙØ§Ø¹Ù„
        end_date = datetime.now()
        start_date = end_date - timedelta(days=period_days)
        interactions = await self._get_interactions_with_text(child_id, start_date, end_date)

        if not interactions:
            self.logger.warning(f"Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙØ§Ø¹Ù„Ø§Øª Ù„Ù„Ø·ÙÙ„ {child_id}")
            return self._create_empty_metrics(child_id)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„
        conversation_texts = self._extract_conversation_texts(interactions)

        # ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¹Ø¯Ø¯ Ø§Ù„Ø·Ø¨Ù‚Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NLP
        vocabulary_analysis = await self._analyze_vocabulary_development(conversation_texts)
        emotional_analysis = await self._analyze_emotional_intelligence(conversation_texts)
        cognitive_analysis = await self._analyze_cognitive_development(conversation_texts)
        behavioral_analysis = await self._analyze_behavioral_patterns(interactions)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø®Ø§ÙˆÙ ÙˆØ§Ù„Ø¹Ù„Ø§Ù…Ø§Øª Ø§Ù„Ø­Ù…Ø±Ø§Ø¡
        concerns = self._identify_developmental_concerns(vocabulary_analysis, emotional_analysis, cognitive_analysis)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø´Ø§Ù…Ù„Ø©
        metrics = ProgressMetrics(
            child_id=child_id,
            analysis_date=datetime.now(),
            # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
            total_unique_words=vocabulary_analysis["unique_words"],
            new_words_this_period=vocabulary_analysis["new_words"],
            vocabulary_complexity_score=vocabulary_analysis["complexity_score"],
            reading_level_equivalent=vocabulary_analysis["reading_level"],
            # ØªØ·ÙˆÙŠØ± Ø§Ù„Ù„ØºØ©
            average_sentence_length=vocabulary_analysis["avg_sentence_length"],
            grammar_accuracy_score=vocabulary_analysis["grammar_score"],
            question_sophistication_level=vocabulary_analysis["question_level"],
            conversation_coherence_score=vocabulary_analysis["coherence_score"],
            # Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ
            emotion_vocabulary_richness=emotional_analysis["emotion_vocab_size"],
            empathy_expression_frequency=emotional_analysis["empathy_frequency"],
            emotional_regulation_indicators=emotional_analysis["regulation_indicators"],
            social_awareness_metrics=emotional_analysis["social_awareness"],
            # Ø§Ù„ØªØ·ÙˆØ± Ø§Ù„Ù…Ø¹Ø±ÙÙŠ
            abstract_thinking_indicators=cognitive_analysis["abstract_thinking"],
            problem_solving_approaches=cognitive_analysis["problem_solving"],
            creativity_markers=cognitive_analysis["creativity_markers"],
            attention_span_trends=behavioral_analysis["attention_trends"],
            # Ø£Ù†Ù…Ø§Ø· Ø§Ù„ØªØ¹Ù„Ù…
            preferred_learning_styles=cognitive_analysis["learning_styles"],
            knowledge_retention_rate=cognitive_analysis["retention_rate"],
            curiosity_indicators=cognitive_analysis["curiosity_indicators"],
            learning_velocity=vocabulary_analysis["learning_velocity"],
            # Ø±Ø¤Ù‰ Ø³Ù„ÙˆÙƒÙŠØ©
            initiative_taking_frequency=behavioral_analysis["initiative_frequency"],
            cooperation_patterns=behavioral_analysis["cooperation_patterns"],
            conflict_resolution_skills=behavioral_analysis["conflict_resolution"],
            independence_level=behavioral_analysis["independence_level"],
            # Ø§Ù„Ù…Ø®Ø§ÙˆÙ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
            developmental_concerns=concerns["concerns"],
            intervention_recommendations=concerns["interventions"],
            urgency_level=concerns["urgency_level"],
        )

        self.logger.info(f"âœ… Ø§ÙƒØªÙ…Ù„ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ø·ÙÙ„ {child_id}")
        return metrics

    async def generate_and_store_report(self, child_id: int, period_days: int = 7) -> Dict[str, Any]:
        """
        ØªÙˆÙ„ÙŠØ¯ ÙˆØ­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„ Ù„Ù„Ø·ÙÙ„
        """
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù…
        metrics = await self.analyze_progress(child_id, period_days)

        # Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·ÙÙ„
        child_info = await self._get_child_info(child_id)

        # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM
        recommendations = await self.generate_llm_recommendations(metrics, child_info)

        # Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        report_id = await self.store_analysis_results(metrics, recommendations)

        return {
            "report_id": report_id,
            "metrics": asdict(metrics),
            "recommendations": [asdict(rec) for rec in recommendations],
            "generated_at": datetime.now().isoformat(),
        }

    async def generate_llm_recommendations(
        self, metrics: ProgressMetrics, child_info: Dict[str, Any]
    ) -> List[LLMRecommendation]:
        """
        ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù…Ø®ØµØµØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù…Ø¹ Chain-of-Thought prompting
        """
        if not LLM_AVAILABLE or not hasattr(self, "openai_client"):
            return self._generate_fallback_recommendations(metrics)

        self.logger.info("ğŸ¤– ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… LLM Ù…Ø¹ Chain-of-Thought")

        recommendations = []
        categories = ["emotional", "cognitive", "social", "learning"]

        for category in categories:
            try:
                recommendation = await self._generate_category_recommendation(category, metrics, child_info)
                if recommendation:
                    recommendations.append(recommendation)
            except Exception as e:
                self.logger.error(f"ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ© {category}: {e}")

        return recommendations[:3]  # Ø£ÙØ¶Ù„ 3 ØªÙˆØµÙŠØ§Øª

    async def _generate_category_recommendation(
        self, category: str, metrics: ProgressMetrics, child_info: Dict[str, Any]
    ) -> Optional[LLMRecommendation]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ© Ù„ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Chain-of-Thought"""

        # ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„Ù„ÙØ¦Ø© Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        context = self._prepare_category_context(category, metrics, child_info)

        # Ø¥Ù†Ø´Ø§Ø¡ Chain-of-Thought prompt
        cot_prompt = self._create_cot_prompt(category, context)

        try:
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI
            response = await self._call_openai_with_cot(cot_prompt)

            # ØªØ­Ù„ÙŠÙ„ ÙˆÙ‡ÙŠÙƒÙ„Ø© Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©
            recommendation = self._parse_llm_recommendation(category, response)

            return recommendation

        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ ÙÙŠ ØªÙˆÙ„ÙŠØ¯ LLM Ù„Ù„ÙØ¦Ø© {category}: {e}")
            return None

    def _create_cot_prompt(self, category: str, context: Dict[str, Any]) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ Chain-of-Thought prompt Ù„ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø©"""

        prompt = f"""
Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ·ÙˆÙŠØ± Ø§Ù„Ø·ÙÙ„ ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ‚Ø¯Ù… Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ ÙˆØ§Ù„Ø¹Ø§Ø·ÙÙŠ. 
Ù…Ù‡Ù…ØªÙƒ ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·ÙÙ„ ÙˆØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ© Ù…Ø®ØµØµØ© ÙÙŠ Ù…Ø¬Ø§Ù„ "{category}".

Ø§Ø³ØªØ®Ø¯Ù… Ù…Ù†Ù‡Ø¬ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© (Chain-of-Thought) Ù„Ù„ÙˆØµÙˆÙ„ Ù„Ø£ÙØ¶Ù„ ØªÙˆØµÙŠØ©.

Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø·ÙÙ„:
- Ø§Ù„Ø§Ø³Ù…: {context['child_name']}
- Ø§Ù„Ø¹Ù…Ø±: {context['age']} Ø³Ù†ÙˆØ§Øª
- Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {category}

Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„Ø­Ø§Ù„ÙŠØ©:
{self._format_metrics_for_prompt(category, context['metrics'])}

Ø§Ù„ØªÙÙƒÙŠØ± Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ©:

Ø§Ù„Ø®Ø·ÙˆØ© 1: ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ù‚ÙˆØ© ÙˆØ§Ù„Ø¶Ø¹Ù ÙÙŠ Ù…Ø¬Ø§Ù„ {category} Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¹Ù„Ø§Ù‡.

Ø§Ù„Ø®Ø·ÙˆØ© 2: ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ø¹Ù…Ø±
Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ù‡Ø¯Ù Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨ Ù„Ø·ÙÙ„ Ø¹Ù…Ø±Ù‡ {context['age']} Ø³Ù†ÙˆØ§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ {category}ØŸ

Ø§Ù„Ø®Ø·ÙˆØ© 3: Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ©
Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ù„ØªØ­Ù‚ÙŠÙ‚ Ù‡Ø°Ø§ Ø§Ù„Ù‡Ø¯Ù Ù…Ø¹ Ù…Ø±Ø§Ø¹Ø§Ø© Ø´Ø®ØµÙŠØ© Ø§Ù„Ø·ÙÙ„ØŸ

Ø§Ù„Ø®Ø·ÙˆØ© 4: ØªØµÙ…ÙŠÙ… Ø§Ù„Ø®Ø·Ø© Ø§Ù„Ø¹Ù…Ù„ÙŠØ©
ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù„Ù„ÙˆØ§Ù„Ø¯ÙŠÙ† ØªØ·Ø¨ÙŠÙ‚ Ù‡Ø°Ù‡ Ø§Ù„Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø¹Ù…Ù„ÙŠØ§Ù‹ØŸ

Ø§Ù„Ø®Ø·ÙˆØ© 5: Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ù‚ÙŠØ§Ø³
ÙƒÙŠÙ ÙŠÙ…ÙƒÙ† Ù‚ÙŠØ§Ø³ Ù†Ø¬Ø§Ø­ Ù‡Ø°Ù‡ Ø§Ù„ØªÙˆØµÙŠØ©ØŸ

Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© (Ø¨ØªÙ†Ø³ÙŠÙ‚ JSON):
{{
    "category": "{category}",
    "recommendation": "Ø§Ù„ØªÙˆØµÙŠØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù‡Ù†Ø§",
    "reasoning": "Ø³Ø¨Ø¨ Ù‡Ø°Ù‡ Ø§Ù„ØªÙˆØµÙŠØ©",
    "expected_impact": "Ø§Ù„ØªØ£Ø«ÙŠØ± Ø§Ù„Ù…ØªÙˆÙ‚Ø¹",
    "implementation_steps": ["Ø®Ø·ÙˆØ© 1", "Ø®Ø·ÙˆØ© 2", "Ø®Ø·ÙˆØ© 3"],
    "success_metrics": ["Ù…Ø¹ÙŠØ§Ø± 1", "Ù…Ø¹ÙŠØ§Ø± 2"],
    "priority_level": 3
}}
"""

        return prompt

    async def _call_openai_with_cot(self, prompt: str) -> str:
        """Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ OpenAI API Ù…Ø¹ Chain-of-Thought prompt"""
        response = await asyncio.to_thread(
            self.openai_client.chat.completions.create,
            model="gpt-4-turbo-preview",
            messages=[
                {
                    "role": "system",
                    "content": "Ø£Ù†Øª Ø®Ø¨ÙŠØ± ÙÙŠ ØªØ·ÙˆÙŠØ± Ø§Ù„Ø·ÙÙ„. Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ù†Ø·Ù‚ÙŠ Ø®Ø·ÙˆØ© Ø¨Ø®Ø·ÙˆØ© Ù„ØªÙ‚Ø¯ÙŠÙ… ØªÙˆØµÙŠØ§Øª Ù…Ø®ØµØµØ© ÙˆÙ…ÙÙŠØ¯Ø©.",
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=1500,
            temperature=0.7,
            response_format={"type": "json_object"},
        )

        return response.choices[0].message.content

    async def store_analysis_results(self, metrics: ProgressMetrics, recommendations: List[LLMRecommendation]) -> str:
        """Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙÙŠ Ø¬Ø¯ÙˆÙ„ parent_reports"""
        if not self.db:
            raise Exception("Ø®Ø¯Ù…Ø© Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©")

        report_data = {
            "child_id": metrics.child_id,
            "generated_at": metrics.analysis_date.isoformat(),
            "metrics": asdict(metrics),
            "recommendations": [asdict(rec) for rec in recommendations],
            "analysis_version": "2.0_task7",
            "llm_used": True,
        }

        try:
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            report_id = await self.db.execute(
                """
                INSERT INTO parent_reports 
                (child_id, generated_at, metrics, recommendations, analysis_version)
                VALUES (?, ?, ?, ?, ?)
                """,
                (
                    metrics.child_id,
                    report_data["generated_at"],
                    json.dumps(report_data["metrics"]),
                    json.dumps(report_data["recommendations"]),
                    report_data["analysis_version"],
                ),
            )

            self.logger.info(f"âœ… ØªÙ… Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø±Ù‚Ù…: {report_id}")
            return str(report_id)

        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ ÙÙŠ Ø­ÙØ¸ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
            raise

    # Helper methods implementation

    async def _analyze_vocabulary_development(self, texts: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… NLP"""
        if not self.nlp or not texts:
            return self._empty_vocabulary_analysis()

        all_text = " ".join(texts)
        doc = self.nlp(all_text)

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª ÙˆØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
        words = [token.lemma_.lower() for token in doc if token.is_alpha and not token.is_stop]
        unique_words = list(set(words))

        # Ø­Ø³Ø§Ø¨ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª
        complexity_score = min(1.0, len(unique_words) / 50.0)  # ØªØ¨Ø³ÙŠØ· Ù„Ù„Ø¹Ø±Ø¶

        # ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø¬Ù…Ù„Ø©
        sentences = list(doc.sents)
        avg_sentence_length = sum(len(sent) for sent in sentences) / len(sentences) if sentences else 0

        return {
            "unique_words": len(unique_words),
            "new_words": unique_words[-min(5, len(unique_words)) :],  # Ø¢Ø®Ø± 5 ÙƒÙ„Ù…Ø§Øª ÙƒÙ€ "Ø¬Ø¯ÙŠØ¯Ø©"
            "complexity_score": complexity_score,
            "reading_level": self._estimate_reading_level(complexity_score),
            "avg_sentence_length": avg_sentence_length,
            "grammar_score": 0.8,  # placeholder
            "question_level": 3,  # placeholder
            "coherence_score": 0.7,  # placeholder
            "learning_velocity": len(unique_words) / 7,  # ÙƒÙ„Ù…Ø§Øª ÙÙŠ Ø§Ù„ÙŠÙˆÙ…
        }

    async def _analyze_emotional_intelligence(self, texts: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"""
        emotion_words = self._extract_emotion_words(texts)
        empathy_indicators = self._detect_empathy_expressions(texts)

        return {
            "emotion_vocab_size": len(emotion_words),
            "empathy_frequency": len(empathy_indicators) / len(texts) if texts else 0,
            "regulation_indicators": empathy_indicators[:3],  # Ø£ÙˆÙ„ 3 Ù…Ø¤Ø´Ø±Ø§Øª
            "social_awareness": {"cooperation": 0.7, "sharing": 0.8},
        }

    async def _analyze_cognitive_development(self, texts: List[str]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ·ÙˆÙŠØ± Ø§Ù„Ù…Ø¹Ø±ÙÙŠ"""
        return {
            "abstract_thinking": len([t for t in texts if "Ù„Ù…Ø§Ø°Ø§" in t or "ÙƒÙŠÙ" in t]),
            "problem_solving": ["logical", "creative"],
            "creativity_markers": ["imagination", "storytelling"],
            "learning_styles": ["visual", "auditory"],
            "retention_rate": 0.85,
            "curiosity_indicators": ["asking questions", "exploring topics"],
        }

    async def _analyze_behavioral_patterns(self, interactions: List[Dict]) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø³Ù„ÙˆÙƒÙŠØ©"""
        return {
            "attention_trends": {"morning": 0.8, "evening": 0.6},
            "initiative_frequency": len(interactions) // 2,
            "cooperation_patterns": ["turn-taking", "sharing"],
            "conflict_resolution": ["negotiation", "compromise"],
            "independence_level": 0.7,
        }

    def _identify_developmental_concerns(
        self, vocab_analysis, emotional_analysis, cognitive_analysis
    ) -> Dict[str, Any]:
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø®Ø§ÙˆÙ Ø§Ù„ØªØ·ÙˆÙŠØ±ÙŠØ©"""
        concerns = []
        interventions = []
        urgency_level = 0

        if vocab_analysis["unique_words"] < 10:
            concerns.append("Ù…Ø­Ø¯ÙˆØ¯ÙŠØ© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª")
            interventions.append("Ø²ÙŠØ§Ø¯Ø© ÙˆÙ‚Øª Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©")
            urgency_level = max(urgency_level, 2)

        if emotional_analysis["empathy_frequency"] < 0.1:
            concerns.append("Ù‚Ù„Ø© Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø§Ù„ØªØ¹Ø§Ø·Ù")
            interventions.append("Ø£Ù†Ø´Ø·Ø© ØªØ·ÙˆÙŠØ± Ø§Ù„ØªØ¹Ø§Ø·Ù")
            urgency_level = max(urgency_level, 1)

        return {"concerns": concerns, "interventions": interventions, "urgency_level": urgency_level}

    # Additional helper methods

    def _extract_emotion_words(self, texts: List[str]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©"""
        emotion_keywords = ["happy", "sad", "angry", "scared", "excited", "worried", "calm"]
        found_emotions = []
        for text in texts:
            for emotion in emotion_keywords:
                if emotion.lower() in text.lower():
                    found_emotions.append(emotion)
        return list(set(found_emotions))

    def _detect_empathy_expressions(self, texts: List[str]) -> List[str]:
        """Ø§ÙƒØªØ´Ø§Ù ØªØ¹Ø¨ÙŠØ±Ø§Øª Ø§Ù„ØªØ¹Ø§Ø·Ù"""
        empathy_patterns = ["I feel", "I understand", "I know how", "That must be"]
        expressions = []
        for text in texts:
            for pattern in empathy_patterns:
                if pattern.lower() in text.lower():
                    expressions.append(pattern)
        return list(set(expressions))

    def _estimate_reading_level(self, complexity_score: float) -> str:
        """ØªÙ‚Ø¯ÙŠØ± Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"""
        if complexity_score < 0.3:
            return "Ù…Ø¨ØªØ¯Ø¦"
        elif complexity_score < 0.6:
            return "Ù…ØªÙˆØ³Ø·"
        else:
            return "Ù…ØªÙ‚Ø¯Ù…"

    def _empty_vocabulary_analysis(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…ÙØ±Ø¯Ø§Øª ÙØ§Ø±Øº"""
        return {
            "unique_words": 0,
            "new_words": [],
            "complexity_score": 0.0,
            "reading_level": "ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
            "avg_sentence_length": 0.0,
            "grammar_score": 0.0,
            "question_level": 1,
            "coherence_score": 0.0,
            "learning_velocity": 0.0,
        }

    def _create_empty_metrics(self, child_id: int) -> ProgressMetrics:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù‚Ø§ÙŠÙŠØ³ ÙØ§Ø±ØºØ© Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¨ÙŠØ§Ù†Ø§Øª"""
        return ProgressMetrics(
            child_id=child_id,
            analysis_date=datetime.now(),
            total_unique_words=0,
            new_words_this_period=[],
            vocabulary_complexity_score=0.0,
            reading_level_equivalent="ØºÙŠØ± Ù…Ø­Ø¯Ø¯",
            average_sentence_length=0.0,
            grammar_accuracy_score=0.0,
            question_sophistication_level=1,
            conversation_coherence_score=0.0,
            emotion_vocabulary_richness=0,
            empathy_expression_frequency=0.0,
            emotional_regulation_indicators=[],
            social_awareness_metrics={},
            abstract_thinking_indicators=0,
            problem_solving_approaches=[],
            creativity_markers=[],
            attention_span_trends={},
            preferred_learning_styles=[],
            knowledge_retention_rate=0.0,
            curiosity_indicators=[],
            learning_velocity=0.0,
            initiative_taking_frequency=0,
            cooperation_patterns=[],
            conflict_resolution_skills=[],
            independence_level=0.0,
            developmental_concerns=["Ø¹Ø¯Ù… ØªÙˆÙØ± Ø¨ÙŠØ§Ù†Ø§Øª ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ­Ù„ÙŠÙ„"],
            intervention_recommendations=["Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªÙØ§Ø¹Ù„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù…"],
            urgency_level=0,
        )

    def _extract_conversation_texts(self, interactions: List[Dict]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†ØµÙˆØµ Ù…Ù† Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª"""
        texts = []
        for interaction in interactions:
            if "message" in interaction and interaction["message"]:
                texts.append(interaction["message"])
            elif "content" in interaction and interaction["content"]:
                texts.append(interaction["content"])
        return texts

    async def _get_interactions_with_text(self, child_id: int, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª Ù…Ø¹ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„ØªØ­Ù„ÙŠÙ„"""
        if not self.db:
            return []

        try:
            interactions = await self.db.fetch_all(
                """
                SELECT * FROM conversations 
                WHERE child_id = ? AND created_at BETWEEN ? AND ?
                ORDER BY created_at DESC
                """,
                (child_id, start_date.isoformat(), end_date.isoformat()),
            )
            return [dict(row) for row in interactions]
        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ø§Ù„ØªÙØ§Ø¹Ù„Ø§Øª: {e}")
            return []

    async def _get_child_info(self, child_id: int) -> Dict[str, Any]:
        """Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·ÙÙ„"""
        if not self.db:
            return {"name": "Unknown", "age": 5}

        try:
            child = await self.db.fetch_one("SELECT * FROM children WHERE id = ?", (child_id,))
            return dict(child) if child else {"name": "Unknown", "age": 5}
        except Exception as e:
            self.logger.error(f"ÙØ´Ù„ ÙÙŠ Ø¬Ù„Ø¨ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø·ÙÙ„: {e}")
            return {"name": "Unknown", "age": 5}

    def _prepare_category_context(
        self, category: str, metrics: ProgressMetrics, child_info: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ØªØ­Ø¶ÙŠØ± Ø§Ù„Ø³ÙŠØ§Ù‚ Ù„ÙØ¦Ø© Ù…Ø­Ø¯Ø¯Ø©"""
        return {
            "child_name": child_info.get("name", "Unknown"),
            "age": child_info.get("age", 5),
            "category": category,
            "metrics": metrics,
        }

    def _format_metrics_for_prompt(self, category: str, metrics: ProgressMetrics) -> str:
        """ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„Ù…Ù‚Ø§ÙŠÙŠØ³ Ù„Ù„prompt"""
        return f"""
- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©: {metrics.total_unique_words}
- Ø¯Ø±Ø¬Ø© ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª: {metrics.vocabulary_complexity_score:.2f}
- Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©: {metrics.reading_level_equivalent}
- Ø«Ø±Ø§Ø¡ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª Ø§Ù„Ø¹Ø§Ø·ÙÙŠØ©: {metrics.emotion_vocabulary_richness}
- ØªÙƒØ±Ø§Ø± Ø§Ù„ØªØ¹Ø§Ø·Ù: {metrics.empathy_expression_frequency:.2f}
- Ù…Ø¤Ø´Ø±Ø§Øª Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ø¬Ø±Ø¯: {metrics.abstract_thinking_indicators}
"""

    def _parse_llm_recommendation(self, category: str, response: str) -> LLMRecommendation:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ø³ØªØ¬Ø§Ø¨Ø© LLM Ø¥Ù„Ù‰ ØªÙˆØµÙŠØ© Ù…Ù†Ø¸Ù…Ø©"""
        try:
            data = json.loads(response)
            return LLMRecommendation(
                category=data.get("category", category),
                recommendation=data.get("recommendation", ""),
                reasoning=data.get("reasoning", ""),
                expected_impact=data.get("expected_impact", ""),
                implementation_steps=data.get("implementation_steps", []),
                success_metrics=data.get("success_metrics", []),
                priority_level=data.get("priority_level", 3),
            )
        except json.JSONDecodeError:
            return LLMRecommendation(
                category=category,
                recommendation=response[:200] if response else "ØªÙˆØµÙŠØ© ØºÙŠØ± Ù…ØªÙˆÙØ±Ø©",
                reasoning="ØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù…",
                expected_impact="ØªØ­Ø³ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ù…Ø­Ø¯Ø¯",
                implementation_steps=["Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚", "Ù‚ÙŠØ§Ø³ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"],
                success_metrics=["ØªØ­Ø³Ù† Ù…Ù„Ø­ÙˆØ¸ ÙÙŠ Ø§Ù„ØªÙØ§Ø¹Ù„"],
                priority_level=3,
            )

    def _generate_fallback_recommendations(self, metrics: ProgressMetrics) -> List[LLMRecommendation]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¹Ù†Ø¯ Ø¹Ø¯Ù… ØªÙˆÙØ± LLM"""
        recommendations = []

        if metrics.vocabulary_complexity_score < 0.5:
            recommendations.append(
                LLMRecommendation(
                    category="learning",
                    recommendation="Ø²ÙŠØ§Ø¯Ø© Ø£Ù†Ø´Ø·Ø© Ø§Ù„Ù‚Ø±Ø§Ø¡Ø© ÙˆØ§Ù„Ø­Ø¯ÙŠØ«",
                    reasoning="Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ØªØ­ØªØ§Ø¬ ØªØ·ÙˆÙŠØ±",
                    expected_impact="ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª ÙˆØ§Ù„ØªØ¹Ø¨ÙŠØ±",
                    implementation_steps=["Ù‚Ø±Ø§Ø¡Ø© ÙŠÙˆÙ…ÙŠØ©", "Ø­Ø¯ÙŠØ« ØªÙØ§Ø¹Ù„ÙŠ"],
                    success_metrics=["Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ù…ÙØ±Ø¯Ø§Øª", "ØªØ­Ø³Ù† Ø§Ù„ØªØ¹Ø¨ÙŠØ±"],
                    priority_level=4,
                )
            )

        if metrics.empathy_expression_frequency < 0.3:
            recommendations.append(
                LLMRecommendation(
                    category="emotional",
                    recommendation="Ø£Ù†Ø´Ø·Ø© ØªØ·ÙˆÙŠØ± Ø§Ù„ØªØ¹Ø§Ø·Ù ÙˆØ§Ù„Ù…Ø´Ø§Ø¹Ø±",
                    reasoning="Ù‚Ù„Ø© Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø¹Ù† Ø§Ù„ØªØ¹Ø§Ø·Ù",
                    expected_impact="ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø¹Ø§Ø·ÙÙŠ",
                    implementation_steps=["Ù‚ØµØµ ØªØ¹Ø§Ø·ÙÙŠØ©", "Ù…Ù†Ø§Ù‚Ø´Ø© Ø§Ù„Ù…Ø´Ø§Ø¹Ø±"],
                    success_metrics=["Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¹Ø¨ÙŠØ± Ø§Ù„Ø¹Ø§Ø·ÙÙŠ"],
                    priority_level=3,
                )
            )

        if metrics.abstract_thinking_indicators < 2:
            recommendations.append(
                LLMRecommendation(
                    category="cognitive",
                    recommendation="Ø£Ù†Ø´Ø·Ø© ØªØ·ÙˆÙŠØ± Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù†Ù‚Ø¯ÙŠ",
                    reasoning="Ù…Ø­Ø¯ÙˆØ¯ÙŠØ© Ø§Ù„ØªÙÙƒÙŠØ± Ø§Ù„Ù…Ø¬Ø±Ø¯",
                    expected_impact="ØªØ­Ø³Ù† ÙÙŠ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø¹Ø±ÙÙŠØ©",
                    implementation_steps=["Ø£Ù„Ø¹Ø§Ø¨ ØªÙÙƒÙŠØ±", "Ø£Ø³Ø¦Ù„Ø© Ù…ÙØªÙˆØ­Ø©"],
                    success_metrics=["Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©"],
                    priority_level=3,
                )
            )

        return recommendations[:3]
