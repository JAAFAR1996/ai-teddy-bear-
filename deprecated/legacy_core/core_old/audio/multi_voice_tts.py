# [AI-Generated by Amazon Q]: تم إضافة هذا الكود تلقائياً وفق دليل المشروع.
import asyncio
from elevenlabs import generate, Voice, VoiceSettings, set_api_key
from typing import Dict, List, Optional
import numpy as np
from pydub import AudioSegment
import io
from datetime import datetime

class VoiceProfile:
    """ملف تعريف الصوت"""
    def __init__(self, voice_id: str, name: str, settings: Dict):
        self.voice_id = voice_id
        self.name = name
        self.settings = VoiceSettings(**settings)

class MultiVoiceTTS:
    """محرك تحويل النص لصوت متعدد الشخصيات"""
    
    def __init__(self, elevenlabs_key: str):
        set_api_key(elevenlabs_key)
        self.voice_profiles = self._initialize_voices()
        
    def _initialize_voices(self) -> Dict[str, VoiceProfile]:
        """تهيئة ملفات الأصوات"""
        return {
            'narrator_female': VoiceProfile(
                voice_id="EXAVITQu4vr4xnSDxMaL",
                name="الراوية",
                settings={
                    "stability": 0.8,
                    "similarity_boost": 0.75,
                    "style": 0.4,
                    "use_speaker_boost": True
                }
            ),
            'child_boy': VoiceProfile(
                voice_id="TxGEqnHWrfWFTfGW9XjX",
                name="طفل",
                settings={
                    "stability": 0.5,
                    "similarity_boost": 0.75,
                    "style": 0.8,
                    "use_speaker_boost": True
                }
            ),
            'mother': VoiceProfile(
                voice_id="pNInz6obpgDQGcFmaJgB",
                name="الأم",
                settings={
                    "stability": 0.85,
                    "similarity_boost": 0.8,
                    "style": 0.3,
                    "use_speaker_boost": True
                }
            )
        }
    
    async def generate_multi_character_audio(
        self, 
        story_parts: Dict[str, List[str]],
        child_name: str,
        child_gender: str = "neutral"
    ) -> str:
        """توليد صوت متعدد الشخصيات للقصة"""
        
        audio_segments = []
        
        # معالجة كل جزء من القصة
        for character_type, texts in story_parts.items():
            # اختيار الصوت المناسب
            voice_profile = self._select_voice(character_type, child_gender)
            
            for text in texts:
                if text.strip():  # تجاهل النصوص الفارغة
                    # توليد الصوت
                    audio_data = await self._generate_voice_segment(
                        text=text,
                        voice_profile=voice_profile,
                        character_type=character_type
                    )
                    
                    audio_segments.append(audio_data)
        
        # دمج جميع المقاطع
        final_audio = self._merge_audio_segments(audio_segments)
        
        # حفظ الملف
        output_path = f"outputs/stories/story_{child_name}_{datetime.now().timestamp()}.mp3"
        final_audio.export(output_path, format="mp3")
        
        return output_path
    
    def _select_voice(self, character_type: str, child_gender: str) -> VoiceProfile:
        """اختيار الصوت المناسب للشخصية"""
        voice_mapping = {
            'narrator': 'narrator_female',
            'hero': 'child_boy',
            'الأم': 'mother'
        }
        
        voice_key = voice_mapping.get(character_type, 'narrator_female')
        return self.voice_profiles.get(voice_key, self.voice_profiles['narrator_female'])
    
    async def _generate_voice_segment(
        self, 
        text: str, 
        voice_profile: VoiceProfile,
        character_type: str
    ) -> AudioSegment:
        """توليد مقطع صوتي واحد"""
        
        # توليد الصوت
        audio = generate(
            text=text,
            voice=Voice(
                voice_id=voice_profile.voice_id,
                settings=voice_profile.settings
            ),
            model="eleven_multilingual_v2"
        )
        
        # تحويل لـ AudioSegment
        audio_segment = AudioSegment.from_mp3(io.BytesIO(audio))
        
        # إضافة فترة صمت قصيرة بين الشخصيات
        silence = AudioSegment.silent(duration=300)  # 300ms
        
        return audio_segment + silence
    
    def _merge_audio_segments(self, segments: List[AudioSegment]) -> AudioSegment:
        """دمج المقاطع الصوتية مع تأثيرات انتقالية"""
        if not segments:
            return AudioSegment.silent(duration=1000)
        
        # البدء بالمقطع الأول
        combined = segments[0]
        
        # دمج باقي المقاطع
        for segment in segments[1:]:
            combined = combined + segment
        
        # تطبيع مستوى الصوت
        combined = combined.normalize()
        
        return combined
    
    async def generate_emotional_response(
        self,
        text: str,
        emotion: str,
        character: str = "teddy"
    ) -> str:
        """توليد رد صوتي عاطفي"""
        
        # تعديل إعدادات الصوت حسب العاطفة
        emotion_settings = {
            'happy': {
                "stability": 0.6,
                "similarity_boost": 0.8,
                "style": 0.7
            },
            'calm': {
                "stability": 0.95,
                "similarity_boost": 0.75,
                "style": 0.2
            }
        }
        
        settings = emotion_settings.get(emotion, emotion_settings['calm'])
        
        # توليد الصوت
        voice_settings = VoiceSettings(**settings, use_speaker_boost=True)
        
        audio = generate(
            text=text,
            voice=Voice(
                voice_id=self.voice_profiles['child_boy'].voice_id,
                settings=voice_settings
            ),
            model="eleven_multilingual_v2"
        )
        
        # حفظ الملف
        output_path = f"outputs/responses/response_{emotion}_{datetime.now().timestamp()}.mp3"
        with open(output_path, 'wb') as f:
            f.write(audio)
        
        return output_path