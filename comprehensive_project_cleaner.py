#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Comprehensive Project Cleaner
ØªÙ†Ø¸ÙŠÙ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
"""

import os
import shutil
import json
import re
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import hashlib

class ComprehensiveProjectCleaner:
    def __init__(self, analysis_file="cleanup_report_20250630_225358.json"):
        """ØªÙ‡ÙŠØ¦Ø© Ù…Ù†Ø¸Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        self.analysis_file = analysis_file
        self.backup_dir = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.actions_log = []
        self.dry_run = True  # Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©
        
        # Ù‚Ø±Ø§Ø¡Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„
        with open(analysis_file, 'r', encoding='utf-8') as f:
            self.analysis = json.load(f)
    
    def create_backup(self):
        """Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ“¦ Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©...")
        if not self.dry_run:
            os.makedirs(self.backup_dir, exist_ok=True)
            
            # Ù†Ø³Ø® Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙ‚Ø·
            important_files = [
                ".py", ".js", ".json", ".yml", ".yaml", 
                ".txt", ".md", ".ini", ".cfg", ".toml"
            ]
            
            for root, dirs, files in os.walk('.'):
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
                dirs[:] = [d for d in dirs if d not in [
                    '.git', '__pycache__', 'node_modules', '.venv', 
                    'venv', 'backup_*'
                ]]
                
                for file in files:
                    if any(file.endswith(ext) for ext in important_files):
                        src_path = os.path.join(root, file)
                        
                        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± ÙÙŠ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
                        rel_path = os.path.relpath(src_path, '.')
                        dst_path = os.path.join(self.backup_dir, rel_path)
                        
                        os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                        shutil.copy2(src_path, dst_path)
            
            print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ: {self.backup_dir}")
        else:
            print("ğŸ” [ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©] Ø³ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©")
    
    def clean_empty_files(self):
        """Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©"""
        print("\nğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©...")
        empty_files = self.analysis.get('empty_files', [])
        
        for file_path in empty_files:
            if os.path.exists(file_path):
                if not self.dry_run:
                    os.remove(file_path)
                    self.actions_log.append(f"Ø­Ø°Ù Ù…Ù„Ù ÙØ§Ø±Øº: {file_path}")
                else:
                    print(f"  ğŸ“„ Ø³ÙŠØªÙ… Ø­Ø°Ù: {file_path}")
        
        print(f"âœ… ØªÙ… Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ {len(empty_files)} Ù…Ù„Ù ÙØ§Ø±Øº")
    
    def merge_duplicate_init_files(self):
        """Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª __init__.py Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        print("\nğŸ”„ Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª __init__.py Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
        
        duplicates = self.analysis.get('duplicate_files', [])
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„ÙØ§Øª __init__.py Ø§Ù„ÙØ§Ø±ØºØ©
        init_groups = [d for d in duplicates if any('__init__.py' in f for f in d['files'])]
        
        for group in init_groups:
            files = group['files']
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ ÙƒÙ„ Ù…Ù„Ù
            file_contents = {}
            for file_path in files:
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read().strip()
                    file_contents[file_path] = content
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ù…ÙŠØ¹Ù‡Ø§ ÙØ§Ø±ØºØ©ØŒ Ø§Ø­ØªÙØ¸ Ø¨Ù‡Ø§ (Ù…Ù‡Ù…Ø© Ù„Ù€ Python packages)
            if all(not content for content in file_contents.values()):
                continue
            
            # Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªÙ„ÙØŒ Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙƒØ«Ø± Ù…Ø­ØªÙˆÙ‰
            if len(set(file_contents.values())) > 1:
                # Ø§Ø®ØªØ± Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø£ÙƒØ«Ø± Ù…Ø­ØªÙˆÙ‰
                best_file = max(file_contents.items(), key=lambda x: len(x[1]))[0]
                
                for file_path, content in file_contents.items():
                    if file_path != best_file and content != file_contents[best_file]:
                        print(f"  âš ï¸ ØªØ­Ø°ÙŠØ±: {file_path} ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªÙ„Ù")
                        self.actions_log.append(f"ØªØ­Ø°ÙŠØ±: Ù…Ø­ØªÙˆÙ‰ Ù…Ø®ØªÙ„Ù ÙÙŠ {file_path}")
    
    def reorganize_misplaced_files(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ø®Ø§Ø·Ø¦Ø©"""
        print("\nğŸ“‚ Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        misplaced = self.analysis.get('misplaced_files', [])
        
        # ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        by_type = defaultdict(list)
        for item in misplaced:
            by_type[item['type']].append(item)
        
        # Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ù†ÙˆØ¹
        for file_type, files in by_type.items():
            print(f"\n  ğŸ“ Ù†Ù‚Ù„ Ù…Ù„ÙØ§Øª {file_type} ({len(files)} Ù…Ù„Ù):")
            
            for item in files[:5]:  # Ø£ÙˆÙ„ 5 ÙÙ‚Ø· Ù„Ù„Ø¹Ø±Ø¶
                current = item['file']
                suggested = item['suggested']
                filename = os.path.basename(current)
                new_path = os.path.join(suggested, filename)
                
                print(f"    {current} â†’ {new_path}")
                
                if not self.dry_run:
                    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù‡Ø¯Ù
                    os.makedirs(suggested, exist_ok=True)
                    
                    # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù
                    if os.path.exists(current):
                        shutil.move(current, new_path)
                        self.actions_log.append(f"Ù†Ù‚Ù„: {current} â†’ {new_path}")
                        
                        # ØªØ­Ø¯ÙŠØ« imports
                        self._update_imports(current, new_path)
    
    def _update_imports(self, old_path, new_path):
        """ØªØ­Ø¯ÙŠØ« imports Ø¨Ø¹Ø¯ Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ module names
        old_module = self._path_to_module(old_path)
        new_module = self._path_to_module(new_path)
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Python
        for root, dirs, files in os.walk('.'):
            dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.venv', 'venv']]
            
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    
                    try:
                        with open(file_path, 'r', encoding='utf-8') as f:
                            content = f.read()
                        
                        # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ imports
                        updated = False
                        patterns = [
                            (f'from {old_module} import', f'from {new_module} import'),
                            (f'import {old_module}', f'import {new_module}'),
                            (f'"{old_module}"', f'"{new_module}"'),
                            (f"'{old_module}'", f"'{new_module}'")
                        ]
                        
                        for old_pattern, new_pattern in patterns:
                            if old_pattern in content:
                                content = content.replace(old_pattern, new_pattern)
                                updated = True
                        
                        if updated and not self.dry_run:
                            with open(file_path, 'w', encoding='utf-8') as f:
                                f.write(content)
                            self.actions_log.append(f"ØªØ­Ø¯ÙŠØ« imports ÙÙŠ: {file_path}")
                    
                    except Exception as e:
                        print(f"    âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« {file_path}: {e}")
    
    def _path_to_module(self, path):
        """ØªØ­ÙˆÙŠÙ„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø§Ø³Ù… module"""
        # Ø¥Ø²Ø§Ù„Ø© .\ Ù…Ù† Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© Ùˆ .py Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
        module = path.replace('.\\', '').replace('/', '.').replace('\\', '.')
        if module.endswith('.py'):
            module = module[:-3]
        return module
    
    def analyze_large_files(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù‚ØªØ±Ø§Ø­ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§"""
        print("\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©...")
        
        large_files = self.analysis.get('large_files', [])
        
        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        categories = {
            'split_recommended': [],  # ÙŠÙÙ†ØµØ­ Ø¨ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§
            'ok_as_is': [],          # Ù…Ù‚Ø¨ÙˆÙ„Ø© ÙƒÙ…Ø§ Ù‡ÙŠ
            'needs_refactor': []     # ØªØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø©
        }
        
        for file_info in large_files:
            file_path = file_info['path']
            lines = file_info['lines']
            
            # ØªØ¬Ø§Ù‡Ù„ Ù…Ù„ÙØ§Øª Ù…Ø¹ÙŠÙ†Ø©
            if any(skip in file_path for skip in ['package-lock.json', '.json', 'test_', 'migrations']):
                categories['ok_as_is'].append(file_info)
            elif lines > 1000:
                categories['split_recommended'].append(file_info)
            elif lines > 700:
                categories['needs_refactor'].append(file_info)
            else:
                categories['ok_as_is'].append(file_info)
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        if categories['split_recommended']:
            print(f"\n  ğŸ”´ Ù…Ù„ÙØ§Øª ÙŠØ¬Ø¨ ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ ({len(categories['split_recommended'])}):")
            for file_info in categories['split_recommended'][:5]:
                print(f"    - {file_info['path']} ({file_info['lines']} Ø³Ø·Ø±)")
        
        if categories['needs_refactor']:
            print(f"\n  ğŸŸ  Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© ({len(categories['needs_refactor'])}):")
            for file_info in categories['needs_refactor'][:5]:
                print(f"    - {file_info['path']} ({file_info['lines']} Ø³Ø·Ø±)")
        
        return categories
    
    def generate_cleanup_script(self):
        """ØªÙˆÙ„ÙŠØ¯ Ø³ÙƒØ±ÙŠØ¨Øª Ù„Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙŠØ¯ÙˆÙŠ"""
        print("\nğŸ“ ØªÙˆÙ„ÙŠØ¯ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ...")
        
        date_str = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        empty_cmds = '\n'.join(f'rm -f "{f}"' for f in self.analysis.get('empty_files', []))
        
        script_content = f"""#!/bin/bash
# Ø³ÙƒØ±ÙŠØ¨Øª ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ - ØªÙ… ØªÙˆÙ„ÙŠØ¯Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
# ØªØ§Ø±ÙŠØ®: {date_str}

echo "ğŸš€ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹..."

# Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
echo "ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©..."
{empty_cmds}

# Ø­Ø°Ù Ù…Ø¬Ù„Ø¯Ø§Øª __pycache__
echo "ğŸ—‘ï¸ Ø­Ø°Ù Ù…Ø¬Ù„Ø¯Ø§Øª __pycache__..."
find . -type d -name "__pycache__" -exec rm -rf {{}} + 2>/dev/null

# ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙƒÙˆØ¯
echo "ğŸ¨ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙƒÙˆØ¯..."
black src/ --line-length 120 2>/dev/null || echo "ØªØ­Ø°ÙŠØ±: black ØºÙŠØ± Ù…Ø«Ø¨Øª"
isort src/ 2>/dev/null || echo "ØªØ­Ø°ÙŠØ±: isort ØºÙŠØ± Ù…Ø«Ø¨Øª"

echo "âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ!"
"""
        
        with open('cleanup_script.sh', 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ cleanup_script.sh")
    
    def generate_final_report(self):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        report_content = f"""# ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ

## ğŸ“… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ
- **Ø§Ù„ØªØ§Ø±ÙŠØ®**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **ÙˆØ¶Ø¹ Ø§Ù„ØªØ´ØºÙŠÙ„**: {'Ù…Ø¹Ø§ÙŠÙ†Ø©' if self.dry_run else 'ØªÙ†ÙÙŠØ° ÙØ¹Ù„ÙŠ'}

## ğŸ“ˆ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ®Ø°Ø©
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª**: {len(self.actions_log)}

### ğŸ“‹ Ø³Ø¬Ù„ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª:
"""
        
        for i, action in enumerate(self.actions_log[:50], 1):
            report_content += f"{i}. {action}\n"
        
        if len(self.actions_log) > 50:
            report_content += f"\n... Ùˆ {len(self.actions_log) - 50} Ø¥Ø¬Ø±Ø§Ø¡ Ø¢Ø®Ø±\n"
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_file = f"cleanup_final_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ: {report_file}")
    
    def run_cleanup(self, dry_run=True):
        """ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
        self.dry_run = dry_run
        
        print(f"ğŸš€ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ({'ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø¹Ø§ÙŠÙ†Ø©' if dry_run else 'Ø§Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ÙØ¹Ù„ÙŠ'})...")
        print("=" * 60)
        
        # 1. Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        if not dry_run:
            self.create_backup()
        
        # 2. Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        self.clean_empty_files()
        
        # 3. Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        self.merge_duplicate_init_files()
        
        # 4. Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª
        self.reorganize_misplaced_files()
        
        # 5. ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        self.analyze_large_files()
        
        # 6. ØªÙˆÙ„ÙŠØ¯ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ
        self.generate_cleanup_script()
        
        # 7. Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        self.generate_final_report()
        
        print("\n" + "=" * 60)
        print("âœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ!")
        
        if dry_run:
            print("\nğŸ’¡ Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ¹Ù„ÙŠØŒ Ø´ØºÙ„:")
            print("   python comprehensive_project_cleaner.py --execute")

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ØªÙ†Ø¸ÙŠÙ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹')
    parser.add_argument('--execute', action='store_true', 
                       help='ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙØ¹Ù„ÙŠ (Ø§ÙØªØ±Ø§Ø¶ÙŠØ§Ù‹: Ù…Ø¹Ø§ÙŠÙ†Ø© ÙÙ‚Ø·)')
    parser.add_argument('--analysis', default='cleanup_report_20250630_225358.json',
                       help='Ù…Ù„Ù ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„')
    
    args = parser.parse_args()
    
    try:
        cleaner = ComprehensiveProjectCleaner(args.analysis)
        cleaner.run_cleanup(dry_run=not args.execute)
    except FileNotFoundError:
        print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„Ù Ø§Ù„ØªØ­Ù„ÙŠÙ„. Ø´ØºÙ„ project_cleanup_analyzer.py Ø£ÙˆÙ„Ø§Ù‹")
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 