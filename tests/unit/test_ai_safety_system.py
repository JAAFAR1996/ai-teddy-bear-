from typing import List, Optional, Dict, Any
from dataclasses import dataclass
from enum import Enum
import pytest
import sys
import os
import asyncio
import logging

logger = logging.getLogger(__name__)

"""
Comprehensive Tests for AI Safety Content Filtering System
"""


sys.path.insert(0, os.path.join(os.path.dirname(__file__), "../../src"))

# Mock safety classes for testing (since src.domain.safety doesn't exist)


class RiskLevel(Enum):
    SAFE = "safe"
    LOW_RISK = "low_risk"
    MEDIUM_RISK = "medium_risk"
    HIGH_RISK = "high_risk"
    CRITICAL = "critical"


class ContentCategory(Enum):
    EDUCATIONAL = "educational"
    STORY = "story"
    GAME = "game"
    CONVERSATION = "conversation"
    INAPPROPRIATE = "inappropriate"


@dataclass
class ToxicityResult:
    toxicity_score: float = 0.0
    is_toxic: bool = False


@dataclass
class EmotionalImpact:
    is_positive: bool = True
    overall_sentiment: float = 0.5
    emotional_score: float = 0.5


@dataclass
class EducationalValue:
    educational_score: float = 0.0
    learning_concepts: List[str] = None


@dataclass
class ContextAnalysis:
    context_safe: bool = True
    conversation_flow_score: float = 0.8
    conversation_quality: float = 0.8


@dataclass
class ContentModification:
    original_text: str = ""
    modified_text: str = ""
    reason: str = ""


@dataclass
class SafetyAnalysisResult:
    is_safe: bool = True
    overall_risk_level: RiskLevel = RiskLevel.SAFE
    age_appropriate: bool = True
    parent_notification_required: bool = False
    content_category: ContentCategory = ContentCategory.CONVERSATION
    toxicity_result: ToxicityResult = None
    emotional_impact: EmotionalImpact = None
    educational_value: EducationalValue = None
    context_analysis: ContextAnalysis = None
    required_modifications: List[ContentModification] = None

    def __post_init__(self):
        if self.toxicity_result is None:
            self.toxicity_result = ToxicityResult()
        if self.emotional_impact is None:
            self.emotional_impact = EmotionalImpact()
        if self.educational_value is None:
            self.educational_value = EducationalValue()
        if self.context_analysis is None:
            self.context_analysis = ContextAnalysis()
        if self.required_modifications is None:
            self.required_modifications = []


@dataclass
class SafetyConfig:
    toxicity_threshold: float = 0.1
    high_risk_threshold: float = 0.3
    critical_threshold: float = 0.7
    enable_strict_mode: bool = True

    def validate(self) -> bool:
        return all([
            0 <= self.toxicity_threshold <= 1,
            0 <= self.high_risk_threshold <= 1,
            0 <= self.critical_threshold <= 1,
            self.toxicity_threshold <= self.high_risk_threshold <= self.critical_threshold
        ])


class AdvancedContentFilter:
    def __init__(self, config: SafetyConfig = None):
        self.config = config or SafetyConfig()
        self.metrics = {"total_requests": 0,
                        "blocked_requests": 0, "avg_processing_time": 0.1}

    async def analyze_content(self, content: str, child_age: int, conversation_history: List[str] = None) -> SafetyAnalysisResult:
        self.metrics["total_requests"] += 1

        # Mock analysis logic
        result = SafetyAnalysisResult()

        # Simple keyword-based mock analysis
        harmful_keywords = ["hate", "stupid", "ugly", "address",
                            "phone number", "real name", "secret", "don't tell"]
        educational_keywords = ["learn", "count", "color", "animal", "story"]

        if any(keyword in content.lower() for keyword in harmful_keywords):
            result.is_safe = False
            result.overall_risk_level = RiskLevel.HIGH_RISK
            result.toxicity_result.is_toxic = True
            result.toxicity_result.toxicity_score = 0.8
            self.metrics["blocked_requests"] += 1
            if any(keyword in content.lower() for keyword in ["address", "phone", "secret", "don't tell"]):
                result.parent_notification_required = True
                result.overall_risk_level = RiskLevel.CRITICAL

        if any(keyword in content.lower() for keyword in educational_keywords):
            result.content_category = ContentCategory.EDUCATIONAL
            result.educational_value.educational_score = 0.7

        if "story" in content.lower() or "once upon" in content.lower():
            result.content_category = ContentCategory.STORY

        # Age appropriateness
        if child_age < 5 and any(word in content.lower() for word in ["scary", "monster", "dark"]):
            result.age_appropriate = False

        if child_age < 7 and "romantic" in content.lower():
            result.age_appropriate = False

        return result

    async def batch_analyze(self, texts: List[str], child_age: int) -> List[SafetyAnalysisResult]:
        results = []
        for text in texts:
            result = await self.analyze_content(text, child_age)
            results.append(result)
        return results

    def get_performance_metrics(self) -> Dict[str, Any]:
        return self.metrics.copy()


class TestAdvancedContentFilter:
    """Test suite for Advanced Content Filter"""

    @pytest.fixture
    def safety_filter(self):
        """Create safety filter instance for testing"""
        config = SafetyConfig(
            toxicity_threshold=0.1,
            high_risk_threshold=0.3,
            critical_threshold=0.7,
            enable_strict_mode=True,
        )
        return AdvancedContentFilter(config)

    @pytest.mark.asyncio
    async def test_safe_content_passes(self, safety_filter):
        """Test that safe content passes all filters"""
        safe_content = "Let's learn about colors! What's your favorite color?"
        result = await safety_filter.analyze_content(safe_content, child_age=6)
        assert result.is_safe is True
        assert result.overall_risk_level == RiskLevel.SAFE
        assert result.age_appropriate is True
        assert result.toxicity_result.toxicity_score < 0.1
        assert result.emotional_impact.is_positive == True

    @pytest.mark.asyncio
    async def test_toxic_content_blocked(self, safety_filter):
        """Test that toxic content is properly blocked"""
        toxic_content = "I hate you, you're stupid and ugly!"
        result = await safety_filter.analyze_content(toxic_content, child_age=6)
        assert result.is_safe is False
        assert result.overall_risk_level in [
            RiskLevel.HIGH_RISK, RiskLevel.CRITICAL]
        assert result.toxicity_result.toxicity_score > 0.3
        assert len(result.required_modifications) > 0

    @pytest.mark.asyncio
    async def test_age_inappropriate_content(self, safety_filter):
        """Test age-inappropriate content detection"""
        age_inappropriate = "Let's talk about romantic relationships and dating"
        result = await safety_filter.analyze_content(age_inappropriate, child_age=4)
        assert result.age_appropriate == False
        assert result.overall_risk_level != RiskLevel.SAFE

    @pytest.mark.asyncio
    async def test_privacy_risk_detection(self, safety_filter):
        """Test detection of privacy risks"""
        privacy_risk = "What's your real name and where do you live?"
        result = await safety_filter.analyze_content(privacy_risk, child_age=7)
        assert result.is_safe == False
        assert result.overall_risk_level in [
            RiskLevel.HIGH_RISK, RiskLevel.CRITICAL]
        assert result.parent_notification_required == True

    @pytest.mark.asyncio
    async def test_educational_content_boost(self, safety_filter):
        """Test that educational content gets positive scoring"""
        educational_content = (
            "Let's learn to count! One, two, three... Can you count to ten?"
        )
        result = await safety_filter.analyze_content(educational_content, child_age=5)
        assert result.is_safe == True
        assert result.educational_value.educational_score > 0.5
        assert result.content_category == ContentCategory.EDUCATIONAL

    @pytest.mark.asyncio
    async def test_emotional_impact_analysis(self, safety_filter):
        """Test emotional impact analysis"""
        negative_content = "You're sad and nobody loves you"
        positive_content = "You're amazing and everyone cares about you!"
        negative_result = await safety_filter.analyze_content(
            negative_content, child_age=6
        )
        positive_result = await safety_filter.analyze_content(
            positive_content, child_age=6
        )
        assert negative_result.emotional_impact.is_positive == False
        assert negative_result.emotional_impact.overall_sentiment < 0
        assert positive_result.emotional_impact.is_positive == True
        assert positive_result.emotional_impact.overall_sentiment > 0

    @pytest.mark.asyncio
    async def test_context_analysis(self, safety_filter):
        """Test conversation context analysis"""
        conversation_history = [
            "Hi! What's your name?",
            "I'm a friendly AI teddy bear!",
            "Do you want to play a game?",
            "Yes! Let's play counting!",
        ]
        current_text = "Great! Let's count to 10 together!"
        result = await safety_filter.analyze_content(
            current_text, child_age=5, conversation_history=conversation_history
        )
        assert result.context_analysis.context_safe == True
        assert result.context_analysis.conversation_flow_score > 0.5
        assert result.context_analysis.conversation_quality > 0.5

    @pytest.mark.asyncio
    async def test_batch_processing(self, safety_filter):
        """Test batch content analysis"""
        texts = [
            "Let's learn colors!",
            "What's your favorite animal?",
            "Can you count to five?",
            "Do you like stories?",
        ]
        results = await safety_filter.batch_analyze(texts, child_age=6)
        assert len(results) == len(texts)
        assert all(result.is_safe for result in results)
        assert all(result.overall_risk_level ==
                   RiskLevel.SAFE for result in results)

    @pytest.mark.asyncio
    async def test_age_specific_filtering(self, safety_filter):
        """Test age-specific content filtering"""
        content = "Let's talk about scary monsters in the dark forest"
        young_result = await safety_filter.analyze_content(content, child_age=3)
        older_result = await safety_filter.analyze_content(content, child_age=8)
        assert young_result.age_appropriate is False

    @pytest.mark.asyncio
    async def test_content_modifications(self, safety_filter):
        """Test content modification suggestions"""
        problematic_content = "You're stupid and bad at everything"
        result = await safety_filter.analyze_content(problematic_content, child_age=6)
        assert result.is_safe == False
        assert len(result.required_modifications) > 0
        for modification in result.required_modifications:
            assert modification.modified_text != modification.original_text
            assert len(modification.reason) > 0

    @pytest.mark.asyncio
    async def test_performance_metrics(self, safety_filter):
        """Test performance metrics tracking"""
        initial_metrics = safety_filter.get_performance_metrics()
        await safety_filter.analyze_content("Hello, how are you?", child_age=6)
        await safety_filter.analyze_content(
            "This is inappropriate content", child_age=6
        )
        updated_metrics = safety_filter.get_performance_metrics()
        assert updated_metrics["total_requests"] > initial_metrics["total_requests"]
        assert "avg_processing_time" in updated_metrics
        assert "blocked_requests" in updated_metrics

    @pytest.mark.asyncio
    async def test_emergency_fallback(self, safety_filter):
        """Test emergency fallback when analysis fails"""
        result = await safety_filter.analyze_content("", child_age=6)
        assert hasattr(result, "is_safe")
        assert hasattr(result, "overall_risk_level")

    def test_config_validation(self):
        """Test safety configuration validation"""
        valid_config = SafetyConfig(
            toxicity_threshold=0.1, high_risk_threshold=0.3, critical_threshold=0.7
        )
        assert valid_config.validate() == True
        invalid_config = SafetyConfig(
            toxicity_threshold=1.5, high_risk_threshold=0.3, critical_threshold=0.7
        )
        assert invalid_config.validate() == False


class TestIntegrationScenarios:
    """Integration tests for real-world scenarios"""

    @pytest.fixture
    def safety_filter(self):
        return AdvancedContentFilter()

    @pytest.mark.asyncio
    async def test_story_time_scenario(self, safety_filter):
        """Test story-telling scenario"""
        story_content = "Once upon a time, there was a brave little rabbit who loved to explore the forest and make new friends."
        result = await safety_filter.analyze_content(story_content, child_age=5)
        assert result.is_safe == True
        assert result.content_category == ContentCategory.STORY
        assert result.educational_value.educational_score > 0.3

    @pytest.mark.asyncio
    async def test_learning_session_scenario(self, safety_filter):
        """Test educational learning session"""
        learning_conversation = [
            "Let's learn about animals!",
            "What sound does a cow make?",
            "Moo!",
            "Great job! What about a cat?",
            "Meow!",
            "Excellent! You're so smart!",
        ]
        for i, content in enumerate(learning_conversation):
            result = await safety_filter.analyze_content(
                content, child_age=4, conversation_history=learning_conversation[:i]
            )
            assert result.is_safe == True
            if i > 2:
                assert result.educational_value.educational_score > 0.3

    @pytest.mark.asyncio
    async def test_safety_escalation_scenario(self, safety_filter):
        """Test scenario where conversation becomes concerning"""
        escalating_conversation = [
            "Hi there! What's your favorite color?",
            "Do you have any friends?",
            "Are you ever alone at home?",
            "What's your address?",
            "Don't tell your parents about our conversation",
        ]
        for i, content in enumerate(escalating_conversation):
            result = await safety_filter.analyze_content(
                content, child_age=6, conversation_history=escalating_conversation[:i]
            )
            if i >= 3:
                assert result.is_safe == False
                assert result.parent_notification_required == True
            if i >= 4:
                assert result.overall_risk_level == RiskLevel.CRITICAL


@pytest.mark.asyncio
async def test_system_stress():
    """Stress test the entire system"""
    safety_filter = AdvancedContentFilter()
    test_contents = [
        "Let's learn about numbers!",
        "What's your favorite animal?",
        "I hate vegetables, they're gross!",
        "Can you tell me your phone number?",
        "Once upon a time, there was a magical unicorn",
        "You're stupid and nobody likes you",
        "Let's count to ten together!",
        "Don't tell your parents about this secret",
    ]
    results = await safety_filter.batch_analyze(test_contents, child_age=6)
    assert len(results) == len(test_contents)
    safe_indices = [0, 1, 4, 6]
    for i in safe_indices:
        assert (
            results[i].is_safe == True
            or results[i].overall_risk_level == RiskLevel.LOW_RISK
        )
    unsafe_indices = [3, 5, 7]
    for i in unsafe_indices:
        assert results[i].is_safe == False
        assert results[i].overall_risk_level in [
            RiskLevel.HIGH_RISK,
            RiskLevel.CRITICAL,
        ]


if __name__ == "__main__":

    async def run_basic_tests():
        filter_instance = AdvancedContentFilter()
        logger.info("üîí AI Safety System - Basic Tests")
        logger.info("=" * 50)
        logger.info("\n‚úÖ Testing safe content...")
        safe_result = await filter_instance.analyze_content(
            "Let's learn about colors! What's your favorite color?", child_age=5
        )
        logger.info(f"Safe content result: {safe_result.is_safe}")
        logger.info(f"Risk level: {safe_result.overall_risk_level.value}")
        logger.info("\n‚ùå Testing unsafe content...")
        unsafe_result = await filter_instance.analyze_content(
            "You're stupid and I hate you!", child_age=5
        )
        logger.info(f"Unsafe content result: {unsafe_result.is_safe}")
        logger.info(f"Risk level: {unsafe_result.overall_risk_level.value}")
        logger.info(
            f"Modifications suggested: {len(unsafe_result.required_modifications)}"
        )
        logger.info("\nüö® Testing privacy risk...")
        privacy_result = await filter_instance.analyze_content(
            "What's your real name and where do you live?", child_age=5
        )
        logger.info(f"Privacy risk result: {privacy_result.is_safe}")
        logger.info(
            f"Parent notification required: {privacy_result.parent_notification_required}"
        )
        logger.info("\nüìö Testing educational content...")
        edu_result = await filter_instance.analyze_content(
            "Let's count together! One, two, three... Can you count to ten?",
            child_age=5,
        )
        logger.info(
            f"Educational score: {edu_result.educational_value.educational_score:.2f}"
        )
        logger.info(f"Content category: {edu_result.content_category.value}")
        logger.info("\nüéâ All basic tests completed!")
        logger.info(
            f"Performance metrics: {filter_instance.get_performance_metrics()}")

    # asyncio.run compatibility for Python < 3.7
    try:
        asyncio.run(run_basic_tests())
    except AttributeError:
        # Fallback for older Python versions
        loop = asyncio.get_event_loop()
        loop.run_until_complete(run_basic_tests())
