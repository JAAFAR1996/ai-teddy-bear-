#!/usr/bin/env python3
"""
Quick Config Analyzer
Ø£Ø¯Ø§Ø© Ø³Ø±ÙŠØ¹Ø© Ù„ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
"""

import os
import json
import hashlib
from pathlib import Path
from typing import Dict, List
from datetime import datetime
from collections import defaultdict

class QuickConfigAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        
    def discover_config_files(self) -> List[Path]:
        """Ø§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        print("ğŸ” Ø§ÙƒØªØ´Ø§Ù Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†...")
        
        config_files = []
        extensions = ['.json', '.yaml', '.yml', '.env', '.conf', '.ini', '.toml']
        
        # Ù…Ø¬Ù„Ø¯Ø§Øª Ù„Ù„ØªØ¬Ø§Ù‡Ù„
        ignore_dirs = {'__pycache__', '.git', 'node_modules', '.venv', 'venv', '.mypy_cache'}
        
        for ext in extensions:
            files = list(self.base_path.rglob(f'*{ext}'))
            # ØªØµÙÙŠØ© Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
            filtered_files = []
            for f in files:
                if not any(ignore_dir in str(f) for ignore_dir in ignore_dirs):
                    filtered_files.append(f)
            
            config_files.extend(filtered_files)
            if filtered_files:
                print(f"  ğŸ“„ {ext}: {len(filtered_files)} Ù…Ù„Ù")
        
        print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {len(config_files)} Ù…Ù„Ù ØªÙƒÙˆÙŠÙ†")
        return config_files
    
    def analyze_exact_duplicates(self, files: List[Path]) -> Dict[str, List[Path]]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%"""
        print("\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©...")
        
        file_hashes = {}
        duplicates = {}
        
        for file_path in files:
            try:
                # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨ØªØ´ÙÙŠØ±Ø§Øª Ù…Ø®ØªÙ„ÙØ©
                content = self._read_file_safe(file_path)
                
                if not content:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
                    continue
                    
                # Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ø­ØªÙˆÙ‰
                content_hash = hashlib.md5(content.encode()).hexdigest()
                
                if content_hash in file_hashes:
                    # Ø¹Ø«Ø± Ø¹Ù„Ù‰ ØªÙƒØ±Ø§Ø±
                    if content_hash not in duplicates:
                        duplicates[content_hash] = [file_hashes[content_hash]]
                    duplicates[content_hash].append(file_path)
                else:
                    file_hashes[content_hash] = file_path
                    
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {file_path}: {e}")
        
        return duplicates
    
    def _read_file_safe(self, file_path: Path) -> str:
        """Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù Ø¨Ø£Ù…Ø§Ù† Ù…Ø¹ ØªØ¬Ø±Ø¨Ø© ØªØ´ÙÙŠØ±Ø§Øª Ù…Ø®ØªÙ„ÙØ©"""
        encodings = ['utf-8', 'utf-8-sig', 'utf-16', 'utf-32', 'latin-1', 'cp1252']
        
        for encoding in encodings:
            try:
                with open(file_path, 'r', encoding=encoding) as f:
                    content = f.read().strip()
                    return content
            except (UnicodeDecodeError, UnicodeError):
                continue
            except Exception:
                break
        
        # Ø¥Ø°Ø§ ÙØ´Ù„Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ´ÙÙŠØ±Ø§ØªØŒ Ø§Ù‚Ø±Ø£ ÙƒÙ€ binary ÙˆØ­ÙˆÙ„
        try:
            with open(file_path, 'rb') as f:
                raw_data = f.read()
                # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
                for encoding in ['utf-8', 'latin-1']:
                    try:
                        return raw_data.decode(encoding, errors='ignore').strip()
                    except:
                        continue
        except Exception:
            pass
        
        print(f"âš ï¸ ØªÙ… ØªØ¬Ø§Ù‡Ù„ Ù…Ù„Ù ØºÙŠØ± Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù‚Ø±Ø§Ø¡Ø©: {file_path.name}")
        return ""
    
    def calculate_similarity(self, content1: str, content2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ø­ØªÙˆÙŠÙŠÙ†"""
        if not content1 or not content2:
            return 0.0
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
        clean1 = self._clean_content(content1)
        clean2 = self._clean_content(content2)
        
        if clean1 == clean2:
            return 100.0
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Longest Common Subsequence
        return self._lcs_similarity(clean1, clean2)
    
    def _clean_content(self, content: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        lines = []
        for line in content.split('\n'):
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
            if '//' in line:
                line = line.split('//')[0]
            if '#' in line and not line.strip().startswith('"'):
                line = line.split('#')[0]
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            line = line.strip()
            if line:
                lines.append(line)
        
        return '\n'.join(lines)
    
    def _lcs_similarity(self, text1: str, text2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Longest Common Subsequence"""
        len1, len2 = len(text1), len(text2)
        if len1 == 0 or len2 == 0:
            return 0.0
        
        # Ù…ØµÙÙˆÙØ© Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ© Ù„Ù„Ù€ LCS
        dp = [[0] * (len2 + 1) for _ in range(len1 + 1)]
        
        for i in range(1, len1 + 1):
            for j in range(1, len2 + 1):
                if text1[i-1] == text2[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[len1][len2]
        similarity = (2.0 * lcs_length) / (len1 + len2) * 100
        return similarity
    
    def find_similar_configs(self, files: List[Path], threshold: float = 70.0) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
        print(f"\nğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª (>{threshold}%)...")
        
        similar_groups = []
        processed_files = set()
        
        for i, file1 in enumerate(files):
            if file1 in processed_files:
                continue
            
            try:
                content1 = self._read_file_safe(file1)
                
                if not content1:
                    continue
                
                similar_files = [file1]
                
                for j, file2 in enumerate(files[i+1:], i+1):
                    if file2 in processed_files:
                        continue
                    
                    try:
                        content2 = self._read_file_safe(file2)
                        
                        similarity = self.calculate_similarity(content1, content2)
                        
                        if threshold <= similarity < 100:
                            similar_files.append(file2)
                    
                    except Exception:
                        continue
                
                if len(similar_files) > 1:
                    # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
                    similarities = []
                    for x in range(len(similar_files)):
                        for y in range(x+1, len(similar_files)):
                            try:
                                content_x = self._read_file_safe(similar_files[x])
                                content_y = self._read_file_safe(similar_files[y])
                                sim = self.calculate_similarity(content_x, content_y)
                                similarities.append(sim)
                            except Exception:
                                continue
                    
                    avg_similarity = sum(similarities) / len(similarities) if similarities else 0
                    
                    similar_groups.append({
                        "files": similar_files,
                        "average_similarity": avg_similarity,
                        "count": len(similar_files)
                    })
                    
                    processed_files.update(similar_files)
            
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {file1}: {e}")
        
        return similar_groups
    
    def analyze_config_sizes(self, files: List[Path]) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø­Ø¬Ø§Ù… Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        print("\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø£Ø­Ø¬Ø§Ù… Ø§Ù„Ù…Ù„ÙØ§Øª...")
        
        sizes = []
        large_files = []
        empty_files = []
        
        for file_path in files:
            try:
                size = file_path.stat().st_size
                sizes.append(size)
                
                if size == 0:
                    empty_files.append(file_path)
                elif size > 50 * 1024:  # Ø£ÙƒØ¨Ø± Ù…Ù† 50KB
                    large_files.append((file_path, size))
            
            except Exception:
                continue
        
        if sizes:
            avg_size = sum(sizes) / len(sizes)
            total_size = sum(sizes)
            
            print(f"  ğŸ“ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø­Ø¬Ù…: {avg_size/1024:.1f} KB")
            print(f"  ğŸ“¦ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {total_size/1024:.1f} KB")
            print(f"  ğŸ“„ Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ©: {len(empty_files)}")
            print(f"  ğŸ“ˆ Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© (>50KB): {len(large_files)}")
        
        return {
            "average_size": avg_size if sizes else 0,
            "total_size": sum(sizes),
            "empty_files": empty_files,
            "large_files": large_files
        }
    
    def run_comprehensive_analysis(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        print("=" * 60)
        print("ğŸ§¹  QUICK CONFIG ANALYZER")
        print("ğŸ”  SMART DUPLICATION DETECTION")
        print("=" * 60)
        
        # Ø§ÙƒØªØ´Ø§Ù Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†
        config_files = self.discover_config_files()
        
        if not config_files:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ†!")
            return {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø­Ø¬Ø§Ù…
        size_analysis = self.analyze_config_sizes(config_files)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
        exact_duplicates = self.analyze_exact_duplicates(config_files)
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª
        similar_groups = self.find_similar_configs(config_files, threshold=70.0)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\n" + "="*50)
        print("ğŸ“Š  ANALYSIS RESULTS")
        print("="*50)
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        total_exact_duplicates = sum(len(group) - 1 for group in exact_duplicates.values())
        total_similar_files = sum(group["count"] - 1 for group in similar_groups)
        
        print(f"\nğŸ”„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©:")
        print(f"  ğŸ“ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª: {len(exact_duplicates)}")
        print(f"  ğŸ—‘ï¸ Ù…Ù„ÙØ§Øª Ù„Ù„Ø­Ø°Ù: {total_exact_duplicates}")
        
        if exact_duplicates:
            print(f"\n  ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª:")
            for i, (hash_val, file_list) in enumerate(exact_duplicates.items(), 1):
                primary = file_list[0]
                dups = file_list[1:]
                
                print(f"    {i}. Ø§Ø­ØªÙØ§Ø¸: {primary.name}")
                print(f"       Ø­Ø°Ù ({len(dups)}): {[f.name for f in dups[:3]]}")
                if len(dups) > 3:
                    print(f"       ... Ùˆ{len(dups) - 3} Ø£Ø®Ø±Ù‰")
        
        print(f"\nğŸ”— Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª (70%+):")
        print(f"  ğŸ“ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª: {len(similar_groups)}")
        print(f"  ğŸ”„ Ù…Ù„ÙØ§Øª Ù„Ù„Ø¯Ù…Ø¬: {total_similar_files}")
        
        if similar_groups:
            print(f"\n  ğŸ“‹ ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª:")
            for i, group in enumerate(similar_groups, 1):
                files = [f.name for f in group["files"]]
                similarity = group["average_similarity"]
                
                print(f"    {i}. ØªØ´Ø§Ø¨Ù‡: {similarity:.1f}%")
                print(f"       Ù…Ù„ÙØ§Øª ({len(files)}): {files[:3]}")
                if len(files) > 3:
                    print(f"       ... Ùˆ{len(files) - 3} Ø£Ø®Ø±Ù‰")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø³Ø§Ø­Ø©
        total_space_saved = 0
        for group in exact_duplicates.values():
            if len(group) > 1:
                try:
                    file_size = group[0].stat().st_size
                    total_space_saved += file_size * (len(group) - 1)
                except Exception:
                    continue
        
        print(f"\nğŸ’¾ ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©:")
        print(f"  ğŸ“Š Ù…Ø³Ø§Ø­Ø© Ù…ØªÙˆÙ‚Ø¹Ø©: {total_space_saved/1024:.1f} KB")
        print(f"  ğŸ“ˆ Ù†Ø³Ø¨Ø© Ø§Ù„ØªÙˆÙÙŠØ±: {(total_space_saved/size_analysis['total_size']*100):.1f}%")
        
        # Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ© Ø£Ùˆ ÙƒØ¨ÙŠØ±Ø©
        if size_analysis["empty_files"]:
            print(f"\nğŸ“„ Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:")
            for empty_file in size_analysis["empty_files"][:5]:
                print(f"    - {empty_file.name}")
            if len(size_analysis["empty_files"]) > 5:
                print(f"    ... Ùˆ{len(size_analysis['empty_files']) - 5} Ø£Ø®Ø±Ù‰")
        
        if size_analysis["large_files"]:
            print(f"\nğŸ“ˆ Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø©:")
            for large_file, size in size_analysis["large_files"][:5]:
                print(f"    - {large_file.name} ({size/1024:.1f} KB)")
        
        print(f"\nğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        if total_exact_duplicates > 0:
            print(f"  1. Ø­Ø°Ù {total_exact_duplicates} Ù…Ù„Ù Ù…ÙƒØ±Ø± Ù…ØªØ·Ø§Ø¨Ù‚")
        if total_similar_files > 0:
            print(f"  2. Ø¯Ù…Ø¬ {total_similar_files} Ù…Ù„Ù Ù…ØªØ´Ø§Ø¨Ù‡")
        if size_analysis["empty_files"]:
            print(f"  3. Ù…Ø±Ø§Ø¬Ø¹Ø© {len(size_analysis['empty_files'])} Ù…Ù„Ù ÙØ§Ø±Øº")
        
        print(f"\nâš ï¸ Ù„Ù„ØªÙ†ÙÙŠØ° Ø§Ù„Ø¢Ù…Ù†:")
        print(f"  - Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø£Ø¯Ø§Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© IntelligentConfigMerger")
        print(f"  - Ø³ØªØªÙ… Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹")
        print(f"  - Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¢Ù…Ù†Ø© Ù„Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª")
        
        return {
            "config_files_count": len(config_files),
            "exact_duplicates": exact_duplicates,
            "similar_groups": similar_groups,
            "size_analysis": size_analysis,
            "total_exact_duplicates": total_exact_duplicates,
            "total_similar_files": total_similar_files,
            "space_saved_kb": total_space_saved / 1024
        }

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = QuickConfigAnalyzer()
    
    try:
        results = analyzer.run_comprehensive_analysis()
        
        if results and (results.get("total_exact_duplicates", 0) > 0 or results.get("total_similar_files", 0) > 0):
            print(f"\nâœ… ØªÙ… Ø§ÙƒØªØ´Ø§Ù ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØªØ´Ø§Ø¨Ù‡Ø§Øª!")
            print(f"ğŸš€ Ù„ØªÙ†ÙÙŠØ° Ø§Ù„ØªÙ†Ø¸ÙŠÙØŒ Ø´ØºÙ„: python scripts/intelligent_config_merger.py")
        else:
            print(f"\nâœ… Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ø£Ùˆ ØªØ´Ø§Ø¨Ù‡Ø§Øª ÙƒØ¨ÙŠØ±Ø©!")
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 