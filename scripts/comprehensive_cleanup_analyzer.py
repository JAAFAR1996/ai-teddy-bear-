#!/usr/bin/env python3
"""
ŸÖÿ≠ŸÑŸÑ ÿ¥ÿßŸÖŸÑ ŸÑÿ™ŸÜÿ∏ŸäŸÅ Ÿàÿ™ÿ±ÿ™Ÿäÿ® ŸÖÿ¥ÿ±Ÿàÿπ AI Teddy Bear
ŸäŸÇŸàŸÖ ÿ®ÿ™ÿ≠ŸÑŸäŸÑ 400+ ŸÖŸÑŸÅ Ÿàÿ™ÿµŸÜŸäŸÅŸáÿß ŸàÿßŸÉÿ™ÿ¥ÿßŸÅ ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ŸàÿßŸÑÿ™ŸàÿµŸäÿßÿ™
"""

import os
import sys
import json
import hashlib
import ast
import re
from pathlib import Path
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Set
import shutil

# ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑŸÖÿ≥ÿßÿ± ŸÑŸÑŸÖÿ¥ÿ±Ÿàÿπ
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class ComprehensiveCleanupAnalyzer:
    """ŸÖÿ≠ŸÑŸÑ ÿ¥ÿßŸÖŸÑ ŸÑŸÑÿ™ŸÜÿ∏ŸäŸÅ ŸàÿßŸÑÿ™ÿ±ÿ™Ÿäÿ®"""
    
    def __init__(self, project_root: str = '.'):
        self.project_root = Path(project_root).resolve()
        self.results = self._initialize_results()
        
        # ŸÖÿπÿßŸäŸäÿ± ÿßŸÑÿ™ÿµŸÜŸäŸÅ
        self.critical_patterns = [
            'main.py', 'app.py', 'wsgi.py', '__main__.py',
            'security/', 'auth/', 'child_safety/',
            'models/', 'entities/', 'api/endpoints/', 'core/domain/'
        ]
        
        self.trash_patterns = [
            r'.*_old\.py$', r'.*_backup\.py$', r'.*_temp\.py$',
            r'.*_copy\.py$', r'.*\.pyc$', r'.*~$', r'.*\.swp$'
        ]
        
    def _initialize_results(self) -> Dict[str, Any]:
        """ÿ™ŸáŸäÿ¶ÿ© ŸáŸäŸÉŸÑ ÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨"""
        return {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "project_root": str(self.project_root),
                "analyzer_version": "1.0"
            },
            "summary": {
                "total_files": 0,
                "total_lines": 0,
                "total_size_mb": 0,
                "files_by_type": defaultdict(int),
                "files_by_importance": defaultdict(int)
            },
            "classification": {
                "critical": [],
                "high": [],
                "medium": [],
                "low": [],
                "trash": []
            },
            "duplicates": {
                "exact": [],
                "functional": [],
                "similar": []
            },
            "issues": {
                "empty_files": [],
                "large_files": [],
                "security_risks": [],
                "quality_issues": [],
                "misplaced_files": []
            },
            "recommendations": {
                "immediate_deletions": [],
                "suggested_moves": [],
                "refactoring_needed": [],
                "merge_candidates": []
            },
            "detailed_analysis": [],
            "health_score": {
                "overall": 0,
                "organization": 0,
                "quality": 0,
                "security": 0,
                "documentation": 0
            }
        }
    
    def analyze_project(self) -> Dict[str, Any]:
        """ÿ™ÿ≠ŸÑŸäŸÑ ÿ¥ÿßŸÖŸÑ ŸÑŸÑŸÖÿ¥ÿ±Ÿàÿπ"""
        print("\n" + "="*60)
        print("üßπ ŸÖÿ≠ŸÑŸÑ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ¥ÿßŸÖŸÑ ŸÑŸÖÿ¥ÿ±Ÿàÿπ AI Teddy Bear")
        print("="*60 + "\n")
        
        # ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 1: ÿ¨ŸÖÿπ ÿßŸÑŸÖŸÑŸÅÿßÿ™
        print("üìÇ ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 1: ÿ¨ŸÖÿπ ŸàŸÅÿ≠ÿµ ÿßŸÑŸÖŸÑŸÅÿßÿ™...")
        all_files = self._collect_all_files()
        print(f"‚úÖ ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ {len(all_files)} ŸÖŸÑŸÅ")
        
        # ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 2: ÿ™ÿ≠ŸÑŸäŸÑ ŸÉŸÑ ŸÖŸÑŸÅ
        print("\nüìä ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 2: ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑŸÖŸÑŸÅÿßÿ™...")
        for idx, file_path in enumerate(all_files, 1):
            if idx % 50 == 0:
                print(f"   ‚è≥ ÿ™ŸÖ ÿ™ÿ≠ŸÑŸäŸÑ {idx}/{len(all_files)} ŸÖŸÑŸÅ...")
            self._analyze_file(file_path)
        
        # ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 3: ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑŸÖŸÉÿ±ÿ±ÿßÿ™
        print("\nüîç ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 3: ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ©...")
        self._find_duplicates()
        
        # ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 4: ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿ™ŸàÿµŸäÿßÿ™
        print("\nüí° ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 4: ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿ™ŸàÿµŸäÿßÿ™...")
        self._generate_recommendations()
        
        # ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 5: ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿµÿ≠ÿ© ÿßŸÑÿπÿßŸÖÿ©
        print("\nüìà ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 5: ÿ≠ÿ≥ÿßÿ® ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿµÿ≠ÿ©...")
        self._calculate_health_scores()
        
        # ÿ∑ÿ®ÿßÿπÿ© ŸÖŸÑÿÆÿµ ÿ≥ÿ±Ÿäÿπ
        self._print_quick_summary()
        
        return self.results
    
    def _collect_all_files(self) -> List[Path]:
        """ÿ¨ŸÖÿπ ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ŸÅŸä ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ"""
        files = []
        exclude_dirs = {
            '.git', '__pycache__', 'node_modules', '.venv', 'venv', 
            'env', '.tox', '.pytest_cache', 'backup_', 'security_backup_'
        }
        
        # ÿ¨ŸÖÿπ ŸÖŸÑŸÅÿßÿ™ Python
        for file_path in self.project_root.rglob("*.py"):
            if not any(excluded in str(file_path) for excluded in exclude_dirs):
                files.append(file_path)
        
        # ÿ¨ŸÖÿπ ŸÖŸÑŸÅÿßÿ™ ÿßŸÑÿ™ŸÉŸàŸäŸÜ ÿßŸÑŸÖŸáŸÖÿ©
        config_patterns = ["*.json", "*.yaml", "*.yml", "*.toml", "*.ini", "*.cfg"]
        for pattern in config_patterns:
            for file_path in self.project_root.rglob(pattern):
                if not any(excluded in str(file_path) for excluded in exclude_dirs):
                    files.append(file_path)
        
        # ÿ¨ŸÖÿπ ŸÖŸÑŸÅÿßÿ™ ÿ£ÿÆÿ±Ÿâ ŸÖŸáŸÖÿ©
        other_patterns = ["*.md", "*.txt", "*.sh", "*.bat", "Dockerfile*", "*.sql"]
        for pattern in other_patterns:
            for file_path in self.project_root.rglob(pattern):
                if not any(excluded in str(file_path) for excluded in exclude_dirs):
                    files.append(file_path)
        
        return sorted(set(files))
    
    def _analyze_file(self, file_path: Path) -> None:
        """ÿ™ÿ≠ŸÑŸäŸÑ ŸÖŸÑŸÅ Ÿàÿßÿ≠ÿØ ÿ®ÿπŸÖŸÇ"""
        try:
            # ŸÖÿπŸÑŸàŸÖÿßÿ™ ÿ£ÿ≥ÿßÿ≥Ÿäÿ©
            file_info = {
                "path": str(file_path),
                "relative_path": str(file_path.relative_to(self.project_root)),
                "name": file_path.name,
                "extension": file_path.suffix,
                "size_bytes": file_path.stat().st_size,
                "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            }
            
            # ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖÿ≠ÿ™ŸàŸâ
            content = self._read_file_content(file_path)
            file_info["lines"] = len(content.splitlines()) if content else 0
            file_info["hash"] = hashlib.md5(content.encode()).hexdigest() if content else ""
            
            # ÿ™ÿ≠ŸÑŸäŸÑ ÿ≠ÿ≥ÿ® ÿßŸÑŸÜŸàÿπ
            if file_path.suffix == '.py':
                python_analysis = self._analyze_python_file(content, file_path)
                file_info.update(python_analysis)
            
            # ÿ™ÿ≠ÿØŸäÿØ ÿßŸÑŸÜŸàÿπ ŸàÿßŸÑÿ£ŸáŸÖŸäÿ©
            file_info["type"] = self._determine_file_type(file_path, content)
            file_info["importance"] = self._determine_importance(file_path, content, file_info)
            
            # ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ
            file_info["issues"] = self._find_issues(file_path, content, file_info)
            
            # ÿßŸÇÿ™ÿ±ÿßÿ≠ ŸÖŸàŸÇÿπ ÿ£ŸÅÿ∂ŸÑ
            file_info["suggested_location"] = self._suggest_location(file_path, file_info["type"])
            
            # ÿ™ÿ≠ÿØŸäÿ´ ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™
            self._update_statistics(file_info)
            
            # ÿ•ÿ∂ÿßŸÅÿ© ŸÑŸÑŸÜÿ™ÿßÿ¶ÿ¨
            self.results["detailed_analysis"].append(file_info)
            
            # ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÖŸÑŸÅ
            self.results["classification"][file_info["importance"]].append(file_info["relative_path"])
            
        except Exception as e:
            print(f"‚ö†Ô∏è ÿÆÿ∑ÿ£ ŸÅŸä ÿ™ÿ≠ŸÑŸäŸÑ {file_path}: {e}")
    
    def _read_file_content(self, file_path: Path) -> str:
        """ŸÇÿ±ÿßÿ°ÿ© ŸÖÿ≠ÿ™ŸàŸâ ÿßŸÑŸÖŸÑŸÅ ÿ®ÿ£ŸÖÿßŸÜ"""
        try:
            # ŸÖÿ≠ÿßŸàŸÑÿ© ŸÇÿ±ÿßÿ°ÿ© ÿ®ÿ™ÿ±ŸÖŸäÿ≤ÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©
            encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
            for encoding in encodings:
                try:
                    return file_path.read_text(encoding=encoding)
                except UnicodeDecodeError:
                    continue
            return ""
        # FIXME: replace with specific exception
except Exception as exc:return ""
    
    def _analyze_python_file(self, content: str, file_path: Path) -> Dict[str, Any]:
        """ÿ™ÿ≠ŸÑŸäŸÑ ŸÖŸÑŸÅ Python"""
        analysis = {
            "is_python": True,
            "ast_analysis": {},
            "complexity": 0,
            "dependencies": []
        }
        
        try:
            tree = ast.parse(content)
            
            # ÿ™ÿ≠ŸÑŸäŸÑ AST
            imports = []
            classes = []
            functions = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)
                elif isinstance(node, ast.ClassDef):
                    classes.append({
                        "name": node.name,
                        "methods": len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                        "has_docstring": ast.get_docstring(node) is not None
                    })
                elif isinstance(node, ast.FunctionDef):
                    if not self._is_method(node, tree):
                        functions.append({
                            "name": node.name,
                            "args": len(node.args.args),
                            "has_docstring": ast.get_docstring(node) is not None
                        })
            
            analysis["ast_analysis"] = {
                "imports": imports,
                "classes": classes,
                "functions": functions,
                "has_main": self._has_main_block(tree)
            }
            
            # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿπŸÇŸäÿØ
            analysis["complexity"] = len(classes) * 2 + len(functions) + len(imports) // 5
            analysis["dependencies"] = list(set(imports))
            
        except SyntaxError:
            analysis["ast_analysis"]["error"] = "Syntax error"
        except Exception as e:
            analysis["ast_analysis"]["error"] = str(e)
        
        return analysis
    
    def _is_method(self, node: ast.FunctionDef, tree: ast.AST) -> bool:
        """ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑÿØÿßŸÑÿ© ŸáŸä method ÿØÿßÿÆŸÑ class"""
        for parent in ast.walk(tree):
            if isinstance(parent, ast.ClassDef):
                if node in parent.body:
                    return True
        return False
    
    def _has_main_block(self, tree: ast.AST) -> bool:
        """ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ if __name__ == '__main__'"""
        for node in ast.walk(tree):
            if isinstance(node, ast.If):
                if isinstance(node.test, ast.Compare):
                    if isinstance(node.test.left, ast.Name) and node.test.left.id == '__name__':
                        return True
        return False
    
    def _determine_file_type(self, file_path: Path, content: str) -> str:
        """ÿ™ÿ≠ÿØŸäÿØ ŸÜŸàÿπ ÿßŸÑŸÖŸÑŸÅ ÿ®ÿØŸÇÿ©"""
        name_lower = file_path.name.lower()
        path_lower = str(file_path).lower()
        
        # Python files
        if file_path.suffix == '.py':
            if name_lower == '__init__.py':
                return 'init'
            elif name_lower in ['setup.py', 'manage.py']:
                return 'setup'
            elif name_lower in ['main.py', 'app.py', 'wsgi.py']:
                return 'entry_point'
            elif 'test' in path_lower:
                return 'test'
            elif 'migration' in path_lower:
                return 'migration'
            elif any(x in path_lower for x in ['model', 'entity', 'schema']):
                return 'model'
            elif 'service' in path_lower:
                return 'service'
            elif any(x in path_lower for x in ['repository', 'repo']):
                return 'repository'
            elif any(x in path_lower for x in ['controller', 'endpoint', 'handler', 'route', 'api']):
                return 'controller'
            elif any(x in path_lower for x in ['util', 'helper', 'tool']):
                return 'utility'
            elif 'script' in path_lower:
                return 'script'
            elif 'config' in path_lower:
                return 'config'
            else:
                return 'python_other'
        
        # Configuration files
        elif file_path.suffix in ['.json', '.yaml', '.yml', '.toml', '.ini', '.cfg']:
            return 'config'
        
        # Documentation
        elif file_path.suffix in ['.md', '.rst', '.txt']:
            return 'documentation'
        
        # Scripts
        elif file_path.suffix in ['.sh', '.bat', '.ps1']:
            return 'script'
        
        # Docker
        elif 'dockerfile' in name_lower:
            return 'docker'
        
        # SQL
        elif file_path.suffix == '.sql':
            return 'database'
        
        else:
            return 'other'
    
    def _determine_importance(self, file_path: Path, content: str, file_info: Dict) -> str:
        """ÿ™ÿ≠ÿØŸäÿØ ÿ£ŸáŸÖŸäÿ© ÿßŸÑŸÖŸÑŸÅ"""
        path_str = str(file_path).lower()
        
        # ŸÇŸÖÿßŸÖÿ© - ÿßÿ≠ÿ∞ŸÅ ŸÅŸàÿ±ÿßŸã
        if len(content.strip()) == 0 and file_path.suffix != '.py':
            self.results["issues"]["empty_files"].append(str(file_path))
            return 'trash'
        
        # ŸÖŸÑŸÅÿßÿ™ __init__.py ŸÅÿßÿ±ÿ∫ÿ© ŸÖŸÇÿ®ŸàŸÑÿ©
        if file_path.name == '__init__.py' and len(content.strip()) == 0:
            return 'low'
        
        # ŸÖŸÑŸÅÿßÿ™ ŸÖÿ§ŸÇÿ™ÿ© ÿ£Ÿà ŸÇÿØŸäŸÖÿ©
        if any(re.match(pattern, path_str) for pattern in self.trash_patterns):
            return 'trash'
        
        if any(pattern in path_str for pattern in ['_old', '_backup', '_temp', '_copy', '.bak']):
            return 'trash'
        
        # ÿ≠ÿ±ÿ¨ÿ© - ŸÑÿß ÿ™ÿ≠ÿ∞ŸÅ ÿ£ÿ®ÿØÿßŸã
        if file_path.name in ['main.py', 'app.py', 'wsgi.py', '__main__.py', 'manage.py']:
            return 'critical'
        
        if any(x in path_str for x in ['security', 'auth', 'permission', 'child_safety']):
            return 'critical'
        
        if any(pattern in path_str for pattern in self.critical_patterns):
            return 'critical'
        
        # ÿπÿßŸÑŸäÿ© ÿßŸÑÿ£ŸáŸÖŸäÿ©
        if file_info.get("is_python"):
            ast_data = file_info.get("ast_analysis", {})
            if len(ast_data.get("classes", [])) > 2 or len(ast_data.get("functions", [])) > 5:
                return 'high'
        
        if any(x in path_str for x in ['service', 'repository', 'controller', 'api']):
            return 'high'
        
        if file_info.get("type") in ['model', 'service', 'repository', 'controller']:
            return 'high'
        
        # ŸÖŸÜÿÆŸÅÿ∂ÿ© ÿßŸÑÿ£ŸáŸÖŸäÿ©
        if file_info.get("is_python"):
            ast_data = file_info.get("ast_analysis", {})
            if (len(ast_data.get("classes", [])) == 0 and 
                len(ast_data.get("functions", [])) == 0 and
                file_path.name != '__init__.py'):
                return 'low'
        
        if 'example' in path_str or 'sample' in path_str or 'demo' in path_str:
            return 'low'
        
        # ÿßŸÅÿ™ÿ±ÿßÿ∂Ÿä
        return 'medium'
    
    def _find_issues(self, file_path: Path, content: str, file_info: Dict) -> List[Dict[str, str]]:
        """ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ŸÅŸä ÿßŸÑŸÖŸÑŸÅ"""
        issues = []
        
        # ŸÖŸÑŸÅÿßÿ™ ŸÉÿ®Ÿäÿ±ÿ©
        if file_info["size_bytes"] > 50 * 1024:  # ÿ£ŸÉÿ®ÿ± ŸÖŸÜ 50KB
            issues.append({
                "type": "size",
                "severity": "medium",
                "message": f"ŸÖŸÑŸÅ ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿßŸã ({file_info['size_bytes'] / 1024:.1f} KB)"
            })
            self.results["issues"]["large_files"].append({
                "path": str(file_path),
                "size_kb": file_info["size_bytes"] / 1024
            })
        
        # Python-specific issues
        if file_path.suffix == '.py':
            # ŸÖÿ¥ÿßŸÉŸÑ ÿ£ŸÖŸÜŸäÿ©
            security_patterns = [
                (r'eval\s*\(', "ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ eval() - ÿÆÿ∑ÿ± ÿ£ŸÖŸÜŸä"),
                (r'exec\s*\(', "ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ exec() - ÿÆÿ∑ÿ± ÿ£ŸÖŸÜŸä"),
                (r'pickle\.loads', "ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ pickle.loads - ÿÆÿ∑ÿ± ÿ£ŸÖŸÜŸä"),
                (r'(password|secret|key|token)\s*=\s*["\'][^"\']+["\']', "ŸÉŸÑŸÖÿ© ÿ≥ÿ± ŸÖÿ∂ŸÖŸÜÿ©"),
                (r'subprocess.*shell\s*=\s*True', "ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ shell=True - ÿÆÿ∑ÿ± ÿ£ŸÖŸÜŸä")
            ]
            
            for pattern, message in security_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    issues.append({
                        "type": "security",
                        "severity": "high",
                        "message": message
                    })
                    self.results["issues"]["security_risks"].append({
                        "file": str(file_path),
                        "issue": message
                    })
            
            # ŸÖÿ¥ÿßŸÉŸÑ ÿ¨ŸàÿØÿ©
            if 'except:' in content or '# FIXME: replace with specific exception
except Exception as exc:' in content:
                issues.append({
                    "type": "quality",
                    "severity": "medium",
                    "message": "ŸÖÿπÿßŸÑÿ¨ÿ© ÿßÿ≥ÿ™ÿ´ŸÜÿßÿ°ÿßÿ™ ÿπÿßŸÖÿ©"
                })
            
            if re.search(r'print\s*\(', content) and 'test' not in str(file_path):
                issues.append({
                    "type": "quality",
                    "severity": "low",
                    "message": "ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ print ŸÅŸä ŸÉŸàÿØ ÿßŸÑÿ•ŸÜÿ™ÿßÿ¨"
                })
            
            # TODOs
            todos = len(re.findall(r'(TODO|FIXME|XXX|HACK)', content))
            if todos > 0:
                issues.append({
                    "type": "quality",
                    "severity": "low",
                    "message": f"Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ {todos} TODO/FIXME"
                })
        
        return issues
    
    def _suggest_location(self, file_path: Path, file_type: str) -> Optional[str]:
        """ÿßŸÇÿ™ÿ±ÿßÿ≠ ŸÖŸàŸÇÿπ ÿ£ŸÅÿ∂ŸÑ ŸÑŸÑŸÖŸÑŸÅ"""
        current = str(file_path.relative_to(self.project_root))
        
        # ÿßŸÑŸáŸäŸÉŸÑ ÿßŸÑŸÖÿ´ÿßŸÑŸä
        ideal_structure = {
            'model': 'src/core/domain/entities/',
            'service': 'src/core/services/',
            'repository': 'src/infrastructure/persistence/repositories/',
            'controller': 'src/api/endpoints/',
            'test': 'tests/',
            'config': 'configs/',
            'utility': 'src/shared/utils/',
            'script': 'scripts/',
            'documentation': 'docs/',
            'docker': 'deployments/docker/',
            'migration': 'src/infrastructure/persistence/migrations/'
        }
        
        ideal_location = ideal_structure.get(file_type)
        
        if not ideal_location:
            return None
        
        # ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑŸÖŸÑŸÅ ŸÑŸäÿ≥ ŸÅŸä ÿßŸÑŸÖŸÉÿßŸÜ ÿßŸÑÿµÿ≠Ÿäÿ≠ ÿ®ÿßŸÑŸÅÿπŸÑ
        if current.startswith(ideal_location):
            return None
        
        # ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑŸÖÿ≥ÿßÿ± ÿßŸÑÿ¨ÿØŸäÿØ
        new_path = ideal_location + file_path.name
        
        # ÿ•ÿ∂ÿßŸÅÿ© ŸÑŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÖŸèÿ≤ÿßÿ≠ÿ©
        self.results["issues"]["misplaced_files"].append({
            "current": current,
            "suggested": new_path
        })
        
        return new_path
    
    def _update_statistics(self, file_info: Dict) -> None:
        """ÿ™ÿ≠ÿØŸäÿ´ ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™"""
        summary = self.results["summary"]
        
        summary["total_files"] += 1
        summary["total_lines"] += file_info.get("lines", 0)
        summary["total_size_mb"] += file_info["size_bytes"] / (1024 * 1024)
        summary["files_by_type"][file_info["type"]] += 1
        summary["files_by_importance"][file_info["importance"]] += 1
    
    def _find_duplicates(self) -> None:
        """ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ©"""
        # ÿ™ÿ¨ŸÖŸäÿπ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑŸÄ hash
        hash_groups = defaultdict(list)
        
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("hash"):
                hash_groups[file_info["hash"]].append(file_info)
        
        # ÿ•Ÿäÿ¨ÿßÿØ ÿßŸÑŸÖŸÉÿ±ÿ±ÿßÿ™ ÿßŸÑÿØŸÇŸäŸÇÿ©
        for file_hash, files in hash_groups.items():
            if len(files) > 1:
                self.results["duplicates"]["exact"].append({
                    "hash": file_hash,
                    "files": [f["relative_path"] for f in files],
                    "count": len(files),
                    "size_total_kb": sum(f["size_bytes"] for f in files) / 1024
                })
        
        # ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑÿ™ÿ¥ÿßÿ®Ÿá ÿßŸÑŸàÿ∏ŸäŸÅŸä (Python files only)
        function_signatures = defaultdict(list)
        
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("is_python") and file_info.get("ast_analysis"):
                functions = file_info["ast_analysis"].get("functions", [])
                for func in functions:
                    sig = f"{func['name']}({func['args']})"
                    function_signatures[sig].append(file_info["relative_path"])
        
        # ÿ•Ÿäÿ¨ÿßÿØ ÿßŸÑÿØŸàÿßŸÑ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ©
        for sig, files in function_signatures.items():
            if len(files) > 1:
                self.results["duplicates"]["functional"].append({
                    "signature": sig,
                    "files": files,
                    "count": len(files)
                })
        
        print(f"‚úÖ ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ {len(self.results['duplicates']['exact'])} ŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÑŸÅÿßÿ™ ŸÖŸÉÿ±ÿ±ÿ©")
        print(f"‚úÖ ÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ {len(self.results['duplicates']['functional'])} ÿØÿßŸÑÿ© ŸÖŸÉÿ±ÿ±ÿ©")
    
    def _generate_recommendations(self) -> None:
        """ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑÿ™ŸàÿµŸäÿßÿ™"""
        recs = self.results["recommendations"]
        
        # 1. ÿ™ŸàÿµŸäÿßÿ™ ÿßŸÑÿ≠ÿ∞ŸÅ ÿßŸÑŸÅŸàÿ±Ÿä
        for file_path in self.results["classification"]["trash"]:
            recs["immediate_deletions"].append({
                "file": file_path,
                "reason": "ŸÖŸÑŸÅ ŸÇŸÖÿßŸÖÿ© (ŸÅÿßÿ±ÿ∫/ŸÇÿØŸäŸÖ/ŸÖÿ§ŸÇÿ™)",
                "action": "DELETE"
            })
        
        # 2. ÿ™ŸàÿµŸäÿßÿ™ ÿßŸÑŸÜŸÇŸÑ
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("suggested_location"):
                recs["suggested_moves"].append({
                    "from": file_info["relative_path"],
                    "to": file_info["suggested_location"],
                    "type": file_info["type"],
                    "reason": "ŸÖŸàŸÇÿπ ÿ∫Ÿäÿ± ŸÖŸÜÿßÿ≥ÿ®"
                })
        
        # 3. ÿ™ŸàÿµŸäÿßÿ™ ÿ•ÿπÿßÿØÿ© ÿßŸÑŸáŸäŸÉŸÑÿ©
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("lines", 0) > 500:
                recs["refactoring_needed"].append({
                    "file": file_info["relative_path"],
                    "lines": file_info["lines"],
                    "reason": "ŸÖŸÑŸÅ ŸÉÿ®Ÿäÿ± ÿ¨ÿØÿßŸã",
                    "suggestion": "ÿ™ŸÇÿ≥ŸäŸÖ ÿ•ŸÑŸâ ŸÖŸÑŸÅÿßÿ™ ÿ£ÿµÿ∫ÿ±"
                })
        
        # 4. ÿßŸÇÿ™ÿ±ÿßÿ≠ÿßÿ™ ÿßŸÑÿØŸÖÿ¨
        for dup_group in self.results["duplicates"]["exact"]:
            if dup_group["count"] > 1:
                recs["merge_candidates"].append({
                    "files": dup_group["files"],
                    "action": "MERGE",
                    "reason": "ŸÖŸÑŸÅÿßÿ™ ŸÖÿ™ÿ∑ÿßÿ®ŸÇÿ© ÿ™ŸÖÿßŸÖÿßŸã"
                })
        
        print(f"‚úÖ ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° {len(recs['immediate_deletions'])} ÿ™ŸàÿµŸäÿ© ÿ≠ÿ∞ŸÅ")
        print(f"‚úÖ ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° {len(recs['suggested_moves'])} ÿ™ŸàÿµŸäÿ© ŸÜŸÇŸÑ")
        print(f"‚úÖ ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° {len(recs['refactoring_needed'])} ÿ™ŸàÿµŸäÿ© ÿ•ÿπÿßÿØÿ© ŸáŸäŸÉŸÑÿ©")
    
    def _calculate_health_scores(self) -> None:
        """ÿ≠ÿ≥ÿßÿ® ŸÖŸÇÿßŸäŸäÿ≥ ÿµÿ≠ÿ© ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ"""
        total_files = self.results["summary"]["total_files"]
        
        if total_files == 0:
            return
        
        # ŸÜŸÇÿßÿ∑ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ
        misplaced_ratio = len(self.results["issues"]["misplaced_files"]) / total_files
        trash_ratio = len(self.results["classification"]["trash"]) / total_files
        organization_score = max(0, 100 - (misplaced_ratio * 50) - (trash_ratio * 30))
        
        # ŸÜŸÇÿßÿ∑ ÿßŸÑÿ¨ŸàÿØÿ©
        quality_issues = sum(1 for f in self.results["detailed_analysis"] 
                           if any(i["type"] == "quality" for i in f.get("issues", [])))
        quality_ratio = quality_issues / total_files
        quality_score = max(0, 100 - (quality_ratio * 60))
        
        # ŸÜŸÇÿßÿ∑ ÿßŸÑÿ£ŸÖÿßŸÜ
        security_issues = len(self.results["issues"]["security_risks"])
        security_ratio = security_issues / total_files
        security_score = max(0, 100 - (security_ratio * 100))
        
        # ŸÜŸÇÿßÿ∑ ÿßŸÑÿ™Ÿàÿ´ŸäŸÇ
        documented_files = sum(1 for f in self.results["detailed_analysis"]
                             if f.get("is_python") and 
                             any(c.get("has_docstring") for c in f.get("ast_analysis", {}).get("classes", [])))
        doc_ratio = documented_files / max(1, len([f for f in self.results["detailed_analysis"] if f.get("is_python")]))
        documentation_score = doc_ratio * 100
        
        # ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸäÿ©
        overall_score = (organization_score * 0.3 + 
                        quality_score * 0.3 + 
                        security_score * 0.3 + 
                        documentation_score * 0.1)
        
        self.results["health_score"] = {
            "overall": round(overall_score, 1),
            "organization": round(organization_score, 1),
            "quality": round(quality_score, 1),
            "security": round(security_score, 1),
            "documentation": round(documentation_score, 1)
        }
    
    def _print_quick_summary(self) -> None:
        """ÿ∑ÿ®ÿßÿπÿ© ŸÖŸÑÿÆÿµ ÿ≥ÿ±Ÿäÿπ"""
        print("\n" + "="*60)
        print("üìä ŸÖŸÑÿÆÿµ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ≥ÿ±Ÿäÿπ")
        print("="*60)
        
        summary = self.results["summary"]
        health = self.results["health_score"]
        
        print(f"\nüìà ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™ ÿßŸÑÿπÿßŸÖÿ©:")
        print(f"   ‚Ä¢ ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑŸÖŸÑŸÅÿßÿ™: {summary['total_files']}")
        print(f"   ‚Ä¢ ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑÿ£ÿ≥ÿ∑ÿ±: {summary['total_lines']:,}")
        print(f"   ‚Ä¢ ÿßŸÑÿ≠ÿ¨ŸÖ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸä: {summary['total_size_mb']:.2f} MB")
        
        print(f"\nüè• ÿµÿ≠ÿ© ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ:")
        print(f"   ‚Ä¢ ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸäÿ©: {health['overall']}%")
        print(f"   ‚Ä¢ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ: {health['organization']}%")
        print(f"   ‚Ä¢ ÿßŸÑÿ¨ŸàÿØÿ©: {health['quality']}%")
        print(f"   ‚Ä¢ ÿßŸÑÿ£ŸÖÿßŸÜ: {health['security']}%")
        print(f"   ‚Ä¢ ÿßŸÑÿ™Ÿàÿ´ŸäŸÇ: {health['documentation']}%")
        
        print(f"\nüìÅ ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÖŸÑŸÅÿßÿ™:")
        for importance, count in self.results["summary"]["files_by_importance"].items():
            print(f"   ‚Ä¢ {importance}: {count} ŸÖŸÑŸÅ")
        
        print(f"\nüö® ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑŸÖŸÉÿ™ÿ¥ŸÅÿ©:")
        print(f"   ‚Ä¢ ŸÖŸÑŸÅÿßÿ™ ŸÅÿßÿ±ÿ∫ÿ©: {len(self.results['issues']['empty_files'])}")
        print(f"   ‚Ä¢ ŸÖŸÑŸÅÿßÿ™ ŸÉÿ®Ÿäÿ±ÿ©: {len(self.results['issues']['large_files'])}")
        print(f"   ‚Ä¢ ŸÖÿ¥ÿßŸÉŸÑ ÿ£ŸÖŸÜŸäÿ©: {len(self.results['issues']['security_risks'])}")
        print(f"   ‚Ä¢ ŸÖŸÑŸÅÿßÿ™ ŸÅŸä ŸÖŸÉÿßŸÜ ÿÆÿßÿ∑ÿ¶: {len(self.results['issues']['misplaced_files'])}")
        
        print(f"\nüí° ÿßŸÑÿ™ŸàÿµŸäÿßÿ™:")
        recs = self.results["recommendations"]
        print(f"   ‚Ä¢ ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑÿ≠ÿ∞ŸÅ: {len(recs['immediate_deletions'])}")
        print(f"   ‚Ä¢ ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑŸÜŸÇŸÑ: {len(recs['suggested_moves'])}")
        print(f"   ‚Ä¢ ŸÖŸÑŸÅÿßÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ÿπÿßÿØÿ© ŸáŸäŸÉŸÑÿ©: {len(recs['refactoring_needed'])}")
        print(f"   ‚Ä¢ ŸÖÿ¨ŸÖŸàÿπÿßÿ™ ŸÑŸÑÿØŸÖÿ¨: {len(recs['merge_candidates'])}")
    
    def save_reports(self, output_dir: str = "cleanup_reports") -> None:
        """ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ŸÇÿßÿ±Ÿäÿ± ÿ®ÿ™ŸÜÿ≥ŸäŸÇÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ÿ≠ŸÅÿ∏ JSON ŸÖŸÅÿµŸÑ
        json_file = output_path / f"comprehensive_analysis_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\n‚úÖ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± JSON: {json_file}")
        
        # ÿ≠ŸÅÿ∏ ÿ™ŸÇÿ±Ÿäÿ± Markdown
        md_file = output_path / f"cleanup_report_{timestamp}.md"
        self._save_markdown_report(md_file)
        print(f"‚úÖ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ± Markdown: {md_file}")
        
        # ÿ≠ŸÅÿ∏ ŸÇÿßÿ¶ŸÖÿ© ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™
        actions_file = output_path / f"cleanup_actions_{timestamp}.txt"
        self._save_action_list(actions_file)
        print(f"‚úÖ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ŸÇÿßÿ¶ŸÖÿ© ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™: {actions_file}")
    
    def _save_markdown_report(self, file_path: Path) -> None:
        """ÿ≠ŸÅÿ∏ ÿ™ŸÇÿ±Ÿäÿ± Markdown ÿ¥ÿßŸÖŸÑ"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# üßπ ÿ™ŸÇÿ±Ÿäÿ± ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ¥ÿßŸÖŸÑ ŸÑŸÖÿ¥ÿ±Ÿàÿπ AI Teddy Bear\n\n")
            f.write(f"**ÿßŸÑÿ™ÿßÿ±ŸäÿÆ**: {self.results['metadata']['timestamp']}\n")
            f.write(f"**ÿßŸÑŸÖÿ≠ŸÑŸÑ**: Comprehensive Cleanup Analyzer v{self.results['metadata']['analyzer_version']}\n\n")
            
            # ŸÖŸÑÿÆÿµ ÿ™ŸÜŸÅŸäÿ∞Ÿä
            f.write("## üìä ŸÖŸÑÿÆÿµ ÿ™ŸÜŸÅŸäÿ∞Ÿä\n\n")
            
            health = self.results["health_score"]
            f.write(f"**ÿµÿ≠ÿ© ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸäÿ©**: {health['overall']}% ")
            
            if health['overall'] >= 80:
                f.write("‚úÖ ŸÖŸÖÿ™ÿßÿ≤\n\n")
            elif health['overall'] >= 60:
                f.write("‚ö†Ô∏è Ÿäÿ≠ÿ™ÿßÿ¨ ÿ™ÿ≠ÿ≥ŸäŸÜ\n\n")
            else:
                f.write("‚ùå Ÿäÿ≠ÿ™ÿßÿ¨ ÿπŸÜÿßŸäÿ© ŸÅŸàÿ±Ÿäÿ©\n\n")
            
            # ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿµÿ≠ÿ©
            f.write("### üè• ŸÖŸÇÿßŸäŸäÿ≥ ÿßŸÑÿµÿ≠ÿ©\n\n")
            f.write("| ÿßŸÑŸÖŸÇŸäÿßÿ≥ | ÿßŸÑŸÜŸÇÿßÿ∑ | ÿßŸÑÿ™ŸÇŸäŸäŸÖ |\n")
            f.write("|---------|--------|----------|\n")
            
            metrics = [
                ("ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ", health['organization']),
                ("ÿßŸÑÿ¨ŸàÿØÿ©", health['quality']),
                ("ÿßŸÑÿ£ŸÖÿßŸÜ", health['security']),
                ("ÿßŸÑÿ™Ÿàÿ´ŸäŸÇ", health['documentation'])
            ]
            
            for metric, score in metrics:
                if score >= 80:
                    status = "‚úÖ"
                elif score >= 60:
                    status = "‚ö†Ô∏è"
                else:
                    status = "‚ùå"
                f.write(f"| {metric} | {score}% | {status} |\n")
            
            # ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™
            f.write("\n## üìà ÿßŸÑÿ•ÿ≠ÿµÿßÿ¶Ÿäÿßÿ™ ÿßŸÑÿπÿßŸÖÿ©\n\n")
            summary = self.results["summary"]
            f.write(f"- **ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑŸÖŸÑŸÅÿßÿ™**: {summary['total_files']}\n")
            f.write(f"- **ÿ•ÿ¨ŸÖÿßŸÑŸä ÿßŸÑÿ£ÿ≥ÿ∑ÿ±**: {summary['total_lines']:,}\n")
            f.write(f"- **ÿßŸÑÿ≠ÿ¨ŸÖ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸä**: {summary['total_size_mb']:.2f} MB\n\n")
            
            # ÿ™Ÿàÿ≤Ÿäÿπ ÿßŸÑŸÖŸÑŸÅÿßÿ™
            f.write("### üìÅ ÿ™Ÿàÿ≤Ÿäÿπ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑŸÜŸàÿπ\n\n")
            f.write("| ÿßŸÑŸÜŸàÿπ | ÿßŸÑÿπÿØÿØ | ÿßŸÑŸÜÿ≥ÿ®ÿ© |\n")
            f.write("|-------|-------|--------|\n")
            
            total = summary['total_files']
            for file_type, count in sorted(summary['files_by_type'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total) * 100 if total > 0 else 0
                f.write(f"| {file_type} | {count} | {percentage:.1f}% |\n")
            
            # ÿ™ÿµŸÜŸäŸÅ ÿßŸÑÿ£ŸáŸÖŸäÿ©
            f.write("\n### üéØ ÿ™ÿµŸÜŸäŸÅ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿ≠ÿ≥ÿ® ÿßŸÑÿ£ŸáŸÖŸäÿ©\n\n")
            importance_map = {
                'critical': 'üî¥ ÿ≠ÿ±ÿ¨ÿ©',
                'high': 'üü† ÿπÿßŸÑŸäÿ©',
                'medium': 'üü° ŸÖÿ™Ÿàÿ≥ÿ∑ÿ©',
                'low': 'üü¢ ŸÖŸÜÿÆŸÅÿ∂ÿ©',
                'trash': '‚ö´ ŸÇŸÖÿßŸÖÿ©'
            }
            
            for importance, label in importance_map.items():
                count = summary['files_by_importance'].get(importance, 0)
                f.write(f"- **{label}**: {count} ŸÖŸÑŸÅ\n")
            
            # ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑŸÖŸÉÿ™ÿ¥ŸÅÿ©
            f.write("\n## üö® ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑŸÖŸÉÿ™ÿ¥ŸÅÿ©\n\n")
            
            issues = self.results["issues"]
            
            # ŸÖŸÑŸÅÿßÿ™ ŸÅÿßÿ±ÿ∫ÿ©
            if issues["empty_files"]:
                f.write(f"### üìÑ ŸÖŸÑŸÅÿßÿ™ ŸÅÿßÿ±ÿ∫ÿ© ({len(issues['empty_files'])})\n\n")
                for file in issues["empty_files"][:10]:
                    f.write(f"- `{file}`\n")
                if len(issues["empty_files"]) > 10:
                    f.write(f"- ... Ÿà {len(issues['empty_files']) - 10} ŸÖŸÑŸÅ ÿ¢ÿÆÿ±\n")
            
            # ŸÖŸÑŸÅÿßÿ™ ŸÉÿ®Ÿäÿ±ÿ©
            if issues["large_files"]:
                f.write(f"\n### üì¶ ŸÖŸÑŸÅÿßÿ™ ŸÉÿ®Ÿäÿ±ÿ© ({len(issues['large_files'])})\n\n")
                for file in sorted(issues["large_files"], key=lambda x: x['size_kb'], reverse=True)[:10]:
                    f.write(f"- `{file['path']}` ({file['size_kb']:.1f} KB)\n")
            
            # ŸÖÿ¥ÿßŸÉŸÑ ÿ£ŸÖŸÜŸäÿ©
            if issues["security_risks"]:
                f.write(f"\n### üîê ŸÖÿ¥ÿßŸÉŸÑ ÿ£ŸÖŸÜŸäÿ© ({len(issues['security_risks'])})\n\n")
                for risk in issues["security_risks"][:10]:
                    f.write(f"- `{risk['file']}`: {risk['issue']}\n")
            
            # ŸÖŸÑŸÅÿßÿ™ ŸÅŸä ŸÖŸÉÿßŸÜ ÿÆÿßÿ∑ÿ¶
            if issues["misplaced_files"]:
                f.write(f"\n### üìç ŸÖŸÑŸÅÿßÿ™ ŸÅŸä ŸÖŸÉÿßŸÜ ÿÆÿßÿ∑ÿ¶ ({len(issues['misplaced_files'])})\n\n")
                f.write("| ÿßŸÑŸÖŸÑŸÅ | ÿßŸÑŸÖŸÉÿßŸÜ ÿßŸÑÿ≠ÿßŸÑŸä | ÿßŸÑŸÖŸÉÿßŸÜ ÿßŸÑŸÖŸÇÿ™ÿ±ÿ≠ |\n")
                f.write("|-------|----------------|------------------|\n")
                for misplaced in issues["misplaced_files"][:20]:
                    current = misplaced['current']
                    suggested = misplaced['suggested']
                    f.write(f"| {Path(current).name} | `{Path(current).parent}` | `{Path(suggested).parent}` |\n")
            
            # ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ©
            f.write("\n## üîÑ ÿßŸÑŸÖŸÑŸÅÿßÿ™ ÿßŸÑŸÖŸÉÿ±ÿ±ÿ©\n\n")
            
            duplicates = self.results["duplicates"]
            
            if duplicates["exact"]:
                f.write(f"### üìë ŸÖŸÑŸÅÿßÿ™ ŸÖÿ™ÿ∑ÿßÿ®ŸÇÿ© ÿ™ŸÖÿßŸÖÿßŸã ({len(duplicates['exact'])} ŸÖÿ¨ŸÖŸàÿπÿ©)\n\n")
                for idx, dup_group in enumerate(duplicates["exact"][:10], 1):
                    f.write(f"**ÿßŸÑŸÖÿ¨ŸÖŸàÿπÿ© {idx}** ({dup_group['count']} ŸÖŸÑŸÅÿßÿ™ÿå {dup_group['size_total_kb']:.1f} KB ÿ•ÿ¨ŸÖÿßŸÑŸä):\n")
                    for file in dup_group['files']:
                        f.write(f"- `{file}`\n")
                    f.write("\n")
            
            # ÿßŸÑÿ™ŸàÿµŸäÿßÿ™
            f.write("## üí° ÿßŸÑÿ™ŸàÿµŸäÿßÿ™ ŸàÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™\n\n")
            
            recs = self.results["recommendations"]
            
            # ÿ≠ÿ∞ŸÅ ŸÅŸàÿ±Ÿä
            if recs["immediate_deletions"]:
                f.write(f"### üóëÔ∏è ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑÿ≠ÿ∞ŸÅ ÿßŸÑŸÅŸàÿ±Ÿä ({len(recs['immediate_deletions'])})\n\n")
                f.write("‚ö†Ô∏è **ÿ™ÿ≠ÿ∞Ÿäÿ±**: Ÿáÿ∞Ÿá ÿßŸÑŸÖŸÑŸÅÿßÿ™ ŸäŸÖŸÉŸÜ ÿ≠ÿ∞ŸÅŸáÿß ÿ®ÿ£ŸÖÿßŸÜ\n\n")
                
                for deletion in recs["immediate_deletions"][:20]:
                    f.write(f"- `{deletion['file']}` - {deletion['reason']}\n")
                
                if len(recs["immediate_deletions"]) > 20:
                    f.write(f"\n... Ÿà {len(recs['immediate_deletions']) - 20} ŸÖŸÑŸÅ ÿ¢ÿÆÿ±\n")
                
                # ÿ£ŸÖÿ± ÿßŸÑÿ≠ÿ∞ŸÅ
                f.write("\n```bash\n# ÿ£ŸÖÿ± ÿ≠ÿ∞ŸÅ ÿßŸÑŸÖŸÑŸÅÿßÿ™ (ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿπŸÖŸÑ ŸÜÿ≥ÿÆÿ© ÿßÿ≠ÿ™Ÿäÿßÿ∑Ÿäÿ© ÿ£ŸàŸÑÿßŸã)\n")
                for deletion in recs["immediate_deletions"][:5]:
                    f.write(f'rm "{deletion["file"]}"\n')
                f.write("```\n")
            
            # ŸÜŸÇŸÑ ÿßŸÑŸÖŸÑŸÅÿßÿ™
            if recs["suggested_moves"]:
                f.write(f"\n### üì¶ ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑŸÜŸÇŸÑ ({len(recs['suggested_moves'])})\n\n")
                
                # ÿ™ÿ¨ŸÖŸäÿπ ÿ≠ÿ≥ÿ® ÿßŸÑŸÜŸàÿπ
                moves_by_type = defaultdict(list)
                for move in recs["suggested_moves"]:
                    moves_by_type[move['type']].append(move)
                
                for file_type, moves in moves_by_type.items():
                    f.write(f"\n**{file_type.title()} Files ({len(moves)})**:\n")
                    for move in moves[:5]:
                        f.write(f"- `{move['from']}` ‚Üí `{move['to']}`\n")
            
            # ÿ•ÿπÿßÿØÿ© ŸáŸäŸÉŸÑÿ©
            if recs["refactoring_needed"]:
                f.write(f"\n### üîß ŸÖŸÑŸÅÿßÿ™ ÿ™ÿ≠ÿ™ÿßÿ¨ ÿ•ÿπÿßÿØÿ© ŸáŸäŸÉŸÑÿ© ({len(recs['refactoring_needed'])})\n\n")
                for refactor in recs["refactoring_needed"][:10]:
                    f.write(f"- `{refactor['file']}` ({refactor['lines']} ÿ≥ÿ∑ÿ±) - {refactor['suggestion']}\n")
            
            # ÿÆÿ∑ÿ© ÿßŸÑÿπŸÖŸÑ
            f.write("\n## üìã ÿÆÿ∑ÿ© ÿßŸÑÿπŸÖŸÑ ÿßŸÑŸÖŸÇÿ™ÿ±ÿ≠ÿ©\n\n")
            f.write("### ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 1: ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑŸÅŸàÿ±Ÿä (ŸäŸàŸÖ 1)\n")
            f.write(f"1. ÿ≠ÿ∞ŸÅ {len(recs['immediate_deletions'])} ŸÖŸÑŸÅ ŸÇŸÖÿßŸÖÿ©\n")
            f.write(f"2. ÿØŸÖÿ¨ {len(duplicates['exact'])} ŸÖÿ¨ŸÖŸàÿπÿ© ŸÖŸÑŸÅÿßÿ™ ŸÖŸÉÿ±ÿ±ÿ©\n")
            f.write("3. ÿπŸÖŸÑ ŸÜÿ≥ÿÆÿ© ÿßÿ≠ÿ™Ÿäÿßÿ∑Ÿäÿ© ŸÇÿ®ŸÑ ÿ£Ÿä ÿ™ÿ∫ŸäŸäÿ±\n\n")
            
            f.write("### ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 2: ÿ•ÿπÿßÿØÿ© ÿßŸÑÿ™ŸÜÿ∏ŸäŸÖ (ŸäŸàŸÖ 2-3)\n")
            f.write(f"1. ŸÜŸÇŸÑ {len(recs['suggested_moves'])} ŸÖŸÑŸÅ ŸÑŸÑÿ£ŸÖÿßŸÉŸÜ ÿßŸÑÿµÿ≠Ÿäÿ≠ÿ©\n")
            f.write("2. ÿ™ÿ≠ÿØŸäÿ´ ÿ¨ŸÖŸäÿπ imports ÿßŸÑŸÖÿ™ÿ£ÿ´ÿ±ÿ©\n")
            f.write("3. ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿπŸÖŸÑ ÿ¨ŸÖŸäÿπ ÿßŸÑÿßÿÆÿ™ÿ®ÿßÿ±ÿßÿ™\n\n")
            
            f.write("### ÿßŸÑŸÖÿ±ÿ≠ŸÑÿ© 3: ÿ•ÿπÿßÿØÿ© ÿßŸÑŸáŸäŸÉŸÑÿ© (ŸäŸàŸÖ 4-5)\n")
            f.write(f"1. ÿ™ŸÇÿ≥ŸäŸÖ {len(recs['refactoring_needed'])} ŸÖŸÑŸÅ ŸÉÿ®Ÿäÿ±\n")
            f.write(f"2. ÿ•ÿµŸÑÿßÿ≠ {len(issues['security_risks'])} ŸÖÿ¥ŸÉŸÑÿ© ÿ£ŸÖŸÜŸäÿ©\n")
            f.write("3. ÿ™ÿ≠ÿ≥ŸäŸÜ ÿßŸÑÿ™Ÿàÿ´ŸäŸÇ ŸàÿßŸÑÿ™ÿπŸÑŸäŸÇÿßÿ™\n\n")
            
            # ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÖÿ™ŸàŸÇÿπÿ©
            f.write("## üéØ ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ÿßŸÑŸÖÿ™ŸàŸÇÿπÿ© ÿ®ÿπÿØ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ\n\n")
            
            # ÿ≠ÿ≥ÿßÿ® ÿßŸÑÿ™ÿ≠ÿ≥ŸäŸÜÿßÿ™
            files_after = summary['total_files'] - len(recs['immediate_deletions'])
            size_saved = sum(f['size_bytes'] for f in self.results['detailed_analysis'] 
                           if f['relative_path'] in [d['file'] for d in recs['immediate_deletions']]) / (1024 * 1024)
            
            f.write(f"- **ÿπÿØÿØ ÿßŸÑŸÖŸÑŸÅÿßÿ™**: {summary['total_files']} ‚Üí {files_after} (‚¨áÔ∏è {len(recs['immediate_deletions'])})\n")
            f.write(f"- **ÿßŸÑÿ≠ÿ¨ŸÖ**: {summary['total_size_mb']:.2f} MB ‚Üí {summary['total_size_mb'] - size_saved:.2f} MB (‚¨áÔ∏è {size_saved:.2f} MB)\n")
            f.write(f"- **ÿµÿ≠ÿ© ÿßŸÑŸÖÿ¥ÿ±Ÿàÿπ**: {health['overall']}% ‚Üí ~90% (‚¨ÜÔ∏è {90 - health['overall']:.0f}%)\n")
            f.write(f"- **ŸáŸäŸÉŸÑ ÿ£Ÿàÿ∂ÿ≠ Ÿàÿ£ÿ≥ŸáŸÑ ŸÑŸÑÿµŸäÿßŸÜÿ©**\n")
            f.write(f"- **ÿ£ŸÖÿßŸÜ ŸÖÿ≠ÿ≥ŸëŸÜ Ÿàÿ£ÿØÿßÿ° ÿ£ŸÅÿ∂ŸÑ**\n")
    
    def _save_action_list(self, file_path: Path) -> None:
        """ÿ≠ŸÅÿ∏ ŸÇÿßÿ¶ŸÖÿ© ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ≥ÿ±Ÿäÿπÿ©"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("ŸÇÿßÿ¶ŸÖÿ© ÿßŸÑÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿßŸÑÿ≥ÿ±Ÿäÿπÿ© - ŸÖÿ¥ÿ±Ÿàÿπ AI Teddy Bear\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("1. ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑÿ≠ÿ∞ŸÅ ÿßŸÑŸÅŸàÿ±Ÿä:\n")
            f.write("-" * 30 + "\n")
            
            for deletion in self.results["recommendations"]["immediate_deletions"]:
                f.write(f'DEL "{deletion["file"]}"\n')
            
            f.write("\n2. ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑŸÜŸÇŸÑ:\n")
            f.write("-" * 30 + "\n")
            
            for move in self.results["recommendations"]["suggested_moves"]:
                f.write(f'MOVE "{move["from"]}" -> "{move["to"]}"\n')
            
            f.write("\n3. ŸÖŸÑŸÅÿßÿ™ ŸÑŸÑÿØŸÖÿ¨:\n")
            f.write("-" * 30 + "\n")
            
            for merge in self.results["recommendations"]["merge_candidates"]:
                f.write(f'MERGE: {", ".join(merge["files"])}\n')


def main():
    """ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑŸÖÿ≠ŸÑŸÑ ÿßŸÑÿ¥ÿßŸÖŸÑ"""
    print("\nüöÄ ÿ®ÿØÿ° ÿ™ÿ≠ŸÑŸäŸÑ ÿßŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿßŸÑÿ¥ÿßŸÖŸÑ...")
    
    # ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑŸÖÿ≠ŸÑŸÑ
    analyzer = ComprehensiveCleanupAnalyzer()
    
    # ÿ™ÿ¥ÿ∫ŸäŸÑ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ
    results = analyzer.analyze_project()
    
    # ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ŸÇÿßÿ±Ÿäÿ±
    analyzer.save_reports()
    
    print("\n‚úÖ ÿßŸÉÿ™ŸÖŸÑ ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ÿ®ŸÜÿ¨ÿßÿ≠!")
    print("üìã ÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ŸÖÿ¨ŸÑÿØ cleanup_reports ŸÑŸÑÿ™ŸÇÿßÿ±Ÿäÿ± ÿßŸÑŸÖŸÅÿµŸÑÿ©")


if __name__ == "__main__":
    main() 