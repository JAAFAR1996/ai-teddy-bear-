#!/usr/bin/env python3
"""
Ù…Ø­Ù„Ù„ Ø´Ø§Ù…Ù„ Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØªØ±ØªÙŠØ¨ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear
ÙŠÙ‚ÙˆÙ… Ø¨ØªØ­Ù„ÙŠÙ„ 400+ Ù…Ù„Ù ÙˆØªØµÙ†ÙŠÙÙ‡Ø§ ÙˆØ§ÙƒØªØ´Ø§Ù Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª
"""

import os
import sys
import json
import hashlib
import ast
import re
from pathlib import Path
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Tuple, Any, Optional, Set
import shutil

# Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø³Ø§Ø± Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class ComprehensiveCleanupAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙ†Ø¸ÙŠÙ ÙˆØ§Ù„ØªØ±ØªÙŠØ¨"""
    
    def __init__(self, project_root: str = '.'):
        self.project_root = Path(project_root).resolve()
        self.results = self._initialize_results()
        
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØµÙ†ÙŠÙ
        self.critical_patterns = [
            'main.py', 'app.py', 'wsgi.py', '__main__.py',
            'security/', 'auth/', 'child_safety/',
            'models/', 'entities/', 'api/endpoints/', 'core/domain/'
        ]
        
        self.trash_patterns = [
            r'.*_old\.py$', r'.*_backup\.py$', r'.*_temp\.py$',
            r'.*_copy\.py$', r'.*\.pyc$', r'.*~$', r'.*\.swp$'
        ]
        
    def _initialize_results(self) -> Dict[str, Any]:
        """ØªÙ‡ÙŠØ¦Ø© Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬"""
        return {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "project_root": str(self.project_root),
                "analyzer_version": "1.0"
            },
            "summary": {
                "total_files": 0,
                "total_lines": 0,
                "total_size_mb": 0,
                "files_by_type": defaultdict(int),
                "files_by_importance": defaultdict(int)
            },
            "classification": {
                "critical": [],
                "high": [],
                "medium": [],
                "low": [],
                "trash": []
            },
            "duplicates": {
                "exact": [],
                "functional": [],
                "similar": []
            },
            "issues": {
                "empty_files": [],
                "large_files": [],
                "security_risks": [],
                "quality_issues": [],
                "misplaced_files": []
            },
            "recommendations": {
                "immediate_deletions": [],
                "suggested_moves": [],
                "refactoring_needed": [],
                "merge_candidates": []
            },
            "detailed_analysis": [],
            "health_score": {
                "overall": 0,
                "organization": 0,
                "quality": 0,
                "security": 0,
                "documentation": 0
            }
        }
    
    def analyze_project(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("\n" + "="*60)
        print("ğŸ§¹ Ù…Ø­Ù„Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear")
        print("="*60 + "\n")
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¬Ù…Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        print("ğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø¬Ù…Ø¹ ÙˆÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        all_files = self._collect_all_files()
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(all_files)} Ù…Ù„Ù")
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù
        print("\nğŸ“Š Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        for idx, file_path in enumerate(all_files, 1):
            if idx % 50 == 0:
                print(f"   â³ ØªÙ… ØªØ­Ù„ÙŠÙ„ {idx}/{len(all_files)} Ù…Ù„Ù...")
            self._analyze_file(file_path)
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
        print("\nğŸ” Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
        self._find_duplicates()
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª
        print("\nğŸ’¡ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª...")
        self._generate_recommendations()
        
        # Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø­Ø³Ø§Ø¨ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø©
        print("\nğŸ“ˆ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 5: Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØµØ­Ø©...")
        self._calculate_health_scores()
        
        # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹
        self._print_quick_summary()
        
        return self.results
    
    def _collect_all_files(self) -> List[Path]:
        """Ø¬Ù…Ø¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        files = []
        exclude_dirs = {
            '.git', '__pycache__', 'node_modules', '.venv', 'venv', 
            'env', '.tox', '.pytest_cache', 'backup_', 'security_backup_'
        }
        
        # Ø¬Ù…Ø¹ Ù…Ù„ÙØ§Øª Python
        for file_path in self.project_root.rglob("*.py"):
            if not any(excluded in str(file_path) for excluded in exclude_dirs):
                files.append(file_path)
        
        # Ø¬Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…Ù‡Ù…Ø©
        config_patterns = ["*.json", "*.yaml", "*.yml", "*.toml", "*.ini", "*.cfg"]
        for pattern in config_patterns:
            for file_path in self.project_root.rglob(pattern):
                if not any(excluded in str(file_path) for excluded in exclude_dirs):
                    files.append(file_path)
        
        # Ø¬Ù…Ø¹ Ù…Ù„ÙØ§Øª Ø£Ø®Ø±Ù‰ Ù…Ù‡Ù…Ø©
        other_patterns = ["*.md", "*.txt", "*.sh", "*.bat", "Dockerfile*", "*.sql"]
        for pattern in other_patterns:
            for file_path in self.project_root.rglob(pattern):
                if not any(excluded in str(file_path) for excluded in exclude_dirs):
                    files.append(file_path)
        
        return sorted(set(files))
    
    def _analyze_file(self, file_path: Path) -> None:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¨Ø¹Ù…Ù‚"""
        try:
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
            file_info = {
                "path": str(file_path),
                "relative_path": str(file_path.relative_to(self.project_root)),
                "name": file_path.name,
                "extension": file_path.suffix,
                "size_bytes": file_path.stat().st_size,
                "modified": datetime.fromtimestamp(file_path.stat().st_mtime).isoformat()
            }
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content = self._read_file_content(file_path)
            file_info["lines"] = len(content.splitlines()) if content else 0
            file_info["hash"] = hashlib.md5(content.encode()).hexdigest() if content else ""
            
            # ØªØ­Ù„ÙŠÙ„ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
            if file_path.suffix == '.py':
                python_analysis = self._analyze_python_file(content, file_path)
                file_info.update(python_analysis)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†ÙˆØ¹ ÙˆØ§Ù„Ø£Ù‡Ù…ÙŠØ©
            file_info["type"] = self._determine_file_type(file_path, content)
            file_info["importance"] = self._determine_importance(file_path, content, file_info)
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            file_info["issues"] = self._find_issues(file_path, content, file_info)
            
            # Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø£ÙØ¶Ù„
            file_info["suggested_location"] = self._suggest_location(file_path, file_info["type"])
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self._update_statistics(file_info)
            
            # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù†ØªØ§Ø¦Ø¬
            self.results["detailed_analysis"].append(file_info)
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„Ù
            self.results["classification"][file_info["importance"]].append(file_info["relative_path"])
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")
    
    def _read_file_content(self, file_path: Path) -> str:
        """Ù‚Ø±Ø§Ø¡Ø© Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ø¨Ø£Ù…Ø§Ù†"""
        try:
            # Ù…Ø­Ø§ÙˆÙ„Ø© Ù‚Ø±Ø§Ø¡Ø© Ø¨ØªØ±Ù…ÙŠØ²Ø§Øª Ù…Ø®ØªÙ„ÙØ©
            encodings = ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']
            for encoding in encodings:
                try:
                    return file_path.read_text(encoding=encoding)
                except UnicodeDecodeError:
                    continue
            return ""
        # FIXME: replace with specific exception
except Exception as exc:return ""
    
    def _analyze_python_file(self, content: str, file_path: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Python"""
        analysis = {
            "is_python": True,
            "ast_analysis": {},
            "complexity": 0,
            "dependencies": []
        }
        
        try:
            tree = ast.parse(content)
            
            # ØªØ­Ù„ÙŠÙ„ AST
            imports = []
            classes = []
            functions = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        imports.append(node.module)
                elif isinstance(node, ast.ClassDef):
                    classes.append({
                        "name": node.name,
                        "methods": len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                        "has_docstring": ast.get_docstring(node) is not None
                    })
                elif isinstance(node, ast.FunctionDef):
                    if not self._is_method(node, tree):
                        functions.append({
                            "name": node.name,
                            "args": len(node.args.args),
                            "has_docstring": ast.get_docstring(node) is not None
                        })
            
            analysis["ast_analysis"] = {
                "imports": imports,
                "classes": classes,
                "functions": functions,
                "has_main": self._has_main_block(tree)
            }
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            analysis["complexity"] = len(classes) * 2 + len(functions) + len(imports) // 5
            analysis["dependencies"] = list(set(imports))
            
        except SyntaxError:
            analysis["ast_analysis"]["error"] = "Syntax error"
        except Exception as e:
            analysis["ast_analysis"]["error"] = str(e)
        
        return analysis
    
    def _is_method(self, node: ast.FunctionDef, tree: ast.AST) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ø¯Ø§Ù„Ø© Ù‡ÙŠ method Ø¯Ø§Ø®Ù„ class"""
        for parent in ast.walk(tree):
            if isinstance(parent, ast.ClassDef):
                if node in parent.body:
                    return True
        return False
    
    def _has_main_block(self, tree: ast.AST) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ if __name__ == '__main__'"""
        for node in ast.walk(tree):
            if isinstance(node, ast.If):
                if isinstance(node.test, ast.Compare):
                    if isinstance(node.test.left, ast.Name) and node.test.left.id == '__name__':
                        return True
        return False
    
    def _determine_file_type(self, file_path: Path, content: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ø¯Ù‚Ø©"""
        name_lower = file_path.name.lower()
        path_lower = str(file_path).lower()
        
        # Python files
        if file_path.suffix == '.py':
            if name_lower == '__init__.py':
                return 'init'
            elif name_lower in ['setup.py', 'manage.py']:
                return 'setup'
            elif name_lower in ['main.py', 'app.py', 'wsgi.py']:
                return 'entry_point'
            elif 'test' in path_lower:
                return 'test'
            elif 'migration' in path_lower:
                return 'migration'
            elif any(x in path_lower for x in ['model', 'entity', 'schema']):
                return 'model'
            elif 'service' in path_lower:
                return 'service'
            elif any(x in path_lower for x in ['repository', 'repo']):
                return 'repository'
            elif any(x in path_lower for x in ['controller', 'endpoint', 'handler', 'route', 'api']):
                return 'controller'
            elif any(x in path_lower for x in ['util', 'helper', 'tool']):
                return 'utility'
            elif 'script' in path_lower:
                return 'script'
            elif 'config' in path_lower:
                return 'config'
            else:
                return 'python_other'
        
        # Configuration files
        elif file_path.suffix in ['.json', '.yaml', '.yml', '.toml', '.ini', '.cfg']:
            return 'config'
        
        # Documentation
        elif file_path.suffix in ['.md', '.rst', '.txt']:
            return 'documentation'
        
        # Scripts
        elif file_path.suffix in ['.sh', '.bat', '.ps1']:
            return 'script'
        
        # Docker
        elif 'dockerfile' in name_lower:
            return 'docker'
        
        # SQL
        elif file_path.suffix == '.sql':
            return 'database'
        
        else:
            return 'other'
    
    def _determine_importance(self, file_path: Path, content: str, file_info: Dict) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù"""
        path_str = str(file_path).lower()
        
        # Ù‚Ù…Ø§Ù…Ø© - Ø§Ø­Ø°Ù ÙÙˆØ±Ø§Ù‹
        if len(content.strip()) == 0 and file_path.suffix != '.py':
            self.results["issues"]["empty_files"].append(str(file_path))
            return 'trash'
        
        # Ù…Ù„ÙØ§Øª __init__.py ÙØ§Ø±ØºØ© Ù…Ù‚Ø¨ÙˆÙ„Ø©
        if file_path.name == '__init__.py' and len(content.strip()) == 0:
            return 'low'
        
        # Ù…Ù„ÙØ§Øª Ù…Ø¤Ù‚ØªØ© Ø£Ùˆ Ù‚Ø¯ÙŠÙ…Ø©
        if any(re.match(pattern, path_str) for pattern in self.trash_patterns):
            return 'trash'
        
        if any(pattern in path_str for pattern in ['_old', '_backup', '_temp', '_copy', '.bak']):
            return 'trash'
        
        # Ø­Ø±Ø¬Ø© - Ù„Ø§ ØªØ­Ø°Ù Ø£Ø¨Ø¯Ø§Ù‹
        if file_path.name in ['main.py', 'app.py', 'wsgi.py', '__main__.py', 'manage.py']:
            return 'critical'
        
        if any(x in path_str for x in ['security', 'auth', 'permission', 'child_safety']):
            return 'critical'
        
        if any(pattern in path_str for pattern in self.critical_patterns):
            return 'critical'
        
        # Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        if file_info.get("is_python"):
            ast_data = file_info.get("ast_analysis", {})
            if len(ast_data.get("classes", [])) > 2 or len(ast_data.get("functions", [])) > 5:
                return 'high'
        
        if any(x in path_str for x in ['service', 'repository', 'controller', 'api']):
            return 'high'
        
        if file_info.get("type") in ['model', 'service', 'repository', 'controller']:
            return 'high'
        
        # Ù…Ù†Ø®ÙØ¶Ø© Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
        if file_info.get("is_python"):
            ast_data = file_info.get("ast_analysis", {})
            if (len(ast_data.get("classes", [])) == 0 and 
                len(ast_data.get("functions", [])) == 0 and
                file_path.name != '__init__.py'):
                return 'low'
        
        if 'example' in path_str or 'sample' in path_str or 'demo' in path_str:
            return 'low'
        
        # Ø§ÙØªØ±Ø§Ø¶ÙŠ
        return 'medium'
    
    def _find_issues(self, file_path: Path, content: str, file_info: Dict) -> List[Dict[str, str]]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        issues = []
        
        # Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©
        if file_info["size_bytes"] > 50 * 1024:  # Ø£ÙƒØ¨Ø± Ù…Ù† 50KB
            issues.append({
                "type": "size",
                "severity": "medium",
                "message": f"Ù…Ù„Ù ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹ ({file_info['size_bytes'] / 1024:.1f} KB)"
            })
            self.results["issues"]["large_files"].append({
                "path": str(file_path),
                "size_kb": file_info["size_bytes"] / 1024
            })
        
        # Python-specific issues
        if file_path.suffix == '.py':
            # Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ©
            security_patterns = [
                (r'eval\s*\(', "Ø§Ø³ØªØ®Ø¯Ø§Ù… eval() - Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ"),
                (r'exec\s*\(', "Ø§Ø³ØªØ®Ø¯Ø§Ù… exec() - Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ"),
                (r'pickle\.loads', "Ø§Ø³ØªØ®Ø¯Ø§Ù… pickle.loads - Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ"),
                (r'(password|secret|key|token)\s*=\s*["\'][^"\']+["\']', "ÙƒÙ„Ù…Ø© Ø³Ø± Ù…Ø¶Ù…Ù†Ø©"),
                (r'subprocess.*shell\s*=\s*True', "Ø§Ø³ØªØ®Ø¯Ø§Ù… shell=True - Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ")
            ]
            
            for pattern, message in security_patterns:
                if re.search(pattern, content, re.IGNORECASE):
                    issues.append({
                        "type": "security",
                        "severity": "high",
                        "message": message
                    })
                    self.results["issues"]["security_risks"].append({
                        "file": str(file_path),
                        "issue": message
                    })
            
            # Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø©
            if 'except:' in content or '# FIXME: replace with specific exception
except Exception as exc:' in content:
                issues.append({
                    "type": "quality",
                    "severity": "medium",
                    "message": "Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø¹Ø§Ù…Ø©"
                })
            
            if re.search(r'print\s*\(', content) and 'test' not in str(file_path):
                issues.append({
                    "type": "quality",
                    "severity": "low",
                    "message": "Ø§Ø³ØªØ®Ø¯Ø§Ù… print ÙÙŠ ÙƒÙˆØ¯ Ø§Ù„Ø¥Ù†ØªØ§Ø¬"
                })
            
            # TODOs
            todos = len(re.findall(r'(TODO|FIXME|XXX|HACK)', content))
            if todos > 0:
                issues.append({
                    "type": "quality",
                    "severity": "low",
                    "message": f"ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {todos} TODO/FIXME"
                })
        
        return issues
    
    def _suggest_location(self, file_path: Path, file_type: str) -> Optional[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø£ÙØ¶Ù„ Ù„Ù„Ù…Ù„Ù"""
        current = str(file_path.relative_to(self.project_root))
        
        # Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ÙŠ
        ideal_structure = {
            'model': 'src/core/domain/entities/',
            'service': 'src/core/services/',
            'repository': 'src/infrastructure/persistence/repositories/',
            'controller': 'src/api/endpoints/',
            'test': 'tests/',
            'config': 'configs/',
            'utility': 'src/shared/utils/',
            'script': 'scripts/',
            'documentation': 'docs/',
            'docker': 'deployments/docker/',
            'migration': 'src/infrastructure/persistence/migrations/'
        }
        
        ideal_location = ideal_structure.get(file_type)
        
        if not ideal_location:
            return None
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù„ÙŠØ³ ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­ Ø¨Ø§Ù„ÙØ¹Ù„
        if current.startswith(ideal_location):
            return None
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ø¬Ø¯ÙŠØ¯
        new_path = ideal_location + file_path.name
        
        # Ø¥Ø¶Ø§ÙØ© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ²Ø§Ø­Ø©
        self.results["issues"]["misplaced_files"].append({
            "current": current,
            "suggested": new_path
        })
        
        return new_path
    
    def _update_statistics(self, file_info: Dict) -> None:
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        summary = self.results["summary"]
        
        summary["total_files"] += 1
        summary["total_lines"] += file_info.get("lines", 0)
        summary["total_size_mb"] += file_info["size_bytes"] / (1024 * 1024)
        summary["files_by_type"][file_info["type"]] += 1
        summary["files_by_importance"][file_info["importance"]] += 1
    
    def _find_duplicates(self) -> None:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù€ hash
        hash_groups = defaultdict(list)
        
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("hash"):
                hash_groups[file_info["hash"]].append(file_info)
        
        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
        for file_hash, files in hash_groups.items():
            if len(files) > 1:
                self.results["duplicates"]["exact"].append({
                    "hash": file_hash,
                    "files": [f["relative_path"] for f in files],
                    "count": len(files),
                    "size_total_kb": sum(f["size_bytes"] for f in files) / 1024
                })
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ (Python files only)
        function_signatures = defaultdict(list)
        
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("is_python") and file_info.get("ast_analysis"):
                functions = file_info["ast_analysis"].get("functions", [])
                for func in functions:
                    sig = f"{func['name']}({func['args']})"
                    function_signatures[sig].append(file_info["relative_path"])
        
        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for sig, files in function_signatures.items():
            if len(files) > 1:
                self.results["duplicates"]["functional"].append({
                    "signature": sig,
                    "files": files,
                    "count": len(files)
                })
        
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(self.results['duplicates']['exact'])} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø©")
        print(f"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(self.results['duplicates']['functional'])} Ø¯Ø§Ù„Ø© Ù…ÙƒØ±Ø±Ø©")
    
    def _generate_recommendations(self) -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recs = self.results["recommendations"]
        
        # 1. ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø­Ø°Ù Ø§Ù„ÙÙˆØ±ÙŠ
        for file_path in self.results["classification"]["trash"]:
            recs["immediate_deletions"].append({
                "file": file_path,
                "reason": "Ù…Ù„Ù Ù‚Ù…Ø§Ù…Ø© (ÙØ§Ø±Øº/Ù‚Ø¯ÙŠÙ…/Ù…Ø¤Ù‚Øª)",
                "action": "DELETE"
            })
        
        # 2. ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù†Ù‚Ù„
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("suggested_location"):
                recs["suggested_moves"].append({
                    "from": file_info["relative_path"],
                    "to": file_info["suggested_location"],
                    "type": file_info["type"],
                    "reason": "Ù…ÙˆÙ‚Ø¹ ØºÙŠØ± Ù…Ù†Ø§Ø³Ø¨"
                })
        
        # 3. ØªÙˆØµÙŠØ§Øª Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„Ø©
        for file_info in self.results["detailed_analysis"]:
            if file_info.get("lines", 0) > 500:
                recs["refactoring_needed"].append({
                    "file": file_info["relative_path"],
                    "lines": file_info["lines"],
                    "reason": "Ù…Ù„Ù ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹",
                    "suggestion": "ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø£ØµØºØ±"
                })
        
        # 4. Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ø¯Ù…Ø¬
        for dup_group in self.results["duplicates"]["exact"]:
            if dup_group["count"] > 1:
                recs["merge_candidates"].append({
                    "files": dup_group["files"],
                    "action": "MERGE",
                    "reason": "Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹"
                })
        
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(recs['immediate_deletions'])} ØªÙˆØµÙŠØ© Ø­Ø°Ù")
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(recs['suggested_moves'])} ØªÙˆØµÙŠØ© Ù†Ù‚Ù„")
        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ {len(recs['refactoring_needed'])} ØªÙˆØµÙŠØ© Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø©")
    
    def _calculate_health_scores(self) -> None:
        """Ø­Ø³Ø§Ø¨ Ù…Ù‚Ø§ÙŠÙŠØ³ ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        total_files = self.results["summary"]["total_files"]
        
        if total_files == 0:
            return
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙ†Ø¸ÙŠÙ…
        misplaced_ratio = len(self.results["issues"]["misplaced_files"]) / total_files
        trash_ratio = len(self.results["classification"]["trash"]) / total_files
        organization_score = max(0, 100 - (misplaced_ratio * 50) - (trash_ratio * 30))
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_issues = sum(1 for f in self.results["detailed_analysis"] 
                           if any(i["type"] == "quality" for i in f.get("issues", [])))
        quality_ratio = quality_issues / total_files
        quality_score = max(0, 100 - (quality_ratio * 60))
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„Ø£Ù…Ø§Ù†
        security_issues = len(self.results["issues"]["security_risks"])
        security_ratio = security_issues / total_files
        security_score = max(0, 100 - (security_ratio * 100))
        
        # Ù†Ù‚Ø§Ø· Ø§Ù„ØªÙˆØ«ÙŠÙ‚
        documented_files = sum(1 for f in self.results["detailed_analysis"]
                             if f.get("is_python") and 
                             any(c.get("has_docstring") for c in f.get("ast_analysis", {}).get("classes", [])))
        doc_ratio = documented_files / max(1, len([f for f in self.results["detailed_analysis"] if f.get("is_python")]))
        documentation_score = doc_ratio * 100
        
        # Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©
        overall_score = (organization_score * 0.3 + 
                        quality_score * 0.3 + 
                        security_score * 0.3 + 
                        documentation_score * 0.1)
        
        self.results["health_score"] = {
            "overall": round(overall_score, 1),
            "organization": round(organization_score, 1),
            "quality": round(quality_score, 1),
            "security": round(security_score, 1),
            "documentation": round(documentation_score, 1)
        }
    
    def _print_quick_summary(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø³Ø±ÙŠØ¹"""
        print("\n" + "="*60)
        print("ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø±ÙŠØ¹")
        print("="*60)
        
        summary = self.results["summary"]
        health = self.results["health_score"]
        
        print(f"\nğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©:")
        print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {summary['total_files']}")
        print(f"   â€¢ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {summary['total_lines']:,}")
        print(f"   â€¢ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {summary['total_size_mb']:.2f} MB")
        
        print(f"\nğŸ¥ ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹:")
        print(f"   â€¢ Ø§Ù„Ù†Ù‚Ø§Ø· Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©: {health['overall']}%")
        print(f"   â€¢ Ø§Ù„ØªÙ†Ø¸ÙŠÙ…: {health['organization']}%")
        print(f"   â€¢ Ø§Ù„Ø¬ÙˆØ¯Ø©: {health['quality']}%")
        print(f"   â€¢ Ø§Ù„Ø£Ù…Ø§Ù†: {health['security']}%")
        print(f"   â€¢ Ø§Ù„ØªÙˆØ«ÙŠÙ‚: {health['documentation']}%")
        
        print(f"\nğŸ“ ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª:")
        for importance, count in self.results["summary"]["files_by_importance"].items():
            print(f"   â€¢ {importance}: {count} Ù…Ù„Ù")
        
        print(f"\nğŸš¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:")
        print(f"   â€¢ Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ©: {len(self.results['issues']['empty_files'])}")
        print(f"   â€¢ Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©: {len(self.results['issues']['large_files'])}")
        print(f"   â€¢ Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ©: {len(self.results['issues']['security_risks'])}")
        print(f"   â€¢ Ù…Ù„ÙØ§Øª ÙÙŠ Ù…ÙƒØ§Ù† Ø®Ø§Ø·Ø¦: {len(self.results['issues']['misplaced_files'])}")
        
        print(f"\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        recs = self.results["recommendations"]
        print(f"   â€¢ Ù…Ù„ÙØ§Øª Ù„Ù„Ø­Ø°Ù: {len(recs['immediate_deletions'])}")
        print(f"   â€¢ Ù…Ù„ÙØ§Øª Ù„Ù„Ù†Ù‚Ù„: {len(recs['suggested_moves'])}")
        print(f"   â€¢ Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø©: {len(recs['refactoring_needed'])}")
        print(f"   â€¢ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù„Ù„Ø¯Ù…Ø¬: {len(recs['merge_candidates'])}")
    
    def save_reports(self, output_dir: str = "cleanup_reports") -> None:
        """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø¨ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ø®ØªÙ„ÙØ©"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Ø­ÙØ¸ JSON Ù…ÙØµÙ„
        json_file = output_path / f"comprehensive_analysis_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2)
        print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± JSON: {json_file}")
        
        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Markdown
        md_file = output_path / f"cleanup_report_{timestamp}.md"
        self._save_markdown_report(md_file)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Markdown: {md_file}")
        
        # Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
        actions_file = output_path / f"cleanup_actions_{timestamp}.txt"
        self._save_action_list(actions_file)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª: {actions_file}")
    
    def _save_markdown_report(self, file_path: Path) -> None:
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Markdown Ø´Ø§Ù…Ù„"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# ğŸ§¹ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear\n\n")
            f.write(f"**Ø§Ù„ØªØ§Ø±ÙŠØ®**: {self.results['metadata']['timestamp']}\n")
            f.write(f"**Ø§Ù„Ù…Ø­Ù„Ù„**: Comprehensive Cleanup Analyzer v{self.results['metadata']['analyzer_version']}\n\n")
            
            # Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ
            f.write("## ğŸ“Š Ù…Ù„Ø®Øµ ØªÙ†ÙÙŠØ°ÙŠ\n\n")
            
            health = self.results["health_score"]
            f.write(f"**ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©**: {health['overall']}% ")
            
            if health['overall'] >= 80:
                f.write("âœ… Ù…Ù…ØªØ§Ø²\n\n")
            elif health['overall'] >= 60:
                f.write("âš ï¸ ÙŠØ­ØªØ§Ø¬ ØªØ­Ø³ÙŠÙ†\n\n")
            else:
                f.write("âŒ ÙŠØ­ØªØ§Ø¬ Ø¹Ù†Ø§ÙŠØ© ÙÙˆØ±ÙŠØ©\n\n")
            
            # Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØµØ­Ø©
            f.write("### ğŸ¥ Ù…Ù‚Ø§ÙŠÙŠØ³ Ø§Ù„ØµØ­Ø©\n\n")
            f.write("| Ø§Ù„Ù…Ù‚ÙŠØ§Ø³ | Ø§Ù„Ù†Ù‚Ø§Ø· | Ø§Ù„ØªÙ‚ÙŠÙŠÙ… |\n")
            f.write("|---------|--------|----------|\n")
            
            metrics = [
                ("Ø§Ù„ØªÙ†Ø¸ÙŠÙ…", health['organization']),
                ("Ø§Ù„Ø¬ÙˆØ¯Ø©", health['quality']),
                ("Ø§Ù„Ø£Ù…Ø§Ù†", health['security']),
                ("Ø§Ù„ØªÙˆØ«ÙŠÙ‚", health['documentation'])
            ]
            
            for metric, score in metrics:
                if score >= 80:
                    status = "âœ…"
                elif score >= 60:
                    status = "âš ï¸"
                else:
                    status = "âŒ"
                f.write(f"| {metric} | {score}% | {status} |\n")
            
            # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            f.write("\n## ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©\n\n")
            summary = self.results["summary"]
            f.write(f"- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª**: {summary['total_files']}\n")
            f.write(f"- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±**: {summary['total_lines']:,}\n")
            f.write(f"- **Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ**: {summary['total_size_mb']:.2f} MB\n\n")
            
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
            f.write("### ğŸ“ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹\n\n")
            f.write("| Ø§Ù„Ù†ÙˆØ¹ | Ø§Ù„Ø¹Ø¯Ø¯ | Ø§Ù„Ù†Ø³Ø¨Ø© |\n")
            f.write("|-------|-------|--------|\n")
            
            total = summary['total_files']
            for file_type, count in sorted(summary['files_by_type'].items(), key=lambda x: x[1], reverse=True):
                percentage = (count / total) * 100 if total > 0 else 0
                f.write(f"| {file_type} | {count} | {percentage:.1f}% |\n")
            
            # ØªØµÙ†ÙŠÙ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            f.write("\n### ğŸ¯ ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©\n\n")
            importance_map = {
                'critical': 'ğŸ”´ Ø­Ø±Ø¬Ø©',
                'high': 'ğŸŸ  Ø¹Ø§Ù„ÙŠØ©',
                'medium': 'ğŸŸ¡ Ù…ØªÙˆØ³Ø·Ø©',
                'low': 'ğŸŸ¢ Ù…Ù†Ø®ÙØ¶Ø©',
                'trash': 'âš« Ù‚Ù…Ø§Ù…Ø©'
            }
            
            for importance, label in importance_map.items():
                count = summary['files_by_importance'].get(importance, 0)
                f.write(f"- **{label}**: {count} Ù…Ù„Ù\n")
            
            # Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            f.write("\n## ğŸš¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©\n\n")
            
            issues = self.results["issues"]
            
            # Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ©
            if issues["empty_files"]:
                f.write(f"### ğŸ“„ Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ© ({len(issues['empty_files'])})\n\n")
                for file in issues["empty_files"][:10]:
                    f.write(f"- `{file}`\n")
                if len(issues["empty_files"]) > 10:
                    f.write(f"- ... Ùˆ {len(issues['empty_files']) - 10} Ù…Ù„Ù Ø¢Ø®Ø±\n")
            
            # Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©
            if issues["large_files"]:
                f.write(f"\n### ğŸ“¦ Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© ({len(issues['large_files'])})\n\n")
                for file in sorted(issues["large_files"], key=lambda x: x['size_kb'], reverse=True)[:10]:
                    f.write(f"- `{file['path']}` ({file['size_kb']:.1f} KB)\n")
            
            # Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ©
            if issues["security_risks"]:
                f.write(f"\n### ğŸ” Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ© ({len(issues['security_risks'])})\n\n")
                for risk in issues["security_risks"][:10]:
                    f.write(f"- `{risk['file']}`: {risk['issue']}\n")
            
            # Ù…Ù„ÙØ§Øª ÙÙŠ Ù…ÙƒØ§Ù† Ø®Ø§Ø·Ø¦
            if issues["misplaced_files"]:
                f.write(f"\n### ğŸ“ Ù…Ù„ÙØ§Øª ÙÙŠ Ù…ÙƒØ§Ù† Ø®Ø§Ø·Ø¦ ({len(issues['misplaced_files'])})\n\n")
                f.write("| Ø§Ù„Ù…Ù„Ù | Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ø­Ø§Ù„ÙŠ | Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù‚ØªØ±Ø­ |\n")
                f.write("|-------|----------------|------------------|\n")
                for misplaced in issues["misplaced_files"][:20]:
                    current = misplaced['current']
                    suggested = misplaced['suggested']
                    f.write(f"| {Path(current).name} | `{Path(current).parent}` | `{Path(suggested).parent}` |\n")
            
            # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
            f.write("\n## ğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©\n\n")
            
            duplicates = self.results["duplicates"]
            
            if duplicates["exact"]:
                f.write(f"### ğŸ“‘ Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ ({len(duplicates['exact'])} Ù…Ø¬Ù…ÙˆØ¹Ø©)\n\n")
                for idx, dup_group in enumerate(duplicates["exact"][:10], 1):
                    f.write(f"**Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© {idx}** ({dup_group['count']} Ù…Ù„ÙØ§ØªØŒ {dup_group['size_total_kb']:.1f} KB Ø¥Ø¬Ù…Ø§Ù„ÙŠ):\n")
                    for file in dup_group['files']:
                        f.write(f"- `{file}`\n")
                    f.write("\n")
            
            # Ø§Ù„ØªÙˆØµÙŠØ§Øª
            f.write("## ğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª ÙˆØ§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª\n\n")
            
            recs = self.results["recommendations"]
            
            # Ø­Ø°Ù ÙÙˆØ±ÙŠ
            if recs["immediate_deletions"]:
                f.write(f"### ğŸ—‘ï¸ Ù…Ù„ÙØ§Øª Ù„Ù„Ø­Ø°Ù Ø§Ù„ÙÙˆØ±ÙŠ ({len(recs['immediate_deletions'])})\n\n")
                f.write("âš ï¸ **ØªØ­Ø°ÙŠØ±**: Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù„ÙØ§Øª ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡Ø§ Ø¨Ø£Ù…Ø§Ù†\n\n")
                
                for deletion in recs["immediate_deletions"][:20]:
                    f.write(f"- `{deletion['file']}` - {deletion['reason']}\n")
                
                if len(recs["immediate_deletions"]) > 20:
                    f.write(f"\n... Ùˆ {len(recs['immediate_deletions']) - 20} Ù…Ù„Ù Ø¢Ø®Ø±\n")
                
                # Ø£Ù…Ø± Ø§Ù„Ø­Ø°Ù
                f.write("\n```bash\n# Ø£Ù…Ø± Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª (ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ù…Ù„ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø£ÙˆÙ„Ø§Ù‹)\n")
                for deletion in recs["immediate_deletions"][:5]:
                    f.write(f'rm "{deletion["file"]}"\n')
                f.write("```\n")
            
            # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª
            if recs["suggested_moves"]:
                f.write(f"\n### ğŸ“¦ Ù…Ù„ÙØ§Øª Ù„Ù„Ù†Ù‚Ù„ ({len(recs['suggested_moves'])})\n\n")
                
                # ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
                moves_by_type = defaultdict(list)
                for move in recs["suggested_moves"]:
                    moves_by_type[move['type']].append(move)
                
                for file_type, moves in moves_by_type.items():
                    f.write(f"\n**{file_type.title()} Files ({len(moves)})**:\n")
                    for move in moves[:5]:
                        f.write(f"- `{move['from']}` â†’ `{move['to']}`\n")
            
            # Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø©
            if recs["refactoring_needed"]:
                f.write(f"\n### ğŸ”§ Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© ({len(recs['refactoring_needed'])})\n\n")
                for refactor in recs["refactoring_needed"][:10]:
                    f.write(f"- `{refactor['file']}` ({refactor['lines']} Ø³Ø·Ø±) - {refactor['suggestion']}\n")
            
            # Ø®Ø·Ø© Ø§Ù„Ø¹Ù…Ù„
            f.write("\n## ğŸ“‹ Ø®Ø·Ø© Ø§Ù„Ø¹Ù…Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\n\n")
            f.write("### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙÙˆØ±ÙŠ (ÙŠÙˆÙ… 1)\n")
            f.write(f"1. Ø­Ø°Ù {len(recs['immediate_deletions'])} Ù…Ù„Ù Ù‚Ù…Ø§Ù…Ø©\n")
            f.write(f"2. Ø¯Ù…Ø¬ {len(duplicates['exact'])} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø©\n")
            f.write("3. Ø¹Ù…Ù„ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ù‚Ø¨Ù„ Ø£ÙŠ ØªØºÙŠÙŠØ±\n\n")
            
            f.write("### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„ØªÙ†Ø¸ÙŠÙ… (ÙŠÙˆÙ… 2-3)\n")
            f.write(f"1. Ù†Ù‚Ù„ {len(recs['suggested_moves'])} Ù…Ù„Ù Ù„Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„ØµØ­ÙŠØ­Ø©\n")
            f.write("2. ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ imports Ø§Ù„Ù…ØªØ£Ø«Ø±Ø©\n")
            f.write("3. Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ù…Ù„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª\n\n")
            
            f.write("### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„Ø© (ÙŠÙˆÙ… 4-5)\n")
            f.write(f"1. ØªÙ‚Ø³ÙŠÙ… {len(recs['refactoring_needed'])} Ù…Ù„Ù ÙƒØ¨ÙŠØ±\n")
            f.write(f"2. Ø¥ØµÙ„Ø§Ø­ {len(issues['security_risks'])} Ù…Ø´ÙƒÙ„Ø© Ø£Ù…Ù†ÙŠØ©\n")
            f.write("3. ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙˆØ«ÙŠÙ‚ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª\n\n")
            
            # Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
            f.write("## ğŸ¯ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙ†Ø¸ÙŠÙ\n\n")
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
            files_after = summary['total_files'] - len(recs['immediate_deletions'])
            size_saved = sum(f['size_bytes'] for f in self.results['detailed_analysis'] 
                           if f['relative_path'] in [d['file'] for d in recs['immediate_deletions']]) / (1024 * 1024)
            
            f.write(f"- **Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª**: {summary['total_files']} â†’ {files_after} (â¬‡ï¸ {len(recs['immediate_deletions'])})\n")
            f.write(f"- **Ø§Ù„Ø­Ø¬Ù…**: {summary['total_size_mb']:.2f} MB â†’ {summary['total_size_mb'] - size_saved:.2f} MB (â¬‡ï¸ {size_saved:.2f} MB)\n")
            f.write(f"- **ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹**: {health['overall']}% â†’ ~90% (â¬†ï¸ {90 - health['overall']:.0f}%)\n")
            f.write(f"- **Ù‡ÙŠÙƒÙ„ Ø£ÙˆØ¶Ø­ ÙˆØ£Ø³Ù‡Ù„ Ù„Ù„ØµÙŠØ§Ù†Ø©**\n")
            f.write(f"- **Ø£Ù…Ø§Ù† Ù…Ø­Ø³Ù‘Ù† ÙˆØ£Ø¯Ø§Ø¡ Ø£ÙØ¶Ù„**\n")
    
    def _save_action_list(self, file_path: Path) -> None:
        """Ø­ÙØ¸ Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø³Ø±ÙŠØ¹Ø©"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø© - Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear\n")
            f.write("=" * 50 + "\n\n")
            
            f.write("1. Ù…Ù„ÙØ§Øª Ù„Ù„Ø­Ø°Ù Ø§Ù„ÙÙˆØ±ÙŠ:\n")
            f.write("-" * 30 + "\n")
            
            for deletion in self.results["recommendations"]["immediate_deletions"]:
                f.write(f'DEL "{deletion["file"]}"\n')
            
            f.write("\n2. Ù…Ù„ÙØ§Øª Ù„Ù„Ù†Ù‚Ù„:\n")
            f.write("-" * 30 + "\n")
            
            for move in self.results["recommendations"]["suggested_moves"]:
                f.write(f'MOVE "{move["from"]}" -> "{move["to"]}"\n')
            
            f.write("\n3. Ù…Ù„ÙØ§Øª Ù„Ù„Ø¯Ù…Ø¬:\n")
            f.write("-" * 30 + "\n")
            
            for merge in self.results["recommendations"]["merge_candidates"]:
                f.write(f'MERGE: {", ".join(merge["files"])}\n')


def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø´Ø§Ù…Ù„"""
    print("\nğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø´Ø§Ù…Ù„...")
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­Ù„Ù„
    analyzer = ComprehensiveCleanupAnalyzer()
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
    results = analyzer.analyze_project()
    
    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
    analyzer.save_reports()
    
    print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
    print("ğŸ“‹ ØªØ­Ù‚Ù‚ Ù…Ù† Ù…Ø¬Ù„Ø¯ cleanup_reports Ù„Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„Ø©")


if __name__ == "__main__":
    main() 