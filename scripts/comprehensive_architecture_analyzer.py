from typing import Dict, List
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import os
import hashlib
import logging

logger = logging.getLogger(__name__)

"""
Comprehensive Architecture Analyzer for AI-TEDDY-BEAR
Ø£Ø¯Ø§Ø© ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„Ø© Ù„Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø­Ø³Ø¨ Clean Architecture
"""


class ArchitectureAnalyzer:

    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.analysis_report = {
            "timestamp": datetime.now().isoformat(),
            "project_name": "AI-TEDDY-BEAR",
            "duplicates": {
                "files": [],
                "directories": [],
                "services": [],
                "configs": [],
            },
            "evaluation": {},
            "classification": {
                "KEEP": [],
                "MERGE": [],
                "DEPRECATED": [],
                "INCOMPLETE": [],
            },
            "merge_strategy": {},
            "proposed_structure": {},
            "statistics": {
                "total_files": 0,
                "total_duplicates": 0,
                "architecture_violations": 0,
                "clean_score": 0,
            },
        }

    def scan_for_duplicates(self) -> Dict:
        """ÙØ­Øµ Ø´Ø§Ù…Ù„ Ù„Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        logger.info("ğŸ” Ø¨Ø¯Ø¡ ÙØ­Øµ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ø´Ø§Ù…Ù„...")
        file_hashes = defaultdict(list)
        file_names = defaultdict(list)
        service_files = defaultdict(list)
        config_files = defaultdict(list)
        exclude_dirs = {
            ".git",
            "__pycache__",
            ".mypy_cache",
            "node_modules",
            "deleted",
            "deprecated",
            ".pytest_cache",
        }
        for root, dirs, files in os.walk(self.base_path):
            dirs[:] = [d for d in dirs if d not in exclude_dirs]
            for file in files:
                file_path = Path(root) / file
                relative_path = file_path.relative_to(self.base_path)
                if file.endswith((".pyc", ".pyo", ".log", ".tmp", ".cache")):
                    continue
                self.analysis_report["statistics"]["total_files"] += 1
                file_hash = self.calculate_file_hash(file_path)
                if file_hash:
                    file_hashes[file_hash].append(str(relative_path))
                file_names[file].append(str(relative_path))
                if "service" in file.lower() and file.endswith(".py"):
                    service_type = self.extract_service_type(file)
                    service_files[service_type].append(str(relative_path))
                if any(
                    config_indicator in file.lower()
                    for config_indicator in [
                        "config",
                        "setting",
                        "env",
                        ".json",
                        ".yaml",
                        ".yml",
                    ]
                ):
                    config_type = self.extract_config_type(file)
                    config_files[config_type].append(str(relative_path))
        self._analyze_hash_duplicates(file_hashes)
        self._analyze_name_duplicates(file_names)
        self._analyze_service_duplicates(service_files)
        self._analyze_config_duplicates(config_files)
        return self.analysis_report["duplicates"]

    def calculate_file_hash(self, file_path: Path) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù"""
        try:
            if file_path.stat().st_size > 10 * 1024 * 1024:
                return ""
            with open(file_path, "rb") as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception as exc:
            return ""

    def extract_service_type(self, filename: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø© Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù"""
        service_patterns = {
            "openai": ["openai", "gpt", "chatgpt"],
            "audio": ["audio", "voice", "speech", "tts", "stt"],
            "ai": ["ai_service", "ai_processor", "ai_handler"],
            "config": ["config_service", "configuration"],
            "database": ["db_service", "database", "repository"],
            "auth": ["auth", "authentication", "security"],
            "websocket": ["websocket", "ws_service", "realtime"],
        }
        filename_lower = filename.lower()
        for service_type, patterns in service_patterns.items():
            if any(pattern in filename_lower for pattern in patterns):
                return service_type
        return "other"

    def extract_config_type(self, filename: str) -> str:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù†ÙˆØ¹ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù…Ù† Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù"""
        if "package" in filename.lower():
            return "package"
        elif "docker" in filename.lower():
            return "docker"
        elif "env" in filename.lower():
            return "environment"
        elif "api" in filename.lower():
            return "api_config"
        elif filename.endswith(".json"):
            return "json_config"
        elif filename.endswith((".yaml", ".yml")):
            return "yaml_config"
        return "general"

    def _analyze_hash_duplicates(self, file_hashes: Dict):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹"""
        for file_hash, files in file_hashes.items():
            if len(files) > 1:
                self.analysis_report["duplicates"]["files"].append(
                    {
                        "hash": file_hash,
                        "files": files,
                        "type": "identical_content",
                        "action_needed": "remove_duplicates",
                    }
                )
                self.analysis_report["statistics"]["total_duplicates"] += len(
                    files) - 1

    def _analyze_name_duplicates(self, file_names: Dict):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„Ø£Ø³Ù…Ø§Ø¡"""
        for filename, paths in file_names.items():
            if len(paths) > 1 and not filename.startswith("."):
                self.analysis_report["duplicates"]["files"].append(
                    {
                        "filename": filename,
                        "files": paths,
                        "type": "similar_name",
                        "action_needed": "review_and_merge",
                    }
                )

    def _analyze_service_duplicates(self, service_files: Dict):
        """ØªØ­Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ø§Ù„Ø®Ø¯Ù…Ø§Øª"""
        for service_type, files in service_files.items():
            if len(files) > 1:
                self.analysis_report["duplicates"]["services"].append(
                    {
                        "service_type": service_type,
                        "files": files,
                        "count": len(files),
                        "action_needed": "consolidate_service",
                    }
                )

    def _analyze_config_duplicates(self, config_files: Dict):
        """ØªØ­Ù„ÙŠÙ„ ØªÙƒØ±Ø§Ø± Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        for config_type, files in config_files.items():
            if len(files) > 1:
                self.analysis_report["duplicates"]["configs"].append(
                    {
                        "config_type": config_type,
                        "files": files,
                        "count": len(files),
                        "action_needed": "merge_configs",
                    }
                )

    def evaluate_files(self) -> Dict:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© ÙˆØ£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„ÙØ§Øª (1-10)"""
        logger.info("ğŸ“Š ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ù„ÙØ§Øª...")
        critical_files = [
            "main.py",
            "app.py",
            "__init__.py",
            "config.py",
            "requirements.txt",
            "package.json",
            "docker-compose.yml",
        ]
        for duplicate_group in self.analysis_report["duplicates"]["files"]:
            if duplicate_group["type"] == "identical_content":
                for file_path in duplicate_group["files"]:
                    score = self._calculate_file_score(
                        file_path, critical_files)
                    self.analysis_report["evaluation"][file_path] = {
                        "quality_score": score["quality"],
                        "importance_score": score["importance"],
                        "recency_score": score["recency"],
                        "total_score": score["total"],
                        "recommendation": score["recommendation"],
                    }
        return self.analysis_report["evaluation"]

    def _calculate_file_score(self, file_path: str, critical_files: List[str]) -> Dict:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ø§Ù„Ù…Ù„Ù"""
        path_obj = Path(file_path)
        quality_score = 5
        if "test" in file_path.lower():
            quality_score += 2
        if "deprecated" in file_path.lower() or "old" in file_path.lower():
            quality_score -= 3
        if path_obj.suffix == ".py":
            quality_score += 1
        importance_score = 5
        if any(critical in path_obj.name for critical in critical_files):
            importance_score += 3
        if "src/" in file_path or "application/" in file_path:
            importance_score += 2
        if "deprecated/" in file_path or "backup/" in file_path:
            importance_score -= 4
        recency_score = 7
        try:
            file_stat = Path(self.base_path / file_path).stat()
            recency_score = min(10, max(1, recency_score))
        except Exception as exc:
            recency_score = 5
        total_score = (quality_score + importance_score + recency_score) / 3
        if total_score >= 8:
            recommendation = "KEEP"
        elif total_score >= 6:
            recommendation = "REVIEW"
        elif total_score >= 4:
            recommendation = "MERGE"
        else:
            recommendation = "DEPRECATED"
        return {
            "quality": min(10, max(1, quality_score)),
            "importance": min(10, max(1, importance_score)),
            "recency": min(10, max(1, recency_score)),
            "total": round(total_score, 2),
            "recommendation": recommendation,
        }

    def classify_files(self) -> Dict:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        logger.info("ğŸ“‚ ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª...")
        for file_path, evaluation in self.analysis_report["evaluation"].items():
            category = evaluation["recommendation"]
            if category == "REVIEW":
                category = "MERGE"
            self.analysis_report["classification"][category].append(
                {
                    "file": file_path,
                    "score": evaluation["total_score"],
                    "reason": self._get_classification_reason(evaluation),
                }
            )
        for category in self.analysis_report["classification"]:
            self.analysis_report["classification"][category].sort(
                key=lambda x: x["score"], reverse=True
            )
        return self.analysis_report["classification"]

    def _get_classification_reason(self, evaluation: Dict) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¨Ø¨ Ø§Ù„ØªØµÙ†ÙŠÙ"""
        score = evaluation["total_score"]
        if score >= 8:
            return "High quality, critical importance, keep as-is"
        elif score >= 6:
            return "Good quality, consider merging with similar files"
        elif score >= 4:
            return "Medium quality, merge or refactor needed"
        else:
            return "Low quality or deprecated, move to deprecated folder"

    def propose_merge_strategy(self) -> Dict:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯Ù…Ø¬"""
        logger.info("ğŸ”„ Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯Ù…Ø¬...")
        for service_group in self.analysis_report["duplicates"]["services"]:
            service_type = service_group["service_type"]
            files = service_group["files"]
            best_file = self._select_best_file(files)
            other_files = [f for f in files if f != best_file]
            self.analysis_report["merge_strategy"][service_type] = {
                "primary_file": best_file,
                "files_to_merge": other_files,
                "merge_approach": self._get_merge_approach(service_type),
                "unique_features": self._extract_unique_features(files),
            }
        for config_group in self.analysis_report["duplicates"]["configs"]:
            config_type = config_group["config_type"]
            files = config_group["files"]
            best_file = self._select_best_file(files)
            other_files = [f for f in files if f != best_file]
            self.analysis_report["merge_strategy"][f"config_{config_type}"] = {
                "primary_file": best_file,
                "files_to_merge": other_files,
                "merge_approach": "merge_configurations",
                "validation_needed": True,
            }
        return self.analysis_report["merge_strategy"]

    def _select_best_file(self, files: List[str]) -> str:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        best_score = 0
        best_file = files[0]
        for file_path in files:
            if file_path in self.analysis_report["evaluation"]:
                score = self.analysis_report["evaluation"][file_path]["total_score"]
                if score > best_score:
                    best_score = score
                    best_file = file_path
            elif "src/" in file_path:
                best_file = file_path
        return best_file

    def _get_merge_approach(self, service_type: str) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†Ù‡Ø¬ Ø§Ù„Ø¯Ù…Ø¬ Ù„Ù„Ø®Ø¯Ù…Ø©"""
        approaches = {
            "openai": "consolidate_openai_clients",
            "audio": "merge_audio_processing",
            "ai": "unify_ai_services",
            "config": "merge_configuration_services",
            "database": "consolidate_repositories",
            "auth": "unify_authentication",
            "websocket": "merge_realtime_services",
        }
        return approaches.get(service_type, "generic_service_merge")

    def _extract_unique_features(self, files: List[str]) -> List[str]:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª"""
        features = []
        for file_path in files:
            if "async" in file_path.lower():
                features.append("async_support")
            if "streaming" in file_path.lower():
                features.append("streaming_capability")
            if "enterprise" in file_path.lower():
                features.append("enterprise_features")
        return list(set(features))

    def propose_clean_architecture(self) -> Dict:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ù‡ÙŠÙƒÙ„ Clean Architecture Ù†Ù‡Ø§Ø¦ÙŠ"""
        logger.info("ğŸ—ï¸ Ø§Ù‚ØªØ±Ø§Ø­ Ù‡ÙŠÙƒÙ„ Clean Architecture...")
        proposed_structure = {
            "src/": {
                "domain/": {
                    "entities/": ["child.py", "conversation.py", "voice_command.py"],
                    "value_objects/": ["age.py", "voice_data.py", "device_id.py"],
                    "repositories/": [
                        "child_repository.py",
                        "conversation_repository.py",
                    ],
                    "services/": ["domain_services.py"],
                    "events/": ["domain_events.py"],
                },
                "application/": {
                    "use_cases/": [
                        "process_voice_command.py",
                        "manage_conversation.py",
                    ],
                    "services/": ["ai_orchestrator.py", "conversation_service.py"],
                    "interfaces/": ["ai_service_interface.py", "audio_interface.py"],
                    "dto/": ["voice_command_dto.py", "response_dto.py"],
                    "handlers/": ["command_handlers.py", "event_handlers.py"],
                },
                "infrastructure/": {
                    "ai/": ["openai_service.py", "ai_safety_service.py"],
                    "audio/": [
                        "audio_processor.py",
                        "tts_service.py",
                        "stt_service.py",
                    ],
                    "persistence/": ["database_repository.py", "cache_repository.py"],
                    "external_services/": ["cloud_api.py", "device_api.py"],
                    "security/": ["authentication.py", "encryption.py"],
                    "messaging/": ["websocket_handler.py", "event_bus.py"],
                },
                "presentation/": {
                    "api/": ["endpoints/", "websocket/"],
                    "web/": ["dashboard/", "admin_panel/"],
                    "cli/": ["management_commands.py"],
                },
            },
            "config/": {
                "environments/": ["development.json", "production.json"],
                "schemas/": ["config_schema.json"],
                "api_keys.json.example": None,
            },
            "tests/": {
                "unit/": ["domain/", "application/", "infrastructure/"],
                "integration/": ["api_tests/", "service_tests/"],
                "e2e/": ["full_journey_tests/"],
            },
            "docs/": {
                "architecture/": ["clean_architecture.md", "api_docs.md"],
                "deployment/": ["docker_guide.md", "k8s_guide.md"],
            },
            "deprecated/": {
                "old_implementations/": [],
                "legacy_code/": [],
                "reports/": [],
            },
        }
        self.analysis_report["proposed_structure"] = proposed_structure
        return proposed_structure

    def calculate_clean_score(self) -> int:
        """Ø­Ø³Ø§Ø¨ Ù†Ù‚Ø§Ø· Ù†Ø¸Ø§ÙØ© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ (0-100)"""
        total_files = self.analysis_report["statistics"]["total_files"]
        total_duplicates = self.analysis_report["statistics"]["total_duplicates"]
        if total_files == 0:
            return 0
        base_score = 50
        duplication_penalty = min(40, total_duplicates / total_files * 100)
        structure_bonus = 0
        if any("domain/" in str(f) for f in Path(self.base_path).rglob("*.py")):
            structure_bonus += 10
        if any("application/" in str(f) for f in Path(self.base_path).rglob("*.py")):
            structure_bonus += 10
        if any("infrastructure/" in str(f) for f in Path(self.base_path).rglob("*.py")):
            structure_bonus += 10
        clean_score = int(base_score - duplication_penalty + structure_bonus)
        self.analysis_report["statistics"]["clean_score"] = max(
            0, min(100, clean_score)
        )
        return self.analysis_report["statistics"]["clean_score"]

    def generate_comprehensive_report(self) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        clean_score = self.calculate_clean_score()
        report = f"""
# ğŸ—ï¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø´Ø±ÙˆØ¹ AI-TEDDY-BEAR
**Ø§Ù„ØªØ§Ø±ÙŠØ®**: {self.analysis_report['timestamp']}
**Ø§Ù„Ù…Ø­Ù„Ù„**: ArchitectureAnalyzer Pro v2.0
**Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ø¸Ø§ÙØ©**: {clean_score}/100

## ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª**: {self.analysis_report['statistics']['total_files']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©**: {self.analysis_report['statistics']['total_duplicates']}
- **Ø§Ù†ØªÙ‡Ø§ÙƒØ§Øª Ø§Ù„Ø¨Ù†ÙŠØ©**: {len(self.analysis_report['duplicates']['services'])}
- **Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ø¸Ø§ÙØ©**: {clean_score}/100

## ğŸ” Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©

### ğŸ“„ Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
"""
        for duplicate in self.analysis_report["duplicates"]["files"]:
            if duplicate["type"] == "identical_content":
                report += f"""
**Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©**:
{chr(10).join(f'- `{f}`' for f in duplicate['files'])}
**Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡**: {duplicate['action_needed']}
"""
        report += "\n### ğŸ”§ Ø®Ø¯Ù…Ø§Øª Ù…ÙƒØ±Ø±Ø©\n"
        for service in self.analysis_report["duplicates"]["services"]:
            report += f"""
**Ù†ÙˆØ¹ Ø§Ù„Ø®Ø¯Ù…Ø©**: {service['service_type']}
**Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ø³Ø®**: {service['count']}
**Ø§Ù„Ù…Ù„ÙØ§Øª**: {', '.join(f'`{f}`' for f in service['files'])}
"""
        report += "\n## ğŸ“‚ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù‚ØªØ±Ø­\n\n### âœ… KEEP - Ø§Ø­ØªÙØ¸ Ø¨Ù‡Ø§\n"
        for item in self.analysis_report["classification"]["KEEP"]:
            report += f"- `{item['file']}` (Ù†Ù‚Ø§Ø·: {item['score']})\n"
        report += "\n### ğŸ”„ MERGE - Ø§Ø¯Ù…Ø¬Ù‡Ø§\n"
        for item in self.analysis_report["classification"]["MERGE"]:
            report += f"- `{item['file']}` (Ù†Ù‚Ø§Ø·: {item['score']})\n"
        report += "\n### ğŸ“¦ DEPRECATED - Ø§Ù†Ù‚Ù„Ù‡Ø§ Ù„Ù„Ù…Ù‡Ù…Ù„Ø§Øª\n"
        for item in self.analysis_report["classification"]["DEPRECATED"]:
            report += f"- `{item['file']}` (Ù†Ù‚Ø§Ø·: {item['score']})\n"
        report += "\n## ğŸ”„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©\n"
        for strategy_name, strategy in self.analysis_report["merge_strategy"].items():
            report += f"""
### {strategy_name}
- **Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**: `{strategy['primary_file']}`
- **Ù…Ù„ÙØ§Øª Ù„Ù„Ø¯Ù…Ø¬**: {', '.join(f'`{f}`' for f in strategy['files_to_merge'])}
- **Ù†Ù‡Ø¬ Ø§Ù„Ø¯Ù…Ø¬**: {strategy['merge_approach']}
"""
        report += f"""
## ğŸ—ï¸ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ Ø§Ù„Ù…Ù‚ØªØ±Ø­ (Clean Architecture)

```
src/
â”œâ”€â”€ domain/               # Ù…Ù†Ø·Ù‚ Ø§Ù„Ø£Ø¹Ù…Ø§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
â”‚   â”œâ”€â”€ entities/        # ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù†Ø·Ø§Ù‚
â”‚   â”œâ”€â”€ value_objects/   # ÙƒØ§Ø¦Ù†Ø§Øª Ø§Ù„Ù‚ÙŠÙ…
â”‚   â”œâ”€â”€ repositories/    # ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª
â”‚   â””â”€â”€ services/        # Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù†Ø·Ø§Ù‚
â”œâ”€â”€ application/         # Ù…Ù†Ø·Ù‚ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
â”‚   â”œâ”€â”€ use_cases/       # Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
â”‚   â”œâ”€â”€ services/        # Ø®Ø¯Ù…Ø§Øª Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
â”‚   â”œâ”€â”€ interfaces/      # ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ø®Ø¯Ù…Ø§Øª
â”‚   â””â”€â”€ dto/            # ÙƒØ§Ø¦Ù†Ø§Øª Ù†Ù‚Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”œâ”€â”€ infrastructure/      # Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªÙ‚Ù†ÙŠØ©
â”‚   â”œâ”€â”€ ai/             # Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ
â”‚   â”œâ”€â”€ audio/          # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„ØµÙˆØª
â”‚   â”œâ”€â”€ persistence/    # Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
â”‚   â””â”€â”€ external_services/ # Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
â””â”€â”€ presentation/        # ÙˆØ§Ø¬Ù‡Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
    â”œâ”€â”€ api/            # REST API
    â”œâ”€â”€ web/            # ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ÙˆÙŠØ¨
    â””â”€â”€ websocket/      # Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ù…Ø¨Ø§Ø´Ø±
```

## ğŸ¯ Ø®Ø·Ø© Ø§Ù„ØªÙ†ÙÙŠØ°

### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªÙ†Ø¸ÙŠÙ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
1. Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø¥Ù„Ù‰ `deprecated/`
2. Ø¯Ù…Ø¬ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
3. ØªÙˆØ­ÙŠØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†

### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù‡ÙŠÙƒÙ„Ø©
1. Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Clean Architecture
2. Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Ù…ÙˆØ§Ù‚Ø¹Ù‡Ø§ Ø§Ù„ØµØ­ÙŠØ­Ø©
3. ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆØ§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª

### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø§Ù„ØªØ­Ø³ÙŠÙ†
1. Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø©
2. ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚
3. ØªØ·Ø¨ÙŠÙ‚ Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ù…Ø§Ø±Ø³Ø§Øª

## ğŸš€ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©
- **ØªÙ‚Ù„ÙŠÙ„ 60%** ÙÙŠ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
- **ØªØ­Ø³ÙŠÙ† 80%** ÙÙŠ ÙˆØ¶ÙˆØ­ Ø§Ù„Ø¨Ù†ÙŠØ©
- **Ø²ÙŠØ§Ø¯Ø© 90%** ÙÙŠ Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØµÙŠØ§Ù†Ø©
- **Ù†Ù‚Ø§Ø· Ù†Ø¸Ø§ÙØ© 95+/100**

---
**ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©**: ArchitectureAnalyzer Pro
**Ø§Ù„ØªÙˆÙ‚ÙŠØª**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""
        return report

    def run_comprehensive_analysis(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        logger.info("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹...")
        self.scan_for_duplicates()
        self.evaluate_files()
        self.classify_files()
        self.propose_merge_strategy()
        self.propose_clean_architecture()
        report_content = self.generate_comprehensive_report()
        report_path = (
            self.base_path
            / "deleted"
            / "reports"
            / "COMPREHENSIVE_ARCHITECTURE_ANALYSIS.md"
        )
        report_path.parent.mkdir(parents=True, exist_ok=True)
        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)
        logger.info(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {report_path}")
        return self.analysis_report


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = ArchitectureAnalyzer()
    try:
        logger.info("=" * 60)
        logger.info("ğŸ—ï¸  COMPREHENSIVE ARCHITECTURE ANALYZER")
        logger.info("ğŸ¯  AI-TEDDY-BEAR PROJECT RESTRUCTURING")
        logger.info("=" * 60)
        report = analyzer.run_comprehensive_analysis()
        logger.info("\nğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„!")
        logger.info(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {report['statistics']['total_files']}")
        logger.info(f"ğŸ”„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª: {report['statistics']['total_duplicates']}")
        logger.info(
            f"ğŸ† Ù†Ù‚Ø§Ø· Ø§Ù„Ù†Ø¸Ø§ÙØ©: {report['statistics']['clean_score']}/100")
        logger.info(
            "ğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„: deleted/reports/COMPREHENSIVE_ARCHITECTURE_ANALYSIS.md"
        )
    except Exception as e:
        logger.info(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
