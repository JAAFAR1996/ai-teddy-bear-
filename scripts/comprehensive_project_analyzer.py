from typing import Any, Dict, List
from pathlib import Path
from datetime import datetime, timezone
from collections import defaultdict
import re
import os
import json
import hashlib
import ast
import logging

logger = logging.getLogger(__name__)

"""
ğŸ” Comprehensive Project File Analyzer
Analyzes every Python file in the project and generates detailed reports
"""


class ComprehensiveProjectAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ù…Ø¹ Ù‚Ø¯Ø±Ø§Øª Ù…ØªÙ‚Ø¯Ù…Ø©"""

    def __init__(self, project_root: str = '.'):
        self.project_root = Path(project_root)
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "total_files": 0,
            "total_lines": 0,
            "file_types": defaultdict(int),
            "duplicate_candidates": [],
            "large_files": [],
            "empty_files": [],
            "test_files": [],
            "config_files": [],
            "security_issues": [],
            "code_quality_issues": [],
            "dependency_analysis": defaultdict(list),
            "detailed_analysis": [],
            "suggested_moves": [],
            "health_score": 0
        }

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù„Ù„ØªØµÙ†ÙŠÙ
        self.critical_patterns = [
            'main.py', 'app.py', 'wsgi.py', '__main__.py',
            'security/', 'auth/', 'child_safety/',
            'models/', 'entities/', 'api/endpoints/', 'core/domain/'
        ]

        self.trash_patterns = [
            r'.*_old\.py$', r'.*_backup\.py$', r'.*_temp\.py$',
            r'.*_copy\.py$', r'.*\.pyc$', r'.*~$'
        ]

    def analyze_project(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ÙƒÙ„ Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹...")

        # Ø¬Ù…Ø¹ ÙƒÙ„ Ù…Ù„ÙØ§Øª Python
        python_files = list(self.project_root.rglob("*.py"))
        self.analysis_results["total_files"] = len(python_files)

        # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù
        for idx, file_path in enumerate(python_files, 1):
            if idx % 50 == 0:
                print(f"ğŸ“Š ØªÙ… ØªØ­Ù„ÙŠÙ„ {idx}/{len(python_files)} Ù…Ù„Ù...")

            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            if any(skip in str(file_path) for skip in ['.git', '__pycache__', 'node_modules', '.venv', 'venv', 'backup_']):
                continue

            self.analyze_python_file(file_path)

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØµØ­Ø© Ø§Ù„Ø¹Ø§Ù…Ø© Ù„Ù„Ù…Ø´Ø±ÙˆØ¹
        self.calculate_project_health()

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù†Ù‚Ù„
        self.generate_move_suggestions()

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
        self.find_duplicates()

        return self.analysis_results

    def analyze_python_file(self, file_path: Path) -> None:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Python ÙˆØ§Ø­Ø¯ Ø¨Ø¹Ù…Ù‚"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.splitlines()

            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø£Ø³Ø§Ø³ÙŠØ©
            self.analysis_results["total_lines"] += len(lines)

            # Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù
            file_hash = hashlib.sha256(content.encode()).hexdigest()

            # ØªØ­Ù„ÙŠÙ„ AST Ø¥Ø°Ø§ Ø£Ù…ÙƒÙ†
            ast_analysis = self.analyze_ast(content)

            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ ÙˆØ£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù
            file_type = self.determine_file_type(file_path, content)
            importance = self.determine_importance(
                file_path, content, ast_analysis)

            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            issues = self.find_issues(content, file_path)

            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
            dependencies = self.analyze_dependencies(content, ast_analysis)

            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„Ù
            file_report = {
                "path": str(file_path),
                "relative_path": str(file_path.relative_to(self.project_root)),
                "type": file_type,
                "importance": importance,
                "size": len(content),
                "lines": len(lines),
                "hash": file_hash,
                "ast_analysis": ast_analysis,
                "dependencies": dependencies,
                "issues": issues,
                "suggested_location": self.suggest_location(file_path, file_type),
                "can_be_deleted": importance == 'trash',
                "needs_refactoring": len(issues) > 3
            }

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            self.analysis_results["file_types"][file_type] += 1
            self.analysis_results["detailed_analysis"].append(file_report)

            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§ØµØ©
            if len(lines) == 0:
                self.analysis_results["empty_files"].append(str(file_path))
            elif len(lines) > 500:
                self.analysis_results["large_files"].append({
                    "path": str(file_path),
                    "lines": len(lines)
                })

            if 'test_' in str(file_path) or '_test.py' in str(file_path):
                self.analysis_results["test_files"].append(str(file_path))

            if 'config' in str(file_path).lower():
                self.analysis_results["config_files"].append(str(file_path))

            # ØªØ­Ø¯ÙŠØ« Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            if any(issue.startswith("Security:") for issue in issues):
                self.analysis_results["security_issues"].append({
                    "file": str(file_path),
                    "issues": [i for i in issues if i.startswith("Security:")]
                })

            if any(issue.startswith("Quality:") for issue in issues):
                self.analysis_results["code_quality_issues"].append({
                    "file": str(file_path),
                    "issues": [i for i in issues if i.startswith("Quality:")]
                })

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")

    def analyze_ast(self, content: str) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ AST Ù„Ù„Ù…Ù„Ù"""
        try:
            tree = ast.parse(content)

            # Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            imports = []
            import_froms = []
            classes = []
            functions = []

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    imports.extend([alias.name for alias in node.names])
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        import_froms.append(node.module)
                elif isinstance(node, ast.ClassDef):
                    classes.append({
                        "name": node.name,
                        "methods": len([n for n in node.body if isinstance(n, ast.FunctionDef)]),
                        "lines": node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                    })
                elif isinstance(node, ast.FunctionDef):
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…ÙŠØ«ÙˆØ¯Ø² Ø¯Ø§Ø®Ù„ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
                    if not any(isinstance(parent, ast.ClassDef) for parent in ast.walk(tree) if node in getattr(parent, 'body', [])):
                        functions.append({
                            "name": node.name,
                            "args": len(node.args.args),
                            "lines": node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0,
                            "has_docstring": ast.get_docstring(node) is not None
                        })

            return {
                "imports": imports,
                "import_froms": import_froms,
                "classes": classes,
                "functions": functions,
                "total_imports": len(imports) + len(import_froms),
                "total_classes": len(classes),
                "total_functions": len(functions)
            }

        except:
            return {
                "imports": [],
                "import_froms": [],
                "classes": [],
                "functions": [],
                "total_imports": 0,
                "total_classes": 0,
                "total_functions": 0
            }

    def determine_file_type(self, file_path: Path, content: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ø¯Ù‚Ø©"""
        path_str = str(file_path).lower()

        # Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        if 'test' in path_str or 'spec' in path_str:
            return 'test'

        # Ø§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª
        elif any(x in path_str for x in ['config', 'settings', 'env']):
            return 'config'

        # Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØ§Ù„ÙƒÙŠØ§Ù†Ø§Øª
        elif any(x in path_str for x in ['model', 'entity', 'schema']):
            return 'model'

        # Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        elif 'service' in path_str or 'manager' in path_str:
            return 'service'

        # Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª
        elif 'repository' in path_str or 'repo' in path_str:
            return 'repository'

        # Ø§Ù„ØªØ­ÙƒÙ… ÙˆØ§Ù„ÙˆØ§Ø¬Ù‡Ø§Øª
        elif any(x in path_str for x in ['controller', 'endpoint', 'route', 'api', 'view']):
            return 'controller'

        # Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©
        elif any(x in path_str for x in ['util', 'helper', 'tool']):
            return 'utility'

        # Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©
        elif 'infrastructure' in path_str or 'infra' in path_str:
            return 'infrastructure'

        # Ø§Ù„Ù…Ø¬Ø§Ù„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
        elif 'domain' in path_str or 'core' in path_str:
            return 'domain'

        # Ø§Ù„Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª
        elif 'script' in path_str:
            return 'script'

        # Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        elif '__init__.py' in str(file_path):
            return 'init'

        else:
            return 'other'

    def determine_importance(self, file_path: Path, content: str, ast_analysis: Dict) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ù…ØªØ¹Ø¯Ø¯Ø©"""
        path_str = str(file_path)

        # Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø© - Ø§Ø­Ø°Ù ÙÙˆØ±Ø§Ù‹
        if any(re.match(pattern, path_str) for pattern in self.trash_patterns):
            return 'trash'

        # Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ©
        if len(content.strip()) == 0:
            return 'trash'

        # Ù…Ù„ÙØ§Øª Ù…Ù‡Ù…Ø© Ø¬Ø¯Ø§Ù‹
        if any(pattern in path_str for pattern in self.critical_patterns):
            return 'critical'

        # Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ù…ØµØ§Ø¯Ù‚Ø©
        if any(x in path_str.lower() for x in ['security', 'auth', 'permission', 'child_safety']):
            return 'critical'

        # Ù…Ù„ÙØ§Øª Ø¨Ù‡Ø§ Ù…Ù†Ø·Ù‚ Ù…Ø¹Ù‚Ø¯
        if ast_analysis['total_classes'] > 2 or ast_analysis['total_functions'] > 5:
            return 'high'

        # Ø®Ø¯Ù…Ø§Øª ÙˆÙ…Ø³ØªÙˆØ¯Ø¹Ø§Øª
        if any(x in path_str for x in ['service', 'repository', 'manager']):
            return 'high'

        # Ù…Ù„ÙØ§Øª Ø¨Ø¯ÙˆÙ† Ù…Ù†Ø·Ù‚ Ø­Ù‚ÙŠÙ‚ÙŠ
        if ast_analysis['total_classes'] == 0 and ast_analysis['total_functions'] == 0:
            if '__init__.py' not in path_str:  # Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…Ù„Ù ØªÙ‡ÙŠØ¦Ø©
                return 'low'

        # Ù…Ù„ÙØ§Øª Ù…Ø¤Ù‚ØªØ© Ø£Ùˆ ØªØ¬Ø±ÙŠØ¨ÙŠØ©
        if any(x in path_str.lower() for x in ['test_', 'example', 'demo', 'sample']):
            if len(content) < 100:  # ÙˆØµØºÙŠØ±Ø©
                return 'low'

        return 'medium'

    def find_issues(self, content: str, file_path: Path) -> List[str]:
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        issues = []

        # Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ©
        if 'eval(' in content or 'exec(' in content:
            issues.append("Security: ÙŠØ³ØªØ®Ø¯Ù… eval/exec (Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ)")

        if re.search(r'(password|secret|key|token)\s*=\s*["\'][^"\']+["\']', content, re.IGNORECASE):
            issues.append("Security: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª Ø³Ø± Ù…Ø¶Ù…Ù†Ø©")

        if 'pickle.loads' in content:
            issues.append("Security: ÙŠØ³ØªØ®Ø¯Ù… pickle.loads (Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ)")

        # Ù…Ø´Ø§ÙƒÙ„ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯
        if 'except:' in content or '# FIXME: replace with specific exception
except Exception as exc:' in content:
            issues.append("Quality: Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ø³ØªØ«Ù†Ø§Ø¡Ø§Øª Ø¹Ø§Ù…Ø©")
        
        if re.search(r'print\s*\(', content) and 'test' not in str(file_path):
            issues.append("Quality: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ print ÙÙŠ ÙƒÙˆØ¯ Ø§Ù„Ø¥Ù†ØªØ§Ø¬")
        
        if 'TODO' in content or 'FIXME' in content or 'XXX' in content:
            issues.append("Quality: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ TODO/FIXME")
        
        if len(content.splitlines()) > 500:
            issues.append("Quality: Ù…Ù„Ù ÙƒØ¨ÙŠØ± Ø¬Ø¯Ø§Ù‹ (>500 Ø³Ø·Ø±)")
        
        if len(content.strip()) == 0:
            issues.append("Quality: Ù…Ù„Ù ÙØ§Ø±Øº")
        
        # Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯
        if 'from ..' in content and content.count('from ..') > 5:
            issues.append("Quality: Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ø³Ø¨ÙŠØ©")
        
        if 'import *' in content:
            issues.append("Quality: ÙŠØ³ØªØ®Ø¯Ù… import * (ØºÙŠØ± Ù…Ø³ØªØ­Ø³Ù†)")
        
        # Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡
        if re.search(r'for .+ in .+:\s*for .+ in .+:', content):
            if re.search(r'for .+ in .+:\s*for .+ in .+:\s*for .+ in .+:', content):
                issues.append("Performance: Ø­Ù„Ù‚Ø§Øª Ù…ØªØ¯Ø§Ø®Ù„Ø© Ø¹Ù…ÙŠÙ‚Ø© (3+ Ù…Ø³ØªÙˆÙŠØ§Øª)")
        
        # Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ØªÙˆØ«ÙŠÙ‚
        if not re.search(r'""".*"""', content, re.DOTALL) and len(content) > 200:
            issues.append("Documentation: Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ docstrings")
        
        return issues
    
    def analyze_dependencies(self, content: str, ast_analysis: Dict) -> Dict[str, List[str]]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ù„Ù„Ù…Ù„Ù"""
        dependencies = {
            "internal": [],
            "external": [],
            "circular_risk": []
        }
        
        # Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©
        for imp in ast_analysis['import_froms']:
            if imp and (imp.startswith('.') or imp.startswith('src') or imp.startswith('app')):
                dependencies["internal"].append(imp)
        
        # Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø®Ø§Ø±Ø¬ÙŠØ©
        for imp in ast_analysis['imports']:
            if imp and not imp.startswith('.'):
                dependencies["external"].append(imp)
        
        # Ø®Ø·Ø± Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯Ø§Ø¦Ø±ÙŠØ©
        if len(dependencies["internal"]) > 10:
            dependencies["circular_risk"].append("Ø§Ù„ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø¯Ø§Ø®Ù„ÙŠØ©")
        
        return dependencies
    
    def suggest_location(self, file_path: Path, file_type: str) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø£ÙØ¶Ù„ Ù„Ù„Ù…Ù„Ù"""
        current_path = str(file_path.relative_to(self.project_root))
        
        # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù†Ù‚Ù„ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
        type_to_location = {
            'model': 'src/core/domain/entities/',
            'service': 'src/core/services/',
            'repository': 'src/infrastructure/persistence/repositories/',
            'controller': 'src/api/endpoints/',
            'test': 'tests/unit/' if 'unit' in current_path else 'tests/',
            'config': 'configs/',
            'utility': 'src/shared/utils/',
            'domain': 'src/core/domain/',
            'infrastructure': 'src/infrastructure/',
            'script': 'scripts/',
            'init': None  # Ù„Ø§ ØªÙ†Ù‚Ù„ Ù…Ù„ÙØ§Øª __init__.py
        }
        
        suggested = type_to_location.get(file_type)
        
        if not suggested or current_path.startswith(suggested):
            return None  # Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø¬Ø¯ÙŠØ¯
        filename = file_path.name
        return suggested + filename
    
    def find_duplicates(self) -> None:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø£Ùˆ Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù€ hash
        hash_to_files = defaultdict(list)
        
        for file_info in self.analysis_results["detailed_analysis"]:
            hash_to_files[file_info['hash']].append(file_info['path'])
        
        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
        for file_hash, files in hash_to_files.items():
            if len(files) > 1:
                self.analysis_results["duplicate_candidates"].append({
                    "type": "exact",
                    "hash": file_hash,
                    "files": files
                })
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ
        function_signatures = defaultdict(list)
        
        for file_info in self.analysis_results["detailed_analysis"]:
            if file_info['ast_analysis']['functions']:
                for func in file_info['ast_analysis']['functions']:
                    signature = f"{func['name']}({func['args']})"
                    function_signatures[signature].append(file_info['path'])
        
        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for signature, files in function_signatures.items():
            if len(files) > 1:
                self.analysis_results["duplicate_candidates"].append({
                    "type": "functional",
                    "signature": signature,
                    "files": files
                })
    
    def calculate_project_health(self) -> None:
        """Ø­Ø³Ø§Ø¨ ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø¹Ø§Ù…Ø©"""
        total_files = len(self.analysis_results["detailed_analysis"])
        if total_files == 0:
            self.analysis_results["health_score"] = 0
            return
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø·
        score = 100
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        empty_ratio = len(self.analysis_results["empty_files"]) / total_files
        score -= empty_ratio * 20
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        large_ratio = len(self.analysis_results["large_files"]) / total_files
        score -= large_ratio * 15
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©
        security_ratio = len(self.analysis_results["security_issues"]) / total_files
        score -= security_ratio * 30
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø¬ÙˆØ¯Ø©
        quality_issues = sum(1 for f in self.analysis_results["detailed_analysis"] if f['needs_refactoring'])
        quality_ratio = quality_issues / total_files
        score -= quality_ratio * 20
        
        # Ø®ØµÙ… Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„Ø®Ø§Ø·Ø¦Ø©
        misplaced = sum(1 for f in self.analysis_results["detailed_analysis"] if f['suggested_location'])
        misplaced_ratio = misplaced / total_files
        score -= misplaced_ratio * 15
        
        self.analysis_results["health_score"] = max(0, min(100, score))
    
    def generate_move_suggestions(self) -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù‚Ø§Ø¦Ù…Ø© Ø¨Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù†Ù‚Ù„"""
        for file_info in self.analysis_results["detailed_analysis"]:
            if file_info['suggested_location'] and file_info['importance'] != 'trash':
                self.analysis_results["suggested_moves"].append({
                    "from": file_info['path'],
                    "to": file_info['suggested_location'],
                    "reason": f"Ù†Ù‚Ù„ {file_info['type']} Ù„Ù„Ù…ÙƒØ§Ù† Ø§Ù„Ù…Ù†Ø§Ø³Ø¨",
                    "priority": "high" if file_info['importance'] == 'critical' else "medium"
                })
    
    def generate_report(self, output_file: str = "project_analysis_report.json") -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± JSON Ù…ÙØµÙ„"""
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2)
        
        print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {output_file}")
        
        # Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ
        print("\nğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        print(f"  - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {self.analysis_results['total_files']}")
        print(f"  - Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±: {self.analysis_results['total_lines']:,}")
        print(f"  - ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {self.analysis_results['health_score']:.1f}%")
        print(f"  - Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ©: {len(self.analysis_results['empty_files'])}")
        print(f"  - Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©: {len(self.analysis_results['large_files'])}")
        print(f"  - Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ©: {len(self.analysis_results['security_issues'])}")
        print(f"  - Ù…Ù„ÙØ§Øª ØªØ­ØªØ§Ø¬ Ù†Ù‚Ù„: {len(self.analysis_results['suggested_moves'])}")


def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
    analyzer = ComprehensiveProjectAnalyzer()
    results = analyzer.analyze_project()
    analyzer.generate_report()
    
    # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± markdown Ø£ÙŠØ¶Ø§Ù‹
    create_markdown_report(results)


def create_markdown_report(results: Dict[str, Any]) -> None:
    """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown Ø³Ù‡Ù„ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"""
    with open("project_analysis_report.md", "w", encoding="utf-8") as f:
        f.write("# ğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ù„Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear\n\n")
        f.write(f"**Ø§Ù„ØªØ§Ø±ÙŠØ®**: {results['timestamp']}\n\n")
        
        f.write("## ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©\n\n")
        f.write(f"- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª**: {results['total_files']}\n")
        f.write(f"- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø£Ø³Ø·Ø±**: {results['total_lines']:,}\n")
        f.write(f"- **ØµØ­Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹**: {results['health_score']:.1f}%\n\n")
        
        f.write("## ğŸ“ ØªÙˆØ²ÙŠØ¹ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª\n\n")
        for file_type, count in sorted(results['file_types'].items(), key=lambda x: x[1], reverse=True):
            f.write(f"- **{file_type}**: {count} Ù…Ù„Ù\n")
        
        f.write("\n## ğŸš¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©\n\n")
        f.write(f"### Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ù†ÙŠØ© ({len(results['security_issues'])})\n")
        for issue in results['security_issues'][:5]:  # Ø£ÙˆÙ„ 5 ÙÙ‚Ø·
            f.write(f"- `{issue['file']}`: {', '.join(issue['issues'])}\n")
        
        f.write(f"\n### Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ© ({len(results['empty_files'])})\n")
        for file in results['empty_files'][:10]:  # Ø£ÙˆÙ„ 10 ÙÙ‚Ø·
            f.write(f"- `{file}`\n")
        
        f.write(f"\n### Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø© ({len(results['large_files'])})\n")
        for file in sorted(results['large_files'], key=lambda x: x['lines'], reverse=True)[:5]:
            f.write(f"- `{file['path']}`: {file['lines']} Ø³Ø·Ø±\n")
        
        f.write(f"\n## ğŸ“¦ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ({len(results['duplicate_candidates'])})\n")
        exact_dups = [d for d in results['duplicate_candidates'] if d['type'] == 'exact']
        if exact_dups:
            f.write(f"### ØªÙƒØ±Ø§Ø± ÙƒØ§Ù…Ù„ ({len(exact_dups)})\n")
            for dup in exact_dups[:5]:
                f.write(f"- Ø§Ù„Ù…Ù„ÙØ§Øª: {', '.join(f'`{f}`' for f in dup['files'])}\n")
        
        f.write(f"\n## ğŸšš Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù†Ù‚Ù„ ({len(results['suggested_moves'])})\n")
        high_priority = [m for m in results['suggested_moves'] if m['priority'] == 'high']
        if high_priority:
            f.write("### Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©\n")
            for move in high_priority[:10]:
                f.write(f"- Ù†Ù‚Ù„ `{move['from']}` â† `{move['to']}`\n")
    
    print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown: project_analysis_report.md")


if __name__ == "__main__":
    main()
