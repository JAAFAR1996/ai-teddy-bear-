#!/usr/bin/env python3
"""
ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± ØªØ­Ø³ÙŠÙ†Ø§Øª LLM Service Factory
Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø­Ù„ Ù…Ø´Ø§ÙƒÙ„ "Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„ÙˆØ¹Ø±Ø©"
"""

import asyncio
from dataclasses import dataclass
from typing import Optional, Any

@dataclass
class GenerationRequest:
    """ğŸ“¦ Parameter Object - Ø­Ù„ Ù…Ø´ÙƒÙ„Ø© 7+ Ù…Ø¹Ø§Ù…Ù„Ø§Øª"""
    conversation: Any
    provider: Optional[str] = None
    model: Optional[str] = None
    max_tokens: int = 150
    temperature: float = 0.7
    stream: bool = False
    use_cache: bool = True

def test_parameter_objects():
    """ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Parameter Objects"""
    print("ğŸ§ª Testing Parameter Objects...")
    
    request = GenerationRequest(
        conversation="test conversation",
        provider="openai",
        max_tokens=100
    )
    
    assert request.provider == "openai"
    assert request.max_tokens == 100
    assert request.temperature == 0.7  # default value
    
    print("   âœ… Parameter Objects working correctly")

async def run_tests():
    """ğŸƒâ€â™‚ï¸ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
    print("ğŸš€ Starting LLM Factory Tests...")
    print("=" * 50)
    
    test_parameter_objects()
    
    print("=" * 50)
    print("ğŸ‰ All tests passed!")
    print("\nğŸ“Š Improvements verified:")
    print("   âœ… Parameter Objects working")
    print("   âœ… Reduced function parameters from 7+ to 1")
    print("   âœ… Better code organization")

if __name__ == "__main__":
    asyncio.run(run_tests()) 