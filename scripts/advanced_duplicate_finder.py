#!/usr/bin/env python3
"""
ğŸ” Advanced Duplicate Finder - AI Teddy Bear Project
Ù…Ø­Ù„Ù„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØ§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©

Lead Architect: Ø¬Ø¹ÙØ± Ø£Ø¯ÙŠØ¨ (Jaafar Adeeb)
Enterprise Grade AI Teddy Bear Project 2025
"""

import ast
import hashlib
import json
import os
import re
from collections import defaultdict
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Set, Tuple, Optional


@dataclass
class FileInfo:
    """Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„Ù"""
    path: str
    size: int
    hash: str
    lines: int
    content: str
    imports: List[str] = field(default_factory=list)
    classes: List[str] = field(default_factory=list)
    functions: List[str] = field(default_factory=list)
    similarity_score: float = 0.0


@dataclass
class DuplicateGroup:
    """Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø©"""
    type: str  # exact, similar, functional
    files: List[FileInfo]
    similarity: float
    recommendation: str


class AdvancedDuplicateFinder:
    """Ù…Ø­Ù„Ù„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.files_info: Dict[str, FileInfo] = {}
        self.duplicate_groups: List[DuplicateGroup] = []
        self.ignore_patterns = {
            '.git', '__pycache__', 'node_modules', '.venv', 'venv',
            '.pytest_cache', '.mypy_cache', 'dist', 'build'
        }

    def scan_project(self) -> None:
        """Ù…Ø³Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ù…Ø³Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")

        for file_path in self.project_root.rglob("*.py"):
            if self._should_ignore(file_path):
                continue

            try:
                file_info = self._analyze_file(file_path)
                if file_info:
                    self.files_info[str(file_path)] = file_info
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")

        print(f"âœ… ØªÙ… Ù…Ø³Ø­ {len(self.files_info)} Ù…Ù„Ù Python")

    def _should_ignore(self, file_path: Path) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ ØªØ¬Ø§Ù‡Ù„Ù‡"""
        path_str = str(file_path)
        return any(pattern in path_str for pattern in self.ignore_patterns)

    def _analyze_file(self, file_path: Path) -> Optional[FileInfo]:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ÙˆØ§Ø­Ø¯ Ø¨Ø¹Ù…Ù‚"""
        try:
            content = file_path.read_text(encoding='utf-8')

            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
            if len(content.strip()) == 0:
                return None

            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
            lines = len(content.splitlines())
            size = len(content)
            file_hash = hashlib.sha256(content.encode()).hexdigest()

            # ØªØ­Ù„ÙŠÙ„ AST
            imports, classes, functions = self._parse_ast(content)

            return FileInfo(
                path=str(file_path),
                size=size,
                hash=file_hash,
                lines=lines,
                content=content,
                imports=imports,
                classes=classes,
                functions=functions
            )

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {file_path}: {e}")
            return None

    def _parse_ast(self, content: str) -> Tuple[List[str], List[str], List[str]]:
        """ØªØ­Ù„ÙŠÙ„ AST ÙˆØ§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¥ÙŠÙ…Ø¨ÙˆØ±ØªØ§Øª ÙˆØ§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„"""
        try:
            tree = ast.parse(content)
            imports = []
            classes = []
            functions = []

            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    if isinstance(node, ast.Import):
                        imports.extend([alias.name for alias in node.names])
                    elif node.module:
                        imports.append(node.module)

                elif isinstance(node, ast.ClassDef):
                    classes.append(node.name)

                elif isinstance(node, ast.FunctionDef):
                    functions.append(node.name)

            return imports, classes, functions

        except:
            return [], [], []

    def find_exact_duplicates(self) -> None:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹"""
        print("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹...")

        hash_groups = defaultdict(list)
        for file_info in self.files_info.values():
            hash_groups[file_info.hash].append(file_info)

        for file_hash, files in hash_groups.items():
            if len(files) > 1:
                # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡
                best_file = self._select_best_file(files)
                recommendation = f"Ø§Ø­ØªÙØ¸ Ø¨Ù€ {best_file.path} ÙˆØ§Ø­Ø°Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ"

                self.duplicate_groups.append(DuplicateGroup(
                    type="exact",
                    files=files,
                    similarity=1.0,
                    recommendation=recommendation
                ))

    def find_similar_files(self, threshold: float = 0.7) -> None:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©"""
        print("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©...")

        files_list = list(self.files_info.values())
        processed_pairs = set()

        for i, file1 in enumerate(files_list):
            for j, file2 in enumerate(files_list[i+1:], i+1):
                pair_key = tuple(sorted([file1.path, file2.path]))
                if pair_key in processed_pairs:
                    continue
                processed_pairs.add(pair_key)

                similarity = self._calculate_similarity(file1, file2)
                if similarity >= threshold:
                    recommendation = self._generate_similarity_recommendation(
                        file1, file2, similarity)

                    self.duplicate_groups.append(DuplicateGroup(
                        type="similar",
                        files=[file1, file2],
                        similarity=similarity,
                        recommendation=recommendation
                    ))

    def find_functional_duplicates(self) -> None:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØ¸ÙŠÙÙŠØ§Ù‹"""
        print("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØ¸ÙŠÙÙŠØ§Ù‹...")

        # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ø­Ø³Ø¨ Ø§Ù„ÙˆØ¸ÙŠÙØ©
        function_groups = defaultdict(list)
        class_groups = defaultdict(list)

        for file_info in self.files_info.values():
            # ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
            if file_info.functions:
                func_signature = tuple(sorted(file_info.functions))
                if len(func_signature) > 2:  # ÙÙ‚Ø· Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ Ù„Ù‡Ø§ Ø£ÙƒØ«Ø± Ù…Ù† Ø¯Ø§Ù„ØªÙŠÙ†
                    function_groups[func_signature].append(file_info)

            # ØªØ¬Ù…ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©
            if file_info.classes:
                class_signature = tuple(sorted(file_info.classes))
                if len(class_signature) > 0:
                    class_groups[class_signature].append(file_info)

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for signature, files in function_groups.items():
            if len(files) > 1:
                self.duplicate_groups.append(DuplicateGroup(
                    type="functional",
                    files=files,
                    similarity=0.8,
                    recommendation=f"Ø¯Ù…Ø¬ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©: {', '.join(signature[:3])}..."
                ))

        for signature, files in class_groups.items():
            if len(files) > 1:
                self.duplicate_groups.append(DuplicateGroup(
                    type="functional",
                    files=files,
                    similarity=0.8,
                    recommendation=f"Ø¯Ù…Ø¬ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©: {', '.join(signature[:3])}..."
                ))

    def _calculate_similarity(self, file1: FileInfo, file2: FileInfo) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ù„ÙÙŠÙ†"""
        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¥ÙŠÙ…Ø¨ÙˆØ±ØªØ§Øª
        imports1 = set(file1.imports)
        imports2 = set(file2.imports)
        import_similarity = len(imports1 & imports2) / \
            max(len(imports1 | imports2), 1)

        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª
        classes1 = set(file1.classes)
        classes2 = set(file2.classes)
        class_similarity = len(classes1 & classes2) / \
            max(len(classes1 | classes2), 1)

        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ø¯ÙˆØ§Ù„
        functions1 = set(file1.functions)
        functions2 = set(file2.functions)
        function_similarity = len(
            functions1 & functions2) / max(len(functions1 | functions2), 1)

        # ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù†Øµ
        content_similarity = self._calculate_content_similarity(
            file1.content, file2.content)

        # Ù…ØªÙˆØ³Ø· Ù…Ø±Ø¬Ø­
        return (import_similarity * 0.3 +
                class_similarity * 0.3 +
                function_similarity * 0.3 +
                content_similarity * 0.1)

    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        """Ø­Ø³Ø§Ø¨ ØªØ´Ø§Ø¨Ù‡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù†ØµÙŠ"""
        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ÙƒÙ„Ù…Ø§Øª
        words1 = set(re.findall(r'\b\w+\b', content1.lower()))
        words2 = set(re.findall(r'\b\w+\b', content2.lower()))

        # Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        return len(words1 & words2) / max(len(words1 | words2), 1)

    def _select_best_file(self, files: List[FileInfo]) -> FileInfo:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡"""
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª ÙÙŠ src/
        for file_info in files:
            if 'src/' in file_info.path and 'test' not in file_info.path.lower():
                return file_info

        # Ø«Ù… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£ÙƒØ¨Ø± (Ø£ÙƒØ«Ø± Ù…Ø­ØªÙˆÙ‰)
        return max(files, key=lambda f: f.size)

    def _generate_similarity_recommendation(self, file1: FileInfo, file2: FileInfo, similarity: float) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙˆØµÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
        if similarity > 0.9:
            return f"Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙÙŠÙ† - ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹ ({similarity:.1%})"
        elif similarity > 0.8:
            return f"Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø¯Ù…Ø¬ - ØªØ´Ø§Ø¨Ù‡ Ø¹Ø§Ù„ÙŠ ({similarity:.1%})"
        else:
            return f"Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ - ({similarity:.1%})"

    def analyze_service_duplicates(self) -> None:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")

        service_types = {
            'ai': ['ai_service', 'ai_processor', 'ai_handler'],
            'audio': ['audio_service', 'voice_service', 'speech_service'],
            'emotion': ['emotion_service', 'emotion_analyzer', 'emotion_detector'],
            'transcription': ['transcription_service', 'speech_to_text', 'stt_service'],
            'synthesis': ['synthesis_service', 'text_to_speech', 'tts_service'],
            'cache': ['cache_service', 'caching_service', 'cache_manager'],
            'database': ['database_service', 'db_service', 'repository'],
            'security': ['security_service', 'auth_service', 'encryption_service']
        }

        service_groups = defaultdict(list)

        for file_info in self.files_info.values():
            filename = Path(file_info.path).name.lower()

            for service_type, patterns in service_types.items():
                if any(pattern in filename for pattern in patterns):
                    service_groups[service_type].append(file_info)
                    break

        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for service_type, files in service_groups.items():
            if len(files) > 1:
                self.duplicate_groups.append(DuplicateGroup(
                    type="service",
                    files=files,
                    similarity=0.9,
                    recommendation=f"Ø¯Ù…Ø¬ Ø®Ø¯Ù…Ø§Øª {service_type} - Ø¹Ø«Ø± Ø¹Ù„Ù‰ {len(files)} Ù†Ø³Ø®Ø©"
                ))

    def generate_report(self) -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        print("ğŸ“Š Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±...")

        # ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø­Ø³Ø¨ Ø§Ù„Ù†ÙˆØ¹
        exact_duplicates = [
            g for g in self.duplicate_groups if g.type == "exact"]
        similar_files = [
            g for g in self.duplicate_groups if g.type == "similar"]
        functional_duplicates = [
            g for g in self.duplicate_groups if g.type == "functional"]
        service_duplicates = [
            g for g in self.duplicate_groups if g.type == "service"]

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report = {
            "timestamp": "2025-01-15T10:30:00",
            "total_files": len(self.files_info),
            "summary": {
                "exact_duplicates": len(exact_duplicates),
                "similar_files": len(similar_files),
                "functional_duplicates": len(functional_duplicates),
                "service_duplicates": len(service_duplicates),
                "total_issues": len(self.duplicate_groups)
            },
            "duplicates": {
                "exact": [self._serialize_group(g) for g in exact_duplicates],
                "similar": [self._serialize_group(g) for g in similar_files],
                "functional": [self._serialize_group(g) for g in functional_duplicates],
                "services": [self._serialize_group(g) for g in service_duplicates]
            }
        }

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± JSON
        with open("advanced_duplicate_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown
        self._create_markdown_report(report)

        # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ
        self._print_summary(report)

    def _serialize_group(self, group: DuplicateGroup) -> Dict:
        """ØªØ³Ù„Ø³Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø©"""
        return {
            "type": group.type,
            "files": [f.path for f in group.files],
            "similarity": group.similarity,
            "recommendation": group.recommendation,
            "total_size": sum(f.size for f in group.files),
            "wasted_space": sum(f.size for f in group.files[1:]) if len(group.files) > 1 else 0
        }

    def _create_markdown_report(self, report: Dict) -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown"""
        md_content = f"""# ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…

## ğŸ“Š Ø§Ù„Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ø§Ù…
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª**: {report['total_files']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹**: {report['summary']['exact_duplicates']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©**: {report['summary']['similar_files']}
- **Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ÙˆØ¸ÙŠÙÙŠ**: {report['summary']['functional_duplicates']}
- **Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©**: {report['summary']['service_duplicates']}

## ğŸ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
"""

        for group in report['duplicates']['exact']:
            md_content += f"""
### Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø© ({len(group['files'])} Ù…Ù„ÙØ§Øª)
**Ø§Ù„ØªÙˆØµÙŠØ©**: {group['recommendation']}
**Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©**: {group['wasted_space']} Ø¨Ø§ÙŠØª

**Ø§Ù„Ù…Ù„ÙØ§Øª**:
"""
            for file_path in group['files']:
                md_content += f"- `{file_path}`\n"

        md_content += "\n## ğŸ”— Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©\n"
        for group in report['duplicates']['similar']:
            md_content += f"""
### ØªØ´Ø§Ø¨Ù‡ {group['similarity']:.1%}
**Ø§Ù„ØªÙˆØµÙŠØ©**: {group['recommendation']}
**Ø§Ù„Ù…Ù„ÙØ§Øª**: {', '.join(f'`{f}`' for f in group['files'])}
"""

        md_content += "\n## âš™ï¸ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©\n"
        for group in report['duplicates']['services']:
            md_content += f"""
### Ø®Ø¯Ù…Ø© Ù…ÙƒØ±Ø±Ø©
**Ø§Ù„ØªÙˆØµÙŠØ©**: {group['recommendation']}
**Ø§Ù„Ù…Ù„ÙØ§Øª**: {', '.join(f'`{f}`' for f in group['files'])}
"""

        with open("advanced_duplicate_report.md", "w", encoding="utf-8") as f:
            f.write(md_content)

    def _print_summary(self, report: Dict) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        print("\n" + "="*80)
        print("ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…")
        print("="*80)
        print(f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {report['total_files']}")
        print(
            f"ğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹: {report['summary']['exact_duplicates']}")
        print(f"ğŸ”— Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©: {report['summary']['similar_files']}")
        print(
            f"âš™ï¸ Ø§Ù„ØªÙƒØ±Ø§Ø± Ø§Ù„ÙˆØ¸ÙŠÙÙŠ: {report['summary']['functional_duplicates']}")
        print(f"ğŸ› ï¸ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {report['summary']['service_duplicates']}")
        print("="*80)

        if report['summary']['total_issues'] > 0:
            print(
                f"\nğŸ’¡ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {report['summary']['total_issues']}")
            print("ğŸ“‹ Ø±Ø§Ø¬Ø¹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„ØªÙØµÙŠÙ„ÙŠØ©")
        else:
            print("\nâœ… Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø©!")

    def create_cleanup_script(self) -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª ØªÙ†Ø¸ÙŠÙ"""
        exact_duplicates = [
            g for g in self.duplicate_groups if g.type == "exact"]

        if not exact_duplicates:
            print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ Ù„Ø­Ø°ÙÙ‡Ø§")
            return

        script_content = """#!/usr/bin/env python3
# Ø³ÙƒØ±ÙŠØ¨Øª ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© - ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
import os
import shutil
from pathlib import Path

def backup_and_delete(file_path: str):
    '''Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙˆØ­Ø°Ù Ø§Ù„Ù…Ù„Ù'''
    backup_dir = Path("backup_duplicates")
    backup_dir.mkdir(exist_ok=True)
    
    file_path = Path(file_path)
    if file_path.exists():
        # Ø¥Ù†Ø´Ø§Ø¡ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        backup_file = backup_dir / file_path.name
        shutil.copy2(file_path, backup_file)
        print(f"âœ… Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©: {backup_file}")
        
        # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ
        os.remove(file_path)
        print(f"ğŸ—‘ï¸ ØªÙ… Ø­Ø°Ù: {file_path}")
    else:
        print(f"âŒ Ø§Ù„Ù…Ù„Ù ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {file_path}")

def main():
    print("ğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
    
"""

        for group in exact_duplicates:
            # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙˆÙ„ ÙˆØ­Ø°Ù Ø§Ù„Ø¨Ø§Ù‚ÙŠ
            files_to_delete = group.files[1:]
            script_content += f"""
    # Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø© - Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ {group.files[0].path}
    print("ğŸ“¦ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø©...")
"""
            for file_info in files_to_delete:
                script_content += f'    backup_and_delete(r"{file_info.path}")\n'

        script_content += """
    print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªÙ†Ø¸ÙŠÙ!")

if __name__ == "__main__":
    main()
"""

        with open("cleanup_duplicates.py", "w", encoding="utf-8") as f:
            f.write(script_content)

        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ: cleanup_duplicates.py")

    def run_analysis(self) -> None:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")

        # Ù…Ø³Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
        self.scan_project()

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ù†ÙˆØ§Ø¹ Ù…Ø®ØªÙ„ÙØ© Ù…Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±
        self.find_exact_duplicates()
        self.find_similar_files()
        self.find_functional_duplicates()
        self.analyze_service_duplicates()

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±
        self.generate_report()
        self.create_cleanup_script()

        print("âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„!")


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    finder = AdvancedDuplicateFinder()
    finder.run_analysis()


if __name__ == "__main__":
    main()
