#!/usr/bin/env python3
"""
Content Similarity Analyzer
Ø£Ø¯Ø§Ø© ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙØ¹Ù„ÙŠ ÙÙŠ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØªØ·Ø§Ø¨Ù‚ 100%
"""

import os
import hashlib
import shutil
from pathlib import Path
from typing import Dict, List, Set, Tuple
from datetime import datetime
import difflib
import ast

class ContentSimilarityAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.analysis_results = {
            "timestamp": datetime.now().isoformat(),
            "identical_files": [],
            "similar_files": [],
            "unique_files": [],
            "merge_candidates": [],
            "delete_candidates": [],
            "actions_taken": []
        }

    def calculate_file_hash(self, file_path: Path) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù {file_path}: {e}")
            return ""

    def get_file_content_normalized(self, file_path: Path) -> str:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ù„Ù Ù…Ø¹ ØªØ·Ø¨ÙŠØ¹ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙˆØ§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
            lines = []
            for line in content.split('\n'):
                # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
                line = line.strip()
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙˆØ§Ù„Ø³Ø·ÙˆØ± Ø§Ù„ÙØ§Ø±ØºØ©
                if line and not line.startswith('#') and not line.startswith('"""') and not line.startswith("'''"):
                    lines.append(line)
            
            return '\n'.join(lines)
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù {file_path}: {e}")
            return ""

    def calculate_similarity_percentage(self, content1: str, content2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ø­ØªÙˆÙŠÙŠÙ†"""
        if not content1 or not content2:
            return 0.0
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… difflib Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity = difflib.SequenceMatcher(None, content1, content2).ratio()
        return similarity * 100

    def analyze_service_group(self, group_name: str) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø®Ø¯Ù…Ø§Øª Ù…Ø­Ø¯Ø¯Ø©"""
        print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©: {group_name}")
        
        deprecated_dir = self.base_path / "deprecated" / "services" / group_name
        if not deprecated_dir.exists():
            print(f"  âš ï¸ Ø§Ù„Ù…Ø¬Ù„Ø¯ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯: {deprecated_dir}")
            return {"identical": [], "similar": [], "unique": []}
        
        files = list(deprecated_dir.glob("*.py"))
        if len(files) < 2:
            print(f"  âš ï¸ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ù‚Ù„ÙŠÙ„ Ø¬Ø¯Ø§Ù‹: {len(files)}")
            return {"identical": [], "similar": [], "unique": list(files)}
        
        results = {
            "identical": [],
            "similar": [], 
            "unique": []
        }
        
        # Ø­Ø³Ø§Ø¨ hash ÙˆmØ­ØªÙˆÙ‰ ÙƒÙ„ Ù…Ù„Ù
        file_data = {}
        for file_path in files:
            content = self.get_file_content_normalized(file_path)
            file_hash = hashlib.md5(content.encode()).hexdigest()
            file_data[file_path] = {
                "hash": file_hash,
                "content": content,
                "size": len(content)
            }
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
        compared_pairs = set()
        
        for file1 in files:
            for file2 in files:
                if file1 == file2:
                    continue
                
                # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…ÙƒØ±Ø±Ø©
                pair = tuple(sorted([str(file1), str(file2)]))
                if pair in compared_pairs:
                    continue
                compared_pairs.add(pair)
                
                data1 = file_data[file1]
                data2 = file_data[file2]
                
                # ÙØ­Øµ Ø§Ù„ØªØ·Ø§Ø¨Ù‚ Ø§Ù„ÙƒØ§Ù…Ù„ (hash)
                if data1["hash"] == data2["hash"]:
                    results["identical"].append({
                        "file1": str(file1),
                        "file2": str(file2),
                        "similarity": 100.0,
                        "action": "delete_duplicate"
                    })
                    print(f"  âœ… Ù…ØªØ·Ø§Ø¨Ù‚ 100%: {file1.name} â†” {file2.name}")
                
                else:
                    # ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
                    similarity = self.calculate_similarity_percentage(data1["content"], data2["content"])
                    
                    if similarity >= 90.0:
                        results["similar"].append({
                            "file1": str(file1),
                            "file2": str(file2),
                            "similarity": similarity,
                            "action": "merge_files"
                        })
                        print(f"  ğŸ”„ Ù…ØªØ´Ø§Ø¨Ù‡ {similarity:.1f}%: {file1.name} â†” {file2.name}")
                    
                    elif similarity >= 70.0:
                        results["similar"].append({
                            "file1": str(file1),
                            "file2": str(file2),
                            "similarity": similarity,
                            "action": "review_merge"
                        })
                        print(f"  ğŸ“ Ù…Ø±Ø§Ø¬Ø¹Ø© {similarity:.1f}%: {file1.name} â†” {file2.name}")
        
        # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© (Ù„Ù… ØªØ¬Ø¯ Ù…Ø·Ø§Ø¨Ù‚Ø©)
        identified_files = set()
        for group in [results["identical"], results["similar"]]:
            for item in group:
                identified_files.add(item["file1"])
                identified_files.add(item["file2"])
        
        for file_path in files:
            if str(file_path) not in identified_files:
                results["unique"].append(str(file_path))
                print(f"  ğŸ†• ÙØ±ÙŠØ¯: {file_path.name}")
        
        return results

    def execute_cleanup_actions(self, group_name: str, analysis: Dict) -> Dict:
        """ØªÙ†ÙÙŠØ° Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
        print(f"ğŸš€ ØªÙ†ÙÙŠØ° Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {group_name}")
        
        actions_taken = {
            "deleted_duplicates": 0,
            "merged_files": 0,
            "files_moved_to_delete": 0,
            "errors": []
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ø­Ø°Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        final_delete_dir = self.base_path / "deleted" / "duplicates" / "final_cleanup" / group_name
        final_delete_dir.mkdir(parents=True, exist_ok=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ø¯Ù…Ø¬
        merge_dir = self.base_path / "deleted" / "duplicates" / "merge_needed" / group_name
        merge_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100% - Ù†Ù‚Ù„ Ù„Ù„Ø­Ø°Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
            processed_files = set()
            for identical in analysis["identical"]:
                file1_path = Path(identical["file1"])
                file2_path = Path(identical["file2"])
                
                # Ø§Ø­ØªÙØ¸ Ø¨Ø£Ø­Ø¯ Ø§Ù„Ù…Ù„ÙÙŠÙ†ØŒ Ø§Ø­Ø°Ù Ø§Ù„Ø¢Ø®Ø±
                if str(file1_path) not in processed_files and str(file2_path) not in processed_files:
                    # Ø§Ø­ØªÙØ¸ Ø¨Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙˆÙ„ØŒ Ø§Ù†Ù‚Ù„ Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ù„Ø­Ø°Ù
                    if file2_path.exists():
                        target_file = final_delete_dir / file2_path.name
                        shutil.move(str(file2_path), str(target_file))
                        actions_taken["files_moved_to_delete"] += 1
                        processed_files.add(str(file2_path))
                        print(f"  âœ… Ù†Ù‚Ù„ Ù„Ù„Ø­Ø°Ù: {file2_path.name}")
                
                elif str(file2_path) not in processed_files:
                    # Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙˆÙ„ Ù…Ø¹Ø§Ù„Ø¬ØŒ Ø§Ù†Ù‚Ù„ Ø§Ù„Ø«Ø§Ù†ÙŠ
                    if file2_path.exists():
                        target_file = final_delete_dir / file2_path.name
                        shutil.move(str(file2_path), str(target_file))
                        actions_taken["files_moved_to_delete"] += 1
                        processed_files.add(str(file2_path))
                        print(f"  âœ… Ù†Ù‚Ù„ Ù„Ù„Ø­Ø°Ù: {file2_path.name}")
            
            # Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© - Ù†Ù‚Ù„ Ù„Ù„Ø¯Ù…Ø¬
            for similar in analysis["similar"]:
                file1_path = Path(similar["file1"])
                file2_path = Path(similar["file2"])
                
                # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¯Ù…Ø¬ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙŠØ¯ÙˆÙŠØ©
                if file1_path.exists() and str(file1_path) not in processed_files:
                    target_file = merge_dir / file1_path.name
                    if not target_file.exists():
                        shutil.copy2(str(file1_path), str(target_file))
                        print(f"  ğŸ”„ Ù†Ø³Ø® Ù„Ù„Ø¯Ù…Ø¬: {file1_path.name}")
                
                if file2_path.exists() and str(file2_path) not in processed_files:
                    target_file = merge_dir / file2_path.name
                    if not target_file.exists():
                        shutil.copy2(str(file2_path), str(target_file))
                        print(f"  ğŸ”„ Ù†Ø³Ø® Ù„Ù„Ø¯Ù…Ø¬: {file2_path.name}")
        
        except Exception as e:
            error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© {group_name}: {str(e)}"
            actions_taken["errors"].append(error_msg)
            print(f"  âŒ {error_msg}")
        
        return actions_taken

    def generate_detailed_analysis_report(self, all_results: Dict) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù…ÙØµÙ„"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ Ù„Ù„ØªØ´Ø§Ø¨Ù‡ ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
**Ø§Ù„ØªØ§Ø±ÙŠØ®**: {timestamp}
**Ø§Ù„Ù…Ø­Ù„Ù„**: ContentSimilarityAnalyzer v1.0

## ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„
"""
        
        total_identical = 0
        total_similar = 0
        total_unique = 0
        total_deleted = 0
        total_merged = 0
        
        for group_name, results in all_results.items():
            if 'analysis' in results:
                analysis = results['analysis']
                actions = results.get('actions', {})
                
                total_identical += len(analysis['identical'])
                total_similar += len(analysis['similar'])
                total_unique += len(analysis['unique'])
                total_deleted += actions.get('files_moved_to_delete', 0)
                total_merged += actions.get('merged_files', 0)
        
        report += f"""
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%**: {total_identical} Ø²ÙˆØ¬
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©**: {total_similar} Ø²ÙˆØ¬  
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©**: {total_unique} Ù…Ù„Ù
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø© Ù„Ù„Ø­Ø°Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ**: {total_deleted} Ù…Ù„Ù
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¹Ø¯Ø© Ù„Ù„Ø¯Ù…Ø¬**: {total_merged} Ù…Ù„Ù

## ğŸ” ØªØ­Ù„ÙŠÙ„ ØªÙØµÙŠÙ„ÙŠ Ù„ÙƒÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø©

"""
        
        for group_name, results in all_results.items():
            if 'analysis' not in results:
                continue
                
            analysis = results['analysis']
            actions = results.get('actions', {})
            
            report += f"""
### ğŸ“ Ù…Ø¬Ù…ÙˆØ¹Ø©: {group_name.replace('_', ' ').title()}

#### âœ… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100% ({len(analysis['identical'])} Ø£Ø²ÙˆØ§Ø¬)
"""
            for identical in analysis['identical']:
                file1_name = Path(identical['file1']).name
                file2_name = Path(identical['file2']).name
                report += f"- `{file1_name}` â†” `{file2_name}` (ØªØ·Ø§Ø¨Ù‚: {identical['similarity']:.1f}%)\n"
            
            report += f"""
#### ğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© ({len(analysis['similar'])} Ø£Ø²ÙˆØ§Ø¬)
"""
            for similar in analysis['similar']:
                file1_name = Path(similar['file1']).name
                file2_name = Path(similar['file2']).name
                action_text = "Ø¯Ù…Ø¬ Ù…Ø·Ù„ÙˆØ¨" if similar['action'] == 'merge_files' else "Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ø·Ù„ÙˆØ¨Ø©"
                report += f"- `{file1_name}` â†” `{file2_name}` (ØªØ´Ø§Ø¨Ù‡: {similar['similarity']:.1f}%) - {action_text}\n"
            
            report += f"""
#### ğŸ†• Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© ({len(analysis['unique'])} Ù…Ù„ÙØ§Øª)
"""
            for unique in analysis['unique']:
                unique_name = Path(unique).name
                report += f"- `{unique_name}`\n"
            
            report += f"""
#### ğŸ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ®Ø°Ø©
- **Ù…Ù„ÙØ§Øª Ù…Ù†Ù‚ÙˆÙ„Ø© Ù„Ù„Ø­Ø°Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ**: {actions.get('files_moved_to_delete', 0)}
- **Ù…Ù„ÙØ§Øª Ù…ÙØ¹Ø¯Ø© Ù„Ù„Ø¯Ù…Ø¬**: {actions.get('merged_files', 0)}
- **Ø£Ø®Ø·Ø§Ø¡**: {len(actions.get('errors', []))}
"""
            
            if actions.get('errors'):
                report += f"""
##### âš ï¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡:
"""
                for error in actions['errors']:
                    report += f"- âŒ {error}\n"
        
        report += f"""
## ğŸ“‚ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ù†Ø´Ø£Ø©

### ğŸ—‘ï¸ Ù„Ù„Ø­Ø°Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
```
deleted/duplicates/final_cleanup/
â”œâ”€â”€ ai_services/           # Ù…Ù„ÙØ§Øª AI Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%
â”œâ”€â”€ audio_services/        # Ù…Ù„ÙØ§Øª ØµÙˆØª Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100% 
â”œâ”€â”€ cache_services/        # Ù…Ù„ÙØ§Øª cache Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%
â””â”€â”€ monitoring_services/   # Ù…Ù„ÙØ§Øª Ù…Ø±Ø§Ù‚Ø¨Ø© Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%
```

### ğŸ”„ Ù„Ù„Ø¯Ù…Ø¬ (Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ© Ù…Ø·Ù„ÙˆØ¨Ø©)
```
deleted/duplicates/merge_needed/
â”œâ”€â”€ ai_services/           # Ù…Ù„ÙØ§Øª AI ØªØ­ØªØ§Ø¬ Ø¯Ù…Ø¬
â”œâ”€â”€ audio_services/        # Ù…Ù„ÙØ§Øª ØµÙˆØª ØªØ­ØªØ§Ø¬ Ø¯Ù…Ø¬
â”œâ”€â”€ cache_services/        # Ù…Ù„ÙØ§Øª cache ØªØ­ØªØ§Ø¬ Ø¯Ù…Ø¬
â””â”€â”€ monitoring_services/   # Ù…Ù„ÙØ§Øª Ù…Ø±Ø§Ù‚Ø¨Ø© ØªØ­ØªØ§Ø¬ Ø¯Ù…Ø¬
```

## ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©

### âœ… Ø¢Ù…Ù† Ù„Ù„Ø­Ø°Ù Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ `deleted/duplicates/final_cleanup/` Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%
- ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡Ø§ Ù†Ù‡Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø£Ù…Ø§Ù†

### ğŸ”„ ÙŠØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙˆØ¯Ù…Ø¬
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ `deleted/duplicates/merge_needed/` ØªØ­ØªØ§Ø¬ Ù…Ø±Ø§Ø¬Ø¹Ø© ÙŠØ¯ÙˆÙŠØ©
- Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù…Ù† ÙƒÙ„ Ù…Ù„Ù
- Ø­Ø°Ù Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ø¹ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù‡Ù…Ø©

### ğŸ“‹ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©
1. **Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ø¯Ù…Ø¬** - ÙØ­Øµ Ø§Ù„Ø§Ø®ØªÙ„Ø§ÙØ§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹
2. **Ø¯Ù…Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„ÙØ±ÙŠØ¯Ø©** - Ù†Ù‚Ù„ Ø§Ù„ÙˆØ¸Ø§Ø¦Ù Ø§Ù„Ù…Ù‡Ù…Ø©
3. **ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹** - Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø§Ù„Ù…ÙƒØ³ÙˆØ±Ø©
4. **Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„** - Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ù…Ù„ ÙƒÙ„ Ø´ÙŠØ¡

---
**ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©**: ContentSimilarityAnalyzer v1.0  
**Ø§Ù„ØªÙˆÙ‚ÙŠØª**: {timestamp}
"""
        
        return report

    def run_complete_similarity_analysis(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„ Ù„Ù„ØªØ´Ø§Ø¨Ù‡"""
        print("=" * 60)
        print("ğŸ”  CONTENT SIMILARITY ANALYZER")
        print("ğŸ¯  CHECKING 100% IDENTICAL vs MERGE NEEDED")
        print("=" * 60)
        
        deprecated_services = self.base_path / "deprecated" / "services"
        if not deprecated_services.exists():
            print("âŒ Ù…Ø¬Ù„Ø¯ deprecated/services ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯!")
            return {}
        
        service_groups = [d.name for d in deprecated_services.iterdir() if d.is_dir()]
        print(f"ğŸ“ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {service_groups}")
        
        all_results = {}
        
        for group_name in service_groups:
            print(f"\n{'='*40}")
            print(f"ğŸ“‹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø©: {group_name}")
            print(f"{'='*40}")
            
            # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            analysis = self.analyze_service_group(group_name)
            
            # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª
            actions = self.execute_cleanup_actions(group_name, analysis)
            
            all_results[group_name] = {
                "analysis": analysis,
                "actions": actions
            }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_content = self.generate_detailed_analysis_report(all_results)
        report_path = self.base_path / "deleted" / "reports" / "CONTENT_SIMILARITY_ANALYSIS.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ!")
        print(f"ğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")
        
        return all_results

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = ContentSimilarityAnalyzer()
    
    try:
        results = analyzer.run_complete_similarity_analysis()
        print(f"\nâœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()