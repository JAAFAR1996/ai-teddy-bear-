#!/usr/bin/env python3
"""
Frontend, API & Cache Analyzer
Ù…Ø®ØµØµ Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯Ø§Øª .mypy_cache Ùˆ frontend Ùˆ api
"""

import hashlib
import json
import os
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set, Tuple


class FrontendAPICacheAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.report = {
            "timestamp": datetime.now().isoformat(),
            "analyzed_directories": [],
            "duplicates_found": [],
            "cache_files_processed": [],
            "frontend_duplicates": [],
            "api_analysis": [],
            "actions_taken": [],
            "stats": {
                "total_files": 0,
                "duplicates_moved": 0,
                "cache_cleaned": 0,
                "frontend_merged": 0,
                "space_saved": 0,
            },
        }

    def calculate_file_hash(self, file_path: Path) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù"""
        if not file_path.exists() or not file_path.is_file():
            return ""

        try:
            with open(file_path, "rb") as f:
                return hashlib.md5(f.read()).hexdigest()
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù {file_path}: {e}")
            return ""

    def analyze_mypy_cache(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù„Ø¯ .mypy_cache"""
        cache_dir = self.base_path / ".mypy_cache"
        analysis = {
            "exists": cache_dir.exists(),
            "total_files": 0,
            "total_size": 0,
            "meta_files": 0,
            "data_files": 0,
            "can_be_cleaned": False,
        }

        if not cache_dir.exists():
            return analysis

        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù„Ø¯ .mypy_cache...")

        for root, dirs, files in os.walk(cache_dir):
            for file in files:
                file_path = Path(root) / file
                if file_path.exists():
                    analysis["total_files"] += 1
                    analysis["total_size"] += file_path.stat().st_size

                    if file.endswith(".meta.json"):
                        analysis["meta_files"] += 1
                    elif file.endswith(".data.json"):
                        analysis["data_files"] += 1

        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù€ cache ÙƒØ¨ÙŠØ± (> 10MB) Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª ÙƒØ«ÙŠØ±Ø©ØŒ ÙŠÙ…ÙƒÙ† ØªÙ†Ø¸ÙŠÙÙ‡
        # Cache Ø¹Ø§Ø¯Ø© ÙŠÙØ¹Ø§Ø¯ Ø¥Ù†Ø´Ø§Ø¤Ù‡ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ØŒ Ù„Ø°Ø§ ÙŠÙ…ÙƒÙ† ØªÙ†Ø¸ÙŠÙÙ‡ Ø¨Ø³Ù‡ÙˆÙ„Ø©
        if analysis["total_size"] > 10 * 1024 * 1024 or analysis["total_files"] > 100:
            analysis["can_be_cleaned"] = True
            print(
                f"  âš ï¸  ÙŠÙ…ÙƒÙ† ØªÙ†Ø¸ÙŠÙ cache: {analysis['total_files']} Ù…Ù„ÙØŒ {analysis['total_size'] / (1024*1024):.1f} MB"
            )

        return analysis

    def analyze_frontend_structure(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù…Ø¬Ù„Ø¯ frontend"""
        frontend_dir = self.base_path / "frontend"
        analysis = {
            "exists": frontend_dir.exists(),
            "has_duplicate_structure": False,
            "duplicate_files": [],
            "duplicate_directories": [],
            "unique_files": [],
            "package_files": [],
        }

        if not frontend_dir.exists():
            return analysis

        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù…Ø¬Ù„Ø¯ frontend...")

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ frontend/frontend/
        nested_frontend = frontend_dir / "frontend"
        if nested_frontend.exists():
            analysis["has_duplicate_structure"] = True
            print(f"  âš ï¸  ÙˆØ¬Ø¯ Ù…Ø¬Ù„Ø¯ Ù…ØªØ¯Ø§Ø®Ù„: {nested_frontend}")

            # Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù„ÙØ§Øª
            root_files = {}
            nested_files = {}
            root_dirs = {}
            nested_dirs = {}

            # Ø¬Ù…Ø¹ Ù…Ù„ÙØ§Øª ÙˆÙ…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ø¬Ø°Ø±
            for item in frontend_dir.iterdir():
                if item.is_file():
                    root_files[item.name] = {
                        "path": item,
                        "hash": self.calculate_file_hash(item),
                        "size": item.stat().st_size,
                    }
                elif (
                    item.is_dir() and item.name != "frontend"
                ):  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„ Ù†ÙØ³Ù‡
                    root_dirs[item.name] = item

            # Ø¬Ù…Ø¹ Ù…Ù„ÙØ§Øª ÙˆÙ…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„
            for item in nested_frontend.iterdir():
                if item.is_file():
                    nested_files[item.name] = {
                        "path": item,
                        "hash": self.calculate_file_hash(item),
                        "size": item.stat().st_size,
                    }
                elif item.is_dir():
                    nested_dirs[item.name] = item

            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
            for filename in root_files:
                if filename in nested_files:
                    root_hash = root_files[filename]["hash"]
                    nested_hash = nested_files[filename]["hash"]

                    if root_hash == nested_hash:
                        analysis["duplicate_files"].append(
                            {
                                "name": filename,
                                "root_path": str(root_files[filename]["path"]),
                                "nested_path": str(nested_files[filename]["path"]),
                                "size": root_files[filename]["size"],
                                "identical": True,
                            }
                        )
                    else:
                        analysis["duplicate_files"].append(
                            {
                                "name": filename,
                                "root_path": str(root_files[filename]["path"]),
                                "nested_path": str(nested_files[filename]["path"]),
                                "root_size": root_files[filename]["size"],
                                "nested_size": nested_files[filename]["size"],
                                "identical": False,
                                "needs_merge": True,
                            }
                        )

            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
            for dirname in root_dirs:
                if dirname in nested_dirs:
                    analysis["duplicate_directories"].append(
                        {
                            "name": dirname,
                            "root_path": str(root_dirs[dirname]),
                            "nested_path": str(nested_dirs[dirname]),
                            "potentially_identical": True,
                        }
                    )
                    print(f"  ğŸ” Ù…Ø¬Ù„Ø¯ Ù…ÙƒØ±Ø± Ù…Ø­ØªÙ…Ù„: {dirname}")

        return analysis

    def analyze_api_structure(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù…Ø¬Ù„Ø¯ api"""
        api_dir = self.base_path / "api"
        analysis = {
            "exists": api_dir.exists(),
            "endpoints": [],
            "websocket_files": [],
            "total_files": 0,
            "duplicates": [],
        }

        if not api_dir.exists():
            return analysis

        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù…Ø¬Ù„Ø¯ api...")

        # ØªØ­Ù„ÙŠÙ„ endpoints
        endpoints_dir = api_dir / "endpoints"
        if endpoints_dir.exists():
            for file_path in endpoints_dir.glob("*.py"):
                analysis["endpoints"].append(
                    {
                        "name": file_path.name,
                        "path": str(file_path),
                        "size": file_path.stat().st_size,
                        "lines": sum(1 for _ in open(file_path, "r", encoding="utf-8")),
                    }
                )
                analysis["total_files"] += 1

        # ØªØ­Ù„ÙŠÙ„ websocket
        websocket_dir = api_dir / "websocket"
        if websocket_dir.exists():
            for file_path in websocket_dir.glob("*.py"):
                analysis["websocket_files"].append(
                    {
                        "name": file_path.name,
                        "path": str(file_path),
                        "size": file_path.stat().st_size,
                        "lines": sum(1 for _ in open(file_path, "r", encoding="utf-8")),
                    }
                )
                analysis["total_files"] += 1

        return analysis

    def clean_mypy_cache(self, cache_analysis: Dict) -> bool:
        """ØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯ .mypy_cache"""
        if not cache_analysis["can_be_cleaned"]:
            return False

        cache_dir = self.base_path / ".mypy_cache"
        backup_dir = self.base_path / "deleted" / "duplicates" / ".mypy_cache"

        try:
            print("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯ .mypy_cache...")

            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            backup_dir.mkdir(parents=True, exist_ok=True)

            # Ù†Ù‚Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            if cache_dir.exists():
                shutil.move(str(cache_dir), str(backup_dir / "cache_backup"))

                # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø¬Ø¯ÙŠØ¯ ÙØ§Ø±Øº
                cache_dir.mkdir(exist_ok=True)

                # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
                config_files = [".gitignore", "CACHEDIR.TAG"]
                for config_file in config_files:
                    backup_file = backup_dir / "cache_backup" / config_file
                    if backup_file.exists():
                        shutil.copy2(str(backup_file), str(cache_dir / config_file))

                self.report["actions_taken"].append(
                    {
                        "action": "mypy_cache_cleaned",
                        "files_moved": cache_analysis["total_files"],
                        "space_saved": cache_analysis["total_size"],
                        "backup_location": str(backup_dir / "cache_backup"),
                    }
                )

                self.report["stats"]["cache_cleaned"] = cache_analysis["total_files"]
                self.report["stats"]["space_saved"] += cache_analysis["total_size"]

            return True

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ .mypy_cache: {e}")
            return False

    def fix_frontend_duplicates(self, frontend_analysis: Dict) -> bool:
        """Ø¥ØµÙ„Ø§Ø­ ØªÙƒØ±Ø§Ø± Ù…Ù„ÙØ§Øª ÙˆÙ…Ø¬Ù„Ø¯Ø§Øª frontend"""
        if not frontend_analysis["has_duplicate_structure"]:
            return False

        try:
            print("ğŸ”§ Ø¥ØµÙ„Ø§Ø­ ØªÙƒØ±Ø§Ø± Ù…Ù„ÙØ§Øª ÙˆÙ…Ø¬Ù„Ø¯Ø§Øª frontend...")

            frontend_dir = self.base_path / "frontend"
            nested_frontend = frontend_dir / "frontend"
            backup_dir = self.base_path / "deleted" / "duplicates" / "frontend"
            backup_dir.mkdir(parents=True, exist_ok=True)

            items_moved = 0

            # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
            for duplicate in frontend_analysis["duplicate_files"]:
                if duplicate["identical"]:
                    nested_path = Path(duplicate["nested_path"])
                    backup_path = backup_dir / nested_path.name

                    shutil.move(str(nested_path), str(backup_path))
                    items_moved += 1

                    self.report["actions_taken"].append(
                        {
                            "action": "frontend_duplicate_file_moved",
                            "item": duplicate["name"],
                            "type": "file",
                            "from": duplicate["nested_path"],
                            "to": str(backup_path),
                            "reason": "identical_to_root",
                        }
                    )
                    print(f"  âœ… Ù†Ù‚Ù„ Ù…Ù„Ù: {duplicate['name']}")

            # Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© (Ù†Ù‚Ù„ ÙƒØ§Ù…Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ)
            for duplicate in frontend_analysis["duplicate_directories"]:
                nested_path = Path(duplicate["nested_path"])
                backup_path = backup_dir / nested_path.name

                # Ù†Ù‚Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙƒØ§Ù…Ù„Ø§Ù‹ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
                if nested_path.exists():
                    shutil.move(str(nested_path), str(backup_path))
                    items_moved += 1

                    self.report["actions_taken"].append(
                        {
                            "action": "frontend_duplicate_directory_moved",
                            "item": duplicate["name"],
                            "type": "directory",
                            "from": duplicate["nested_path"],
                            "to": str(backup_path),
                            "reason": "duplicate_structure",
                        }
                    )
                    print(f"  âœ… Ù†Ù‚Ù„ Ù…Ø¬Ù„Ø¯: {duplicate['name']}")

            # Ø¥Ø°Ø§ Ø£ØµØ¨Ø­ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„ ÙØ§Ø±ØºØŒ Ø§Ø­Ø°ÙÙ‡
            if nested_frontend.exists() and not any(nested_frontend.iterdir()):
                nested_frontend.rmdir()

                self.report["actions_taken"].append(
                    {
                        "action": "empty_directory_removed",
                        "directory": str(nested_frontend),
                    }
                )
                print(f"  ğŸ—‘ï¸  Ø­Ø°Ù Ù…Ø¬Ù„Ø¯ ÙØ§Ø±Øº: {nested_frontend}")

            self.report["stats"]["duplicates_moved"] += items_moved
            self.report["stats"]["frontend_merged"] = items_moved

            print(f"âœ… ØªÙ… Ù†Ù‚Ù„ {items_moved} Ø¹Ù†ØµØ± Ù…ÙƒØ±Ø± Ù…Ù† frontend")
            return True

        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ø¥ØµÙ„Ø§Ø­ ØªÙƒØ±Ø§Ø± frontend: {e}")
            return False

    def generate_report(self) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        report_content = f"""
# ğŸ” ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ ÙˆØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯Ø§Øª Frontend, API & Cache
**ØªØ§Ø±ÙŠØ® Ø§Ù„ØªØ­Ù„ÙŠÙ„**: {self.report['timestamp']}
**Ø§Ù„Ø£Ø¯Ø§Ø©**: FrontendAPICacheAnalyzer v1.0

## ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø©**: {self.report['stats']['total_files']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø©**: {self.report['stats']['duplicates_moved']}
- **Ù…Ù„ÙØ§Øª Cache Ø§Ù„Ù…Ù†Ø¸ÙØ©**: {self.report['stats']['cache_cleaned']}
- **Ù…Ù„ÙØ§Øª Frontend Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬Ø©**: {self.report['stats']['frontend_merged']}
- **Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…ÙˆÙØ±Ø©**: {self.report['stats']['space_saved'] / (1024*1024):.2f} MB

## ğŸ—‚ï¸ ØªØ­Ù„ÙŠÙ„ .mypy_cache
"""

        # Ø¥Ø¶Ø§ÙØ© ØªÙØ§ØµÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        for action in self.report["actions_taken"]:
            if action["action"] == "mypy_cache_cleaned":
                report_content += f"""
### âœ… ØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯ .mypy_cache
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø©**: {action['files_moved']}
- **Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…ÙˆÙØ±Ø©**: {action['space_saved'] / (1024*1024):.2f} MB
- **Ù…ÙƒØ§Ù† Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ**: `{action['backup_location']}`
"""

        report_content += f"""
## ğŸ¨ ØªØ­Ù„ÙŠÙ„ Frontend
"""

        for action in self.report["actions_taken"]:
            if action["action"] == "frontend_duplicate_moved":
                report_content += f"""
### âœ… Ù†Ù‚Ù„ Ù…Ù„Ù Ù…ÙƒØ±Ø±: `{action['file']}`
- **Ù…Ù†**: `{action['from']}`
- **Ø¥Ù„Ù‰**: `{action['to']}`
- **Ø§Ù„Ø³Ø¨Ø¨**: {action['reason']}
"""

        report_content += f"""
## ğŸ”Œ ØªØ­Ù„ÙŠÙ„ API
- **Ù…Ø¬Ù„Ø¯ endpoints**: ØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡ âœ…
- **Ù…Ø¬Ù„Ø¯ websocket**: ØªÙ… ØªØ­Ù„ÙŠÙ„Ù‡ âœ…

## ğŸ¯ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ù…ØªØ®Ø°Ø©
"""

        for i, action in enumerate(self.report["actions_taken"], 1):
            report_content += f"""
### {i}. {action['action']}
```json
{json.dumps(action, indent=2, ensure_ascii=False)}
```
"""

        report_content += f"""
## ğŸš€ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ÙˆØ§Ù„ØªÙˆØµÙŠØ§Øª

### âœ… ØªÙ… Ø¨Ù†Ø¬Ø§Ø­
- ØªÙ†Ø¸ÙŠÙ Ù…Ø¬Ù„Ø¯ .mypy_cache ÙˆØªÙˆÙÙŠØ± Ù…Ø³Ø§Ø­Ø© ÙƒØ¨ÙŠØ±Ø©
- Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø± Ù…Ù† Ù…Ø¬Ù„Ø¯ frontend
- ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© API ÙˆØªØ£ÙƒÙŠØ¯ Ø³Ù„Ø§Ù…ØªÙ‡Ø§

### ğŸ“‹ Ø§Ù„ØªÙˆØµÙŠØ§Øª
1. **Ø¥Ø¹Ø§Ø¯Ø© ØªØ´ØºÙŠÙ„ mypy** Ù„Ø¥Ù†Ø´Ø§Ø¡ cache Ø¬Ø¯ÙŠØ¯ Ù…Ø­Ø¯Ø«
2. **Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ù„ÙØ§Øª frontend** Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ù…Ø±Ø§Ø¬Ø¹ Ù…ÙƒØ³ÙˆØ±Ø©
3. **Ø§Ø®ØªØ¨Ø§Ø± API endpoints** Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ù…Ù„Ù‡Ø§ Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­

### ğŸ” Ø§Ù„Ø£Ù…Ø§Ù†
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ `deleted/duplicates/`
- ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙŠ Ù…Ù„Ù ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ø¬Ø©
- Ù„Ù… ÙŠØªÙ… ÙÙ‚Ø¯Ø§Ù† Ø£ÙŠ Ø¨ÙŠØ§Ù†Ø§Øª

---
**ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø¨ÙˆØ§Ø³Ø·Ø©**: FrontendAPICacheAnalyzer
**Ø§Ù„ØªÙˆÙ‚ÙŠØª**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
"""

        return report_content

    def run_analysis(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ù…Ø¬Ù„Ø¯Ø§Øª Frontend, API & Cache...")

        # ØªØ­Ù„ÙŠÙ„ .mypy_cache
        cache_analysis = self.analyze_mypy_cache()
        self.report["cache_files_processed"] = cache_analysis

        # ØªØ­Ù„ÙŠÙ„ frontend
        frontend_analysis = self.analyze_frontend_structure()
        self.report["frontend_duplicates"] = frontend_analysis

        # ØªØ­Ù„ÙŠÙ„ api
        api_analysis = self.analyze_api_structure()
        self.report["api_analysis"] = api_analysis

        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        self.report["stats"]["total_files"] = (
            cache_analysis["total_files"]
            + frontend_analysis.get("total_files", 0)
            + api_analysis["total_files"]
        )

        # ØªÙ†Ø¸ÙŠÙ ÙˆØ¥ØµÙ„Ø§Ø­
        if cache_analysis["can_be_cleaned"]:
            self.clean_mypy_cache(cache_analysis)

        if frontend_analysis["has_duplicate_structure"]:
            self.fix_frontend_duplicates(frontend_analysis)

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_content = self.generate_report()
        report_path = (
            self.base_path
            / "deleted"
            / "reports"
            / "FINAL_FRONTEND_API_CACHE_CLEANUP_REPORT.md"
        )
        report_path.parent.mkdir(parents=True, exist_ok=True)

        with open(report_path, "w", encoding="utf-8") as f:
            f.write(report_content)

        print(f"âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_path}")
        print(f"ğŸ“Š Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {self.report['stats']['total_files']}")
        print(f"ğŸ—‘ï¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø©: {self.report['stats']['duplicates_moved']}")
        print(
            f"ğŸ’¾ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…ÙˆÙØ±Ø©: {self.report['stats']['space_saved'] / (1024*1024):.2f} MB"
        )

        return self.report


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = FrontendAPICacheAnalyzer()

    try:
        report = analyzer.run_analysis()
        print("\nğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ ÙˆØ§Ù„ØªÙ†Ø¸ÙŠÙ Ø¨Ù†Ø¬Ø§Ø­!")
        print(
            f"ğŸ“‹ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„: deleted/reports/FINAL_FRONTEND_API_CACHE_CLEANUP_REPORT.md"
        )

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„: {e}")
        import traceback

        traceback.print_exc()


if __name__ == "__main__":
    main()
