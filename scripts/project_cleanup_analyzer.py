#!/usr/bin/env python3
"""
AI Teddy Bear Project Cleanup Analyzer
ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ ÙˆØ¥Ø²Ø§Ù„Ø© Ø§Ù„ÙÙˆØ¶Ù‰
"""

import os
import hashlib
import ast
import re
from pathlib import Path
from collections import defaultdict
import json
from typing import List, Dict, Any, Tuple
import shutil
from datetime import datetime

class ProjectCleanupAnalyzer:
    """Ù…Ø­Ù„Ù„ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„Ø°ÙƒÙŠ"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.analysis_results = {
            "total_files": 0,
            "total_directories": 0,
            "file_types": defaultdict(int),
            "duplicate_candidates": [],
            "large_files": [],
            "empty_files": [],
            "test_files": [],
            "config_files": [],
            "critical_files": [],
            "trash_files": [],
            "detailed_analysis": [],
            "hash_map": defaultdict(list),
            "import_dependencies": defaultdict(set),
            "suggested_moves": [],
            "security_issues": []
        }
        
        # ØªØ¹Ø±ÙŠÙ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù‡Ù…Ø©
        self.critical_patterns = [
            'main.py', 'app.py', 'wsgi.py', '__main__.py',
            'security/', 'auth/', 'child_safety/',
            'models/', 'entities/', 'domain/',
            'api/endpoints/', 'core/'
        ]
        
        self.trash_patterns = [
            r'.*_old\.py$', r'.*_backup\.py$', r'.*_temp\.py$',
            r'.*_copy\.py$', r'.*\.pyc$', r'__pycache__',
            r'\.pytest_cache', r'.*_test\.py$'  # Ø¥Ø°Ø§ ÙƒØ§Ù† ÙØ§Ø±ØºØ§Ù‹
        ]
        
        self.ignore_dirs = {
            '.git', '__pycache__', 'node_modules', 
            '.venv', 'venv', '.pytest_cache', '.mypy_cache'
        }

    def analyze_project(self) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
        
        # 1. Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        self._scan_all_files()
        
        # 2. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        self._find_duplicates()
        
        # 3. ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª
        self._analyze_dependencies()
        
        # 4. Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª
        self._suggest_improvements()
        
        # 5. Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ±
        self._generate_report()
        
        return self.analysis_results

    def _scan_all_files(self):
        """Ù…Ø³Ø­ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        for root, dirs, files in os.walk(self.project_root):
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
            self.analysis_results["total_directories"] += len(dirs)
            
            for file in files:
                self.analysis_results["total_files"] += 1
                file_path = Path(root) / file
                
                # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
                file_ext = file_path.suffix.lower()
                self.analysis_results["file_types"][file_ext] += 1
                
                # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Python
                if file_ext == '.py':
                    self._analyze_python_file(file_path)
                elif file_ext in ['.json', '.yaml', '.yml', '.ini', '.env']:
                    self._analyze_config_file(file_path)

    def _analyze_python_file(self, file_path: Path):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Python ÙˆØ§Ø­Ø¯"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                lines = content.splitlines()
            
            # Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù
            file_hash = hashlib.md5(content.encode()).hexdigest()
            self.analysis_results["hash_map"][file_hash].append(str(file_path))
            
            # Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø£Ø³Ø§Ø³ÙŠØ©
            file_size = file_path.stat().st_size
            is_empty = len(content.strip()) == 0
            
            # ØªØ­Ù„ÙŠÙ„ AST Ø¥Ø°Ø§ Ø£Ù…ÙƒÙ†
            ast_info = self._analyze_ast(content, file_path)
            
            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
            file_type = self._determine_file_type(str(file_path), content)
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            importance = self._determine_importance(
                str(file_path), content, 
                ast_info['classes'], ast_info['functions']
            )
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            issues = self._find_issues(content, str(file_path))
            
            # Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø¬Ø¯ÙŠØ¯
            suggested_location = self._suggest_location(str(file_path), file_type)
            
            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„Ù
            file_report = {
                "path": str(file_path),
                "type": file_type,
                "importance": importance,
                "size": file_size,
                "lines": len(lines),
                "is_empty": is_empty,
                "stats": ast_info,
                "hash": file_hash,
                "issues": issues,
                "suggested_location": suggested_location,
                "imports": ast_info.get('imports', [])
            }
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª
            self._categorize_file(file_report)
            
            # Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙØµÙŠÙ„ÙŠ
            self.analysis_results["detailed_analysis"].append(file_report)
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")

    def _analyze_ast(self, content: str, file_path: Path) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ AST Ù„Ù„Ù…Ù„Ù"""
        try:
            tree = ast.parse(content)
            
            imports = []
            import_froms = []
            classes = []
            functions = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        imports.append(alias.name)
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        import_froms.append(node.module)
                elif isinstance(node, ast.ClassDef):
                    classes.append({
                        'name': node.name,
                        'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                    })
                elif isinstance(node, ast.FunctionDef):
                    functions.append({
                        'name': node.name,
                        'args': len(node.args.args),
                        'lines': node.end_lineno - node.lineno if hasattr(node, 'end_lineno') else 0
                    })
            
            # ØªØ­Ø¯ÙŠØ« import dependencies
            all_imports = imports + import_froms
            for imp in all_imports:
                self.analysis_results["import_dependencies"][str(file_path)].add(imp)
            
            return {
                'imports': all_imports,
                'classes': len(classes),
                'functions': len(functions),
                'class_details': classes,
                'function_details': functions
            }
            
        except:
            return {
                'imports': [],
                'classes': 0,
                'functions': 0,
                'class_details': [],
                'function_details': []
            }

    def _determine_file_type(self, file_path: str, content: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù"""
        path_lower = file_path.lower()
        
        if 'test_' in path_lower or '_test.py' in path_lower:
            return 'test'
        elif 'config' in path_lower:
            return 'config'
        elif any(x in path_lower for x in ['model', 'entity', 'entities']):
            return 'model'
        elif 'service' in path_lower:
            return 'service'
        elif any(x in path_lower for x in ['repository', 'repo']):
            return 'repository'
        elif any(x in path_lower for x in ['controller', 'endpoint', 'route', 'api']):
            return 'controller'
        elif any(x in path_lower for x in ['util', 'helper']):
            return 'utility'
        elif '__init__.py' in path_lower:
            return 'init'
        elif 'migration' in path_lower:
            return 'migration'
        elif 'exception' in path_lower:
            return 'exception'
        else:
            return 'other'

    def _determine_importance(self, file_path: str, content: str, 
                            num_classes: int, num_functions: int) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù"""
        path_lower = file_path.lower()
        
        # Critical files
        if any(pattern in path_lower for pattern in self.critical_patterns):
            return 'critical'
        
        # Check for security-related content
        security_keywords = ['auth', 'security', 'child_safety', 'encryption', 'token']
        if any(keyword in path_lower or keyword in content.lower() 
               for keyword in security_keywords):
            return 'critical'
        
        # Empty or trash files
        if len(content.strip()) == 0:
            return 'trash'
        
        # Old/backup files
        if any(re.match(pattern, file_path) for pattern in self.trash_patterns):
            return 'trash'
        
        # High importance - substantial code
        if num_classes > 2 or num_functions > 5:
            return 'high'
        
        # Service/Repository files
        if any(x in path_lower for x in ['service', 'repository', 'controller']):
            return 'high'
        
        # Low importance - minimal code
        if num_classes == 0 and num_functions <= 1:
            return 'low'
        
        return 'medium'

    def _find_issues(self, content: str, file_path: str) -> List[str]:
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        issues = []
        
        # ÙØ­Øµ Ø§Ù„Ø£Ù…Ø§Ù†
        if 'eval(' in content or 'exec(' in content:
            issues.append("ğŸš¨ Security: Uses eval/exec")
            self.analysis_results["security_issues"].append({
                'file': file_path,
                'issue': 'eval/exec usage'
            })
        
        # ÙØ­Øµ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡
        if re.search(r'except\s*:', content) or re.search(r'except\s+Exception\s*:', content):
            issues.append("âš ï¸ Generic exception handling")
        
        # ÙØ­Øµ print statements
        if 'print(' in content and 'test' not in file_path.lower():
            issues.append("ğŸ“ Contains print statements")
        
        # ÙØ­Øµ TODOs
        if 'TODO' in content or 'FIXME' in content:
            issues.append("ğŸ“Œ Contains TODO/FIXME")
        
        # ÙØ­Øµ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
        lines = content.splitlines()
        if len(lines) > 500:
            issues.append(f"ğŸ“ File too large ({len(lines)} lines)")
        
        # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        if len(content.strip()) == 0:
            issues.append("ğŸ“„ Empty file")
        
        # ÙØ­Øµ hardcoded values
        if re.search(r'(password|token|secret)\s*=\s*["\']', content, re.IGNORECASE):
            issues.append("ğŸ”‘ Possible hardcoded secrets")
            self.analysis_results["security_issues"].append({
                'file': file_path,
                'issue': 'hardcoded secrets'
            })
        
        return issues

    def _suggest_location(self, current_path: str, file_type: str) -> str:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø£ÙØ¶Ù„ Ù„Ù„Ù…Ù„Ù"""
        type_to_location = {
            'model': 'src/core/domain/entities/',
            'service': 'src/core/services/',
            'repository': 'src/infrastructure/persistence/repositories/',
            'controller': 'src/api/endpoints/',
            'test': 'tests/',
            'config': 'configs/',
            'utility': 'src/shared/utils/',
            'exception': 'src/core/domain/exceptions/',
            'migration': 'src/infrastructure/persistence/migrations/'
        }
        
        suggested = type_to_location.get(file_type, 'src/')
        
        # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ
        if suggested in current_path:
            return None  # Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­
            
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯
        filename = Path(current_path).name
        return suggested + filename

    def _categorize_file(self, file_report: Dict[str, Any]):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù…Ù†Ø§Ø³Ø¨Ø©"""
        if file_report['is_empty']:
            self.analysis_results["empty_files"].append(file_report['path'])
        
        if file_report['size'] > 100_000:  # 100KB
            self.analysis_results["large_files"].append({
                'path': file_report['path'],
                'size': file_report['size']
            })
        
        if file_report['type'] == 'test':
            self.analysis_results["test_files"].append(file_report['path'])
        
        if file_report['type'] == 'config':
            self.analysis_results["config_files"].append(file_report['path'])
        
        if file_report['importance'] == 'critical':
            self.analysis_results["critical_files"].append(file_report['path'])
        
        if file_report['importance'] == 'trash':
            self.analysis_results["trash_files"].append(file_report['path'])
        
        if file_report['suggested_location']:
            self.analysis_results["suggested_moves"].append({
                'from': file_report['path'],
                'to': file_report['suggested_location'],
                'reason': f"Better organization for {file_report['type']} file"
            })

    def _analyze_config_file(self, file_path: Path):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ†"""
        try:
            size = file_path.stat().st_size
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø£Ø³Ø±Ø§Ø± Ù…Ø­ØªÙ…Ù„Ø©
            if re.search(r'(api_key|secret|password|token)', content, re.IGNORECASE):
                self.analysis_results["security_issues"].append({
                    'file': str(file_path),
                    'issue': 'Contains potential secrets in config'
                })
            
            self.analysis_results["config_files"].append(str(file_path))
            
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† {file_path}: {e}")

    def _find_duplicates(self):
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        print("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
        
        # Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© (Ù†ÙØ³ Ø§Ù„Ù€ hash)
        for file_hash, files in self.analysis_results["hash_map"].items():
            if len(files) > 1:
                self.analysis_results["duplicate_candidates"].append({
                    'type': 'exact',
                    'hash': file_hash,
                    'files': files,
                    'count': len(files)
                })
        
        # Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø§Ù„ÙˆØ¸ÙŠÙÙŠ (ØªØ­Ù„ÙŠÙ„ Ø£Ø¹Ù…Ù‚)
        self._find_functional_duplicates()

    def _find_functional_duplicates(self):
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø© ÙˆØ¸ÙŠÙÙŠØ§Ù‹"""
        function_signatures = defaultdict(list)
        
        for file_info in self.analysis_results["detailed_analysis"]:
            if file_info['type'] in ['service', 'utility', 'helper']:
                # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„
                for func in file_info['stats'].get('function_details', []):
                    sig = f"{func['name']}({func['args']})"
                    function_signatures[sig].append(file_info['path'])
        
        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª
        for sig, files in function_signatures.items():
            if len(files) > 1:
                self.analysis_results["duplicate_candidates"].append({
                    'type': 'functional',
                    'signature': sig,
                    'files': files,
                    'count': len(files)
                })

    def _analyze_dependencies(self):
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø¨ÙŠÙ† Ø§Ù„Ù…Ù„ÙØ§Øª"""
        print("ğŸ”— ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª...")
        
        # Ù‡Ø°Ø§ ÙŠØ³Ø§Ø¹Ø¯ ÙÙŠ Ù…Ø¹Ø±ÙØ© Ø£ÙŠ Ù…Ù„ÙØ§Øª ÙŠÙ…ÙƒÙ† Ø¯Ù…Ø¬Ù‡Ø§ Ø£Ùˆ Ù†Ù‚Ù„Ù‡Ø§
        dependency_graph = defaultdict(set)
        
        for file_path, imports in self.analysis_results["import_dependencies"].items():
            for imp in imports:
                # ØªØ­ÙˆÙŠÙ„ import Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø± Ù…Ù„Ù Ù…Ø­ØªÙ…Ù„
                if imp.startswith('.'):
                    # relative import
                    continue
                    
                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªÙˆØ±Ø¯ ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
                possible_files = self._find_imported_file(imp)
                for pf in possible_files:
                    dependency_graph[file_path].add(pf)

    def _find_imported_file(self, import_name: str) -> List[str]:
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø³ØªÙˆØ±Ø¯"""
        possible_files = []
        
        # ØªØ­ÙˆÙŠÙ„ import Ø¥Ù„Ù‰ Ù…Ø³Ø§Ø±
        path_parts = import_name.split('.')
        possible_path = os.path.join(*path_parts) + '.py'
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
        for file_info in self.analysis_results["detailed_analysis"]:
            if possible_path in file_info['path']:
                possible_files.append(file_info['path'])
        
        return possible_files

    def _suggest_improvements(self):
        """Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª"""
        print("ğŸ’¡ Ø§Ù‚ØªØ±Ø§Ø­ Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª...")
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        improvements = {
            'merge_candidates': [],
            'refactor_candidates': [],
            'security_fixes': []
        }
        
        # Ø§Ù‚ØªØ±Ø§Ø­ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
        utility_files = [
            f for f in self.analysis_results["detailed_analysis"]
            if f['type'] == 'utility' and f['lines'] < 50
        ]
        
        if len(utility_files) > 3:
            improvements['merge_candidates'].append({
                'files': [f['path'] for f in utility_files],
                'target': 'src/shared/utils/common_utils.py',
                'reason': 'Small utility files can be merged'
            })
        
        self.analysis_results['improvements'] = improvements

    def _generate_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        print("ğŸ“Š Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±...")
        
        report = {
            'summary': {
                'total_files': self.analysis_results['total_files'],
                'total_directories': self.analysis_results['total_directories'],
                'critical_files': len(self.analysis_results['critical_files']),
                'trash_files': len(self.analysis_results['trash_files']),
                'duplicate_files': len(self.analysis_results['duplicate_candidates']),
                'files_to_move': len(self.analysis_results['suggested_moves']),
                'security_issues': len(self.analysis_results['security_issues'])
            },
            'file_types': dict(self.analysis_results['file_types']),
            'duplicates': self.analysis_results['duplicate_candidates'],
            'trash_files': self.analysis_results['trash_files'],
            'suggested_moves': self.analysis_results['suggested_moves'][:20],  # Ø£ÙˆÙ„ 20 Ø§Ù‚ØªØ±Ø§Ø­
            'security_issues': self.analysis_results['security_issues']
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open('cleanup_analysis_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± markdown
        self._create_markdown_report(report)

    def _create_markdown_report(self, report: Dict[str, Any]):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown"""
        markdown = f"""# ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear

## ğŸ“… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªØ­Ù„ÙŠÙ„
- **Ø§Ù„ØªØ§Ø±ÙŠØ®**: {datetime.now().strftime('%Y-%m-%d %H:%M')}
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª**: {report['summary']['total_files']}
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª**: {report['summary']['total_directories']}

## ğŸš¨ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‡Ù…Ø©

### ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø±Ø¬Ø©**: {report['summary']['critical_files']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø©**: {report['summary']['trash_files']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©**: {report['summary']['duplicate_files']}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ù†Ù‚Ù„**: {report['summary']['files_to_move']}
- **Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù†**: {report['summary']['security_issues']}

### ğŸ“Š Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª
"""
        for ext, count in sorted(report['file_types'].items(), key=lambda x: x[1], reverse=True)[:10]:
            markdown += f"- `{ext}`: {count} Ù…Ù„Ù\n"
        
        markdown += "\n### ğŸ—‘ï¸ Ù…Ù„ÙØ§Øª Ù„Ù„Ø­Ø°Ù Ø§Ù„ÙÙˆØ±ÙŠ\n"
        for trash_file in report['trash_files'][:20]:
            markdown += f"- `{trash_file}`\n"
        
        markdown += "\n### ğŸ”„ Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø©\n"
        for dup in report['duplicates'][:10]:
            markdown += f"\n#### {dup['type']} duplicate ({dup['count']} files)\n"
            for file in dup['files'][:5]:
                markdown += f"- `{file}`\n"
        
        markdown += "\n### ğŸ“‚ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„Ù†Ù‚Ù„\n"
        for move in report['suggested_moves'][:15]:
            markdown += f"- **Ù…Ù†**: `{move['from']}`\n"
            markdown += f"  **Ø¥Ù„Ù‰**: `{move['to']}`\n"
            markdown += f"  **Ø§Ù„Ø³Ø¨Ø¨**: {move['reason']}\n\n"
        
        if report['security_issues']:
            markdown += "\n### ğŸ” Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù†\n"
            for issue in report['security_issues'][:10]:
                markdown += f"- **{issue['issue']}** ÙÙŠ `{issue['file']}`\n"
        
        markdown += "\n## ğŸ¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©\n"
        markdown += "1. Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø©\n"
        markdown += "2. Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©\n"
        markdown += "3. Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ø£Ù…Ø§ÙƒÙ† Ø§Ù„ØµØ­ÙŠØ­Ø©\n"
        markdown += "4. Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù†\n"
        markdown += "5. ØªÙ†Ø¸ÙŠÙ ÙˆØ¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© Ø§Ù„ÙƒÙˆØ¯\n"
        
        with open('cleanup_analysis_report.md', 'w', encoding='utf-8') as f:
            f.write(markdown)


if __name__ == "__main__":
    analyzer = ProjectCleanupAnalyzer()
    results = analyzer.analyze_project()
    
    print("\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„ØªØ­Ù„ÙŠÙ„!")
    print(f"ğŸ“Š ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±:")
    print("   - cleanup_analysis_report.json")
    print("   - cleanup_analysis_report.md")
    print(f"\nğŸ—‘ï¸ Ù…Ù„ÙØ§Øª Ù„Ù„Ø­Ø°Ù: {len(results['trash_files'])}")
    print(f"ğŸ”„ Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø©: {len(results['duplicate_candidates'])}")
    print(f"ğŸ“‚ Ù…Ù„ÙØ§Øª Ù„Ù„Ù†Ù‚Ù„: {len(results['suggested_moves'])}") 