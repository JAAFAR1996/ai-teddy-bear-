import ast
import hashlib
import json
import os
import re
import shutil
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple


class ProjectCleaner:
    """Ù…Ù†Ø¸Ù Ø´Ø§Ù…Ù„ Ù„Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear"""

    def __init__(
        self,
        project_root: str = ".",
        analysis_file: str = "scripts/project_analysis.json",
    ):
        self.project_root = Path(project_root)
        self.analysis_file = analysis_file
        self.backup_dir = f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.report = {
            "deleted_files": [],
            "moved_files": [],
            "merged_files": [],
            "errors": [],
            "warnings": [],
        }

        # Ù‚Ø±Ø§Ø¡Ø© ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„
        try:
            with open(analysis_file, "r", encoding="utf-8") as f:
                self.analysis = json.load(f)
        except FileNotFoundError:
            print(
                "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„. ÙŠØ±Ø¬Ù‰ ØªØ´ØºÙŠÙ„ project_analyzer.py Ø£ÙˆÙ„Ø§Ù‹"
            )
            sys.exit(1)

    def run_full_cleanup(self, dry_run: bool = False):
        """ØªØ´ØºÙŠÙ„ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„ÙƒØ§Ù…Ù„Ø©"""
        print("ğŸ§¹ Ø¨Ø¯Ø¡ ØªÙ†Ø¸ÙŠÙ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear...")
        print(f"ğŸ“ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ: {self.backup_dir}")

        if dry_run:
            print("âš ï¸  ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© - Ù„Ù† ÙŠØªÙ… ØªÙ†ÙÙŠØ° Ø£ÙŠ ØªØºÙŠÙŠØ±Ø§Øª ÙØ¹Ù„ÙŠØ©")

        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
        if not dry_run:
            os.makedirs(self.backup_dir, exist_ok=True)

        # 1. Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
        print("\nğŸ—‘ï¸ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©...")
        self.remove_trash_files(dry_run)

        # 2. Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        print("\nğŸ”„ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
        self.merge_duplicates(dry_run)

        # 3. Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù‡ÙŠÙƒÙ„
        print("\nğŸ“‚ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ù‡ÙŠÙƒÙ„...")
        self.reorganize_structure(dry_run)

        # 4. ØªÙ†Ø¸ÙŠÙ Ù†Ù‡Ø§Ø¦ÙŠ
        print("\nâœ¨ Ø§Ù„Ù…Ø±Ø­Ù„Ø© 4: Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        self.final_cleanup(dry_run)

        # 5. Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        print("\nğŸ“‹ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ...")
        self.generate_report()

        print("\nâœ… Ø§ÙƒØªÙ…Ù„Øª Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªÙ†Ø¸ÙŠÙ!")

    def remove_trash_files(self, dry_run: bool = False):
        """Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©"""
        trash_patterns = [
            "*_old.py",
            "*_backup.py",
            "*_temp.py",
            "*_copy.py",
            "*_bak.py",
            "*.pyc",
            "__pycache__/",
            ".pytest_cache/",
            ".mypy_cache/",
            "*.log",
            "*.tmp",
            "*.swp",
            ".DS_Store",
            "Thumbs.db",
        ]

        deleted_count = 0

        # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
        for empty_file in self.analysis.get("empty_files", []):
            if os.path.exists(empty_file):
                if not dry_run:
                    self._backup_and_delete(empty_file)
                self.report["deleted_files"].append(
                    {"file": empty_file, "reason": "Empty file"}
                )
                deleted_count += 1
                print(f"  ğŸ—‘ï¸ Ø­Ø°Ù Ù…Ù„Ù ÙØ§Ø±Øº: {empty_file}")

        # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
        for pattern in trash_patterns:
            for file_path in self.project_root.rglob(pattern):
                if file_path.is_file():
                    if not dry_run:
                        self._backup_and_delete(str(file_path))
                    self.report["deleted_files"].append(
                        {
                            "file": str(file_path),
                            "reason": f"Matches pattern: {pattern}",
                        }
                    )
                    deleted_count += 1
                    print(f"  ğŸ—‘ï¸ Ø­Ø°Ù: {file_path}")

        # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØµÙ†ÙØ© ÙƒÙ€ trash ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„
        for file_info in self.analysis.get("detailed_analysis", []):
            if file_info.get("importance") == "trash":
                file_path = file_info["path"]
                if os.path.exists(file_path):
                    if not dry_run:
                        self._backup_and_delete(file_path)
                    self.report["deleted_files"].append(
                        {
                            "file": file_path,
                            "reason": f"Classified as trash: {', '.join(file_info.get('issues', []))}",
                        }
                    )
                    deleted_count += 1
                    print(f"  ğŸ—‘ï¸ Ø­Ø°Ù Ù…Ù„Ù ØºÙŠØ± Ù…Ù‡Ù…: {file_path}")

        print(f"\nâœ… ØªÙ… Ø­Ø°Ù {deleted_count} Ù…Ù„Ù ØºÙŠØ± Ù…Ù‡Ù…")

    def merge_duplicates(self, dry_run: bool = False):
        """Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        merged_count = 0

        for duplicate_group in self.analysis.get("duplicate_candidates", []):
            if duplicate_group["type"] == "exact":
                # Ø¯Ù…Ø¬ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©
                files = duplicate_group["files"]
                if len(files) > 1:
                    best_file = self._select_best_file(files)

                    for file in files:
                        if file != best_file and os.path.exists(file):
                            if not dry_run:
                                self._update_imports(file, best_file)
                                self._backup_and_delete(file)

                            self.report["merged_files"].append(
                                {
                                    "deleted": file,
                                    "kept": best_file,
                                    "type": "exact_duplicate",
                                }
                            )
                            merged_count += 1
                            print(f"  ğŸ”„ Ø¯Ù…Ø¬: {file} -> {best_file}")

        print(f"\nâœ… ØªÙ… Ø¯Ù…Ø¬ {merged_count} Ù…Ù„Ù Ù…ÙƒØ±Ø±")

    def reorganize_structure(self, dry_run: bool = False):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªÙ†Ø¸ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        moved_count = 0

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¬Ø¯ÙŠØ¯
        new_structure = {
            "src/core/domain/entities/": [],
            "src/core/domain/value_objects/": [],
            "src/core/domain/exceptions/": [],
            "src/core/services/": [],
            "src/core/interfaces/": [],
            "src/infrastructure/persistence/models/": [],
            "src/infrastructure/persistence/repositories/": [],
            "src/infrastructure/ai_providers/": [],
            "src/infrastructure/messaging/": [],
            "src/infrastructure/monitoring/": [],
            "src/api/rest/endpoints/": [],
            "src/api/rest/middleware/": [],
            "src/api/rest/schemas/": [],
            "src/api/websocket/": [],
            "src/esp32/": [],
            "src/shared/utils/": [],
            "src/shared/constants/": [],
            "tests/unit/": [],
            "tests/integration/": [],
            "tests/e2e/": [],
            "tests/fixtures/": [],
            "scripts/": [],
            "docs/": [],
            "configs/": [],
        }

        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª ÙˆØªØ­Ø¯ÙŠØ¯ Ø£Ù…Ø§ÙƒÙ†Ù‡Ø§ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
        for file_info in self.analysis.get("detailed_analysis", []):
            if file_info.get("importance") in ["critical", "high", "medium"]:
                current_path = file_info["path"]
                suggested_location = file_info.get("suggested_location")

                if suggested_location and os.path.exists(current_path):
                    # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù„ÙŠØ³ ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­
                    if not Path(current_path).is_relative_to(
                        Path(suggested_location).parent
                    ):
                        if not dry_run:
                            self._move_file_with_imports(
                                current_path, suggested_location
                            )

                        self.report["moved_files"].append(
                            {
                                "from": current_path,
                                "to": suggested_location,
                                "type": file_info["type"],
                            }
                        )
                        moved_count += 1
                        print(f"  ğŸ“‚ Ù†Ù‚Ù„: {current_path} -> {suggested_location}")

        print(f"\nâœ… ØªÙ… Ù†Ù‚Ù„ {moved_count} Ù…Ù„Ù Ø¥Ù„Ù‰ Ø£Ù…Ø§ÙƒÙ†Ù‡Ø§ Ø§Ù„ØµØ­ÙŠØ­Ø©")

    def final_cleanup(self, dry_run: bool = False):
        """Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ"""
        if not dry_run:
            print("  ğŸ¨ ØªÙ†Ø³ÙŠÙ‚ Ø§Ù„ÙƒÙˆØ¯ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… black...")
            os.system("black src/ tests/ scripts/ --quiet")

            print("  ğŸ“¦ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… isort...")
            os.system("isort src/ tests/ scripts/ --quiet")

            print("  ğŸ§¹ Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©...")
            self._remove_empty_dirs()

        print("\nâœ… Ø§ÙƒØªÙ…Ù„ Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ")

    def _backup_and_delete(self, file_path: str):
        """Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ ÙˆØ­Ø°Ù Ø§Ù„Ù…Ù„Ù"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
            backup_path = os.path.join(
                self.backup_dir, os.path.relpath(file_path, self.project_root)
            )
            os.makedirs(os.path.dirname(backup_path), exist_ok=True)

            # Ù†Ø³Ø® Ø§Ù„Ù…Ù„Ù
            if os.path.exists(file_path):
                shutil.copy2(file_path, backup_path)
                os.remove(file_path)
        except Exception as e:
            self.report["errors"].append(f"Error deleting {file_path}: {e}")

    def _select_best_file(self, files: List[str]) -> str:
        """Ø§Ø®ØªÙŠØ§Ø± Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª"""
        # Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„Ø§Ø®ØªÙŠØ§Ø±
        scores = {}

        for file in files:
            score = 0

            # Ø§Ù„Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­ ÙŠØ­ØµÙ„ Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø· Ø£ÙƒØ«Ø±
            if "src/" in file:
                score += 10
            if "core/" in file:
                score += 5

            # Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙƒØ¨Ø± (Ø£ÙƒØ«Ø± Ø§ÙƒØªÙ…Ø§Ù„Ø§Ù‹)
            try:
                size = os.path.getsize(file)
                score += size // 1000  # Ù†Ù‚Ø·Ø© Ù„ÙƒÙ„ KB
            except:
                pass

            # Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
            test_file = file.replace(".py", "_test.py")
            if os.path.exists(test_file):
                score += 20

            # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø©
            if any(x in file for x in ["old", "backup", "temp", "copy"]):
                score -= 50

            scores[file] = score

        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù„Ù Ù…Ø¹ Ø£Ø¹Ù„Ù‰ Ù†Ù‚Ø§Ø·
        return max(scores.items(), key=lambda x: x[1])[0]

    def _update_imports(self, old_file: str, new_file: str):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª"""
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Python
        old_import = self._path_to_import(old_file)
        new_import = self._path_to_import(new_file)

        if old_import == new_import:
            return

        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Python
        for py_file in self.project_root.rglob("*.py"):
            if py_file.is_file():
                try:
                    with open(py_file, "r", encoding="utf-8") as f:
                        content = f.read()

                    # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª
                    updated_content = content
                    updated_content = re.sub(
                        f"from {old_import} import",
                        f"from {new_import} import",
                        updated_content,
                    )
                    updated_content = re.sub(
                        f"import {old_import}", f"import {new_import}", updated_content
                    )

                    if updated_content != content:
                        with open(py_file, "w", encoding="utf-8") as f:
                            f.write(updated_content)
                except:
                    pass

    def _path_to_import(self, file_path: str) -> str:
        """ØªØ­ÙˆÙŠÙ„ Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ø§Ø³ØªÙŠØ±Ø§Ø¯ Python"""
        # Ø¥Ø²Ø§Ù„Ø© .py
        import_path = file_path.replace(".py", "")

        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª Ø¥Ù„Ù‰ Ù†Ù‚Ø§Ø·
        import_path = import_path.replace(os.sep, ".")
        import_path = import_path.replace("/", ".")

        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù†Ø³Ø¨ÙŠ
        if import_path.startswith("."):
            import_path = import_path[1:]

        return import_path

    def _move_file_with_imports(self, old_path: str, new_path: str):
        """Ù†Ù‚Ù„ Ù…Ù„Ù Ù…Ø¹ ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª"""
        try:
            # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø¬Ø¯ÙŠØ¯
            os.makedirs(os.path.dirname(new_path), exist_ok=True)

            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª Ø£ÙˆÙ„Ø§Ù‹
            self._update_imports(old_path, new_path)

            # Ù†Ù‚Ù„ Ø§Ù„Ù…Ù„Ù
            shutil.move(old_path, new_path)
        except Exception as e:
            self.report["errors"].append(f"Error moving {old_path}: {e}")

    def _remove_empty_dirs(self):
        """Ø­Ø°Ù Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙØ§Ø±ØºØ©"""
        for root, dirs, files in os.walk(self.project_root, topdown=False):
            for dir_name in dirs:
                dir_path = os.path.join(root, dir_name)
                try:
                    if not os.listdir(dir_path):  # Ø§Ù„Ù…Ø¬Ù„Ø¯ ÙØ§Ø±Øº
                        os.rmdir(dir_path)
                except:
                    pass

    def generate_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªÙ†Ø¸ÙŠÙ"""
        report_content = f"""# ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear

## ğŸ“… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ
- **Ø§Ù„ØªØ§Ø±ÙŠØ®**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ**: `{self.backup_dir}/`

## ğŸ“ˆ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª

### ğŸ—‘ï¸ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: {len(self.report['deleted_files'])}
"""

        # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©
        if self.report["deleted_files"]:
            report_content += "\n| Ø§Ù„Ù…Ù„Ù | Ø§Ù„Ø³Ø¨Ø¨ |\n|-------|-------|\n"
            for item in self.report["deleted_files"][:10]:  # Ø£ÙˆÙ„ 10 ÙÙ‚Ø·
                report_content += (
                    f"| `{Path(item['file']).name}` | {item['reason']} |\n"
                )

            if len(self.report["deleted_files"]) > 10:
                report_content += (
                    f"\n... Ùˆ {len(self.report['deleted_files']) - 10} Ù…Ù„Ù Ø¢Ø®Ø±\n"
                )

        report_content += (
            f"\n### ğŸ“‚ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø©: {len(self.report['moved_files'])}\n"
        )

        # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø©
        if self.report["moved_files"]:
            report_content += "\n| Ù…Ù† | Ø¥Ù„Ù‰ | Ø§Ù„Ù†ÙˆØ¹ |\n|-----|------|-------|\n"
            for item in self.report["moved_files"][:10]:  # Ø£ÙˆÙ„ 10 ÙÙ‚Ø·
                from_name = Path(item["from"]).name
                to_dir = Path(item["to"]).parent
                report_content += f"| `{from_name}` | `{to_dir}/` | {item['type']} |\n"

        report_content += (
            f"\n### ğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬Ø©: {len(self.report['merged_files'])}\n"
        )

        # ØªÙØ§ØµÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬Ø©
        if self.report["merged_files"]:
            report_content += "\n| Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­Ø°ÙˆÙ | Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù…Ø­ØªÙØ¸ Ø¨Ù‡ | Ø§Ù„Ù†ÙˆØ¹ |\n|----------------|------------------|-------|\n"
            for item in self.report["merged_files"][:10]:  # Ø£ÙˆÙ„ 10 ÙÙ‚Ø·
                deleted_name = Path(item["deleted"]).name
                kept_name = Path(item["kept"]).name
                report_content += (
                    f"| `{deleted_name}` | `{kept_name}` | {item['type']} |\n"
                )

        # Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª
        if self.report["errors"]:
            report_content += f"\n## âŒ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡: {len(self.report['errors'])}\n"
            for error in self.report["errors"]:
                report_content += f"- {error}\n"

        report_content += """
## âœ… Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©

1. [ ] Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
2. [ ] ØªØ´ØºÙŠÙ„ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙƒØ³Ø± Ø£ÙŠ Ø´ÙŠØ¡
3. [ ] ØªØ­Ø¯ÙŠØ« Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
4. [ ] commit Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª ÙÙŠ Git
5. [ ] Ø­Ø°Ù Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¨Ø¹Ø¯ Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ù†Ø¬Ø§Ø­ Ø§Ù„ØªÙ†Ø¸ÙŠÙ

## ğŸ“ Ù…Ù„Ø§Ø­Ø¸Ø§Øª
- Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ© Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ Ù…Ø¬Ù„Ø¯ Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ
- ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ±Ø¬Ø§Ø¹ Ø£ÙŠ Ù…Ù„Ù Ù…Ù† Ø§Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
- ØªÙ… ØªØ­Ø¯ÙŠØ« Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
"""

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open("cleanup_report.md", "w", encoding="utf-8") as f:
            f.write(report_content)

        # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± JSON Ù…ÙØµÙ„
        with open("cleanup_report.json", "w", encoding="utf-8") as f:
            json.dump(self.report, f, indent=2, ensure_ascii=False)

        print("\nâœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±:")
        print("  â€¢ cleanup_report.md - ØªÙ‚Ø±ÙŠØ± Ù…Ù‚Ø±ÙˆØ¡")
        print("  â€¢ cleanup_report.json - ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„")


def main():
    """ØªØ´ØºÙŠÙ„ Ù…Ù†Ø¸Ù Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
    import argparse

    parser = argparse.ArgumentParser(description="ØªÙ†Ø¸ÙŠÙ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear")
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ØªØ´ØºÙŠÙ„ ÙÙŠ ÙˆØ¶Ø¹ Ø§Ù„ØªØ¬Ø±Ø¨Ø© Ø¯ÙˆÙ† ØªÙ†ÙÙŠØ° Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª",
    )
    parser.add_argument(
        "--analysis-file",
        default="scripts/project_analysis.json",
        help="Ù…Ø³Ø§Ø± Ù…Ù„Ù ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„",
    )

    args = parser.parse_args()

    # Ø§Ù„ØªØ£ÙƒÙŠØ¯ Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡
    if not args.dry_run:
        print("âš ï¸  ØªØ­Ø°ÙŠØ±: Ø³ÙŠÙ‚ÙˆÙ… Ù‡Ø°Ø§ Ø§Ù„Ø³ÙƒØ±ÙŠØ¨Øª Ø¨Ø­Ø°Ù ÙˆÙ†Ù‚Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª!")
        print("ğŸ“ Ø³ÙŠØªÙ… Ø­ÙØ¸ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙÙŠ Ù…Ø¬Ù„Ø¯ backup_[timestamp]")
        response = input("\nÙ‡Ù„ ØªØ±ÙŠØ¯ Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø©ØŸ (yes/no): ")
        if response.lower() != "yes":
            print("âŒ ØªÙ… Ø¥Ù„ØºØ§Ø¡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©")
            return

    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù†Ø¸Ù
    cleaner = ProjectCleaner(analysis_file=args.analysis_file)
    cleaner.run_full_cleanup(dry_run=args.dry_run)


if __name__ == "__main__":
    main()
