#!/usr/bin/env python3
"""
ğŸ” Duplicate and Unnecessary Files Analyzer
Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø© ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

Lead Architect: Ø¬Ø¹ÙØ± Ø£Ø¯ÙŠØ¨ (Jaafar Adeeb)
Enterprise Grade AI Teddy Bear Project 2025
"""

import hashlib
import json
import logging
import os
from collections import defaultdict
from pathlib import Path
from typing import Dict, List, Set, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class DuplicateAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.duplicates = defaultdict(list)
        self.unnecessary_files = []
        self.project_files = []
        self.venv_files = []
        self.test_files = []
        self.script_files = []

    def analyze_project_structure(self):
        """ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        logger.info("ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")

        for file_path in self.project_root.rglob("*"):
            if file_path.is_file():
                self._categorize_file(file_path)

        self._find_duplicates()
        self._identify_unnecessary_files()
        self._generate_report()

    def _categorize_file(self, file_path: Path):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª"""
        relative_path = file_path.relative_to(self.project_root)

        # ØªØ¬Ø§Ù‡Ù„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
        if any(part.startswith('.') for part in relative_path.parts):
            return

        # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª
        if "venv" in str(file_path) or ".venv" in str(file_path):
            self.venv_files.append(str(relative_path))
        elif "test" in file_path.name or "tests" in str(file_path):
            self.test_files.append(str(relative_path))
        elif "scripts" in str(file_path):
            self.script_files.append(str(relative_path))
        elif file_path.suffix in ['.py', '.json', '.yaml', '.yml', '.md', '.txt']:
            self.project_files.append(str(relative_path))

    def _calculate_file_hash(self, file_path: Path) -> str:
        """Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù"""
        try:
            with open(file_path, 'rb') as f:
                return hashlib.sha256(f.read()).hexdigest()
        except Exception:
            return ""

    def _find_duplicates(self):
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        logger.info("ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")

        hash_to_files = defaultdict(list)

        for file_path_str in self.project_files:
            file_path = self.project_root / file_path_str
            if file_path.exists():
                file_hash = self._calculate_file_hash(file_path)
                if file_hash:
                    hash_to_files[file_hash].append(file_path_str)

        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for file_hash, files in hash_to_files.items():
            if len(files) > 1:
                self.duplicates[file_hash] = files

    def _identify_unnecessary_files(self):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©"""
        logger.info("ğŸ” ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©...")

        unnecessary_patterns = [
            # Ù…Ù„ÙØ§Øª Ù…Ø¤Ù‚ØªØ©
            "*.tmp", "*.temp", "*.bak", "*.backup",
            # Ù…Ù„ÙØ§Øª Ù†Ø¸Ø§Ù…
            ".DS_Store", "Thumbs.db", "desktop.ini",
            # Ù…Ù„ÙØ§Øª IDE
            "*.swp", "*.swo", "*~", ".vscode/", ".idea/",
            # Ù…Ù„ÙØ§Øª Python
            "*.pyc", "__pycache__/", "*.pyo",
            # Ù…Ù„ÙØ§Øª logs
            "*.log", "logs/",
            # Ù…Ù„ÙØ§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¤Ù‚ØªØ©
            "*.cache", ".cache/",
            # Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø¯ÙŠÙ…Ø©
            "test_*.py.bak", "*_test_old.py",
            # Ù…Ù„ÙØ§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ©
            "demo_*.py", "example_*.py", "sample_*.py",
            # Ù…Ù„ÙØ§Øª ØªØ­Ù„ÙŠÙ„ Ù‚Ø¯ÙŠÙ…Ø©
            "*_analyzer_old.py", "*_audit_old.py",
            # Ù…Ù„ÙØ§Øª PDF
            "*.pdf",
            # Ù…Ù„ÙØ§Øª ZIP
            "*.zip", "*.tar.gz",
            # Ù…Ù„ÙØ§Øª ØµÙˆØ±
            "*.png", "*.jpg", "*.jpeg", "*.gif"
        ]

        for file_path_str in self.project_files:
            file_path = Path(file_path_str)

            # ÙØ­Øµ Ø§Ù„Ø£Ù†Ù…Ø§Ø·
            for pattern in unnecessary_patterns:
                if file_path.match(pattern):
                    self.unnecessary_files.append(file_path_str)
                    break

            # ÙØ­Øµ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø¨ÙˆÙ‡Ø©
            suspicious_names = [
                "temp", "tmp", "old", "backup", "copy", "duplicate",
                "test_old", "demo", "example", "sample", "trial"
            ]

            if any(name in file_path.name.lower() for name in suspicious_names):
                self.unnecessary_files.append(file_path_str)

    def _generate_report(self):
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        report = {
            "timestamp": "2025-07-03T23:58:00",
            "summary": {
                "total_project_files": len(self.project_files),
                "total_venv_files": len(self.venv_files),
                "total_test_files": len(self.test_files),
                "total_script_files": len(self.script_files),
                "duplicate_groups": len(self.duplicates),
                "unnecessary_files": len(self.unnecessary_files)
            },
            "duplicates": {
                hash_val: {
                    "files": files,
                    "count": len(files),
                    "size_mb": self._get_total_size_mb(files)
                }
                for hash_val, files in self.duplicates.items()
            },
            "unnecessary_files": [
                {
                    "file": file_path,
                    "reason": self._get_unnecessary_reason(file_path)
                }
                for file_path in self.unnecessary_files
            ],
            "recommendations": self._generate_recommendations()
        }

        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        with open("duplicate_analysis_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, indent=2, ensure_ascii=False)

        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ù„Ø®Øµ
        self._display_summary(report)

    def _get_total_size_mb(self, files: List[str]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø§Ù„Ø­Ø¬Ù… Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¨Ø§Ù„Ù€ MB"""
        total_size = 0
        for file_path_str in files:
            file_path = self.project_root / file_path_str
            if file_path.exists():
                total_size += file_path.stat().st_size
        return round(total_size / (1024 * 1024), 2)

    def _get_unnecessary_reason(self, file_path: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø³Ø¨Ø¨ Ø¹Ø¯Ù… Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù"""
        path = Path(file_path)

        if "temp" in path.name.lower() or "tmp" in path.name.lower():
            return "Ù…Ù„Ù Ù…Ø¤Ù‚Øª"
        elif "old" in path.name.lower() or "backup" in path.name.lower():
            return "Ù…Ù„Ù Ù‚Ø¯ÙŠÙ… Ø£Ùˆ Ù†Ø³Ø®Ø© Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©"
        elif "demo" in path.name.lower() or "example" in path.name.lower():
            return "Ù…Ù„Ù ØªØ¬Ø±ÙŠØ¨ÙŠ Ø£Ùˆ Ù…Ø«Ø§Ù„"
        elif "test_old" in path.name.lower():
            return "Ø§Ø®ØªØ¨Ø§Ø± Ù‚Ø¯ÙŠÙ…"
        elif path.suffix in ['.pdf', '.zip', '.png', '.jpg']:
            return "Ù…Ù„Ù ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹"
        else:
            return "Ù…Ù„Ù Ù…Ø´Ø¨ÙˆÙ‡"

    def _generate_recommendations(self) -> List[str]:
        """ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª"""
        recommendations = []

        if self.duplicates:
            recommendations.append("ğŸ—‘ï¸ Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ù…Ø³Ø§Ø­Ø©")

        if self.unnecessary_files:
            recommendations.append("ğŸ§¹ ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©")

        if len(self.venv_files) > 100:
            recommendations.append("ğŸ“ Ø¥Ø¶Ø§ÙØ© venv/ Ø¥Ù„Ù‰ .gitignore")

        if len(self.test_files) > 200:
            recommendations.append("ğŸ§ª ØªÙ†Ø¸ÙŠÙ… Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±")

        recommendations.extend([
            "ğŸ“‹ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªØ¬Ø±ÙŠØ¨ÙŠØ©",
            "ğŸ—‚ï¸ ØªÙ†Ø¸ÙŠÙ… Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª",
            "ğŸ“ ØªØ­Ø¯ÙŠØ« .gitignore"
        ])

        return recommendations

    def _display_summary(self, report: Dict):
        """Ø¹Ø±Ø¶ Ù…Ù„Ø®Øµ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        print("\n" + "="*80)
        print("ğŸ” ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© ÙˆØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©")
        print("="*80)
        print(
            f"ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø´Ø±ÙˆØ¹: {report['summary']['total_project_files']}")
        print(f"ğŸ Ù…Ù„ÙØ§Øª venv: {report['summary']['total_venv_files']}")
        print(f"ğŸ§ª Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø±: {report['summary']['total_test_files']}")
        print(f"ğŸ“œ Ù…Ù„ÙØ§Øª Ø³ÙƒØ±ÙŠØ¨Øª: {report['summary']['total_script_files']}")
        print(f"ğŸ”„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…ÙƒØ±Ø±Ø©: {report['summary']['duplicate_groups']}")
        print(f"ğŸ—‘ï¸ Ù…Ù„ÙØ§Øª ØºÙŠØ± Ù…Ù‡Ù…Ø©: {report['summary']['unnecessary_files']}")
        print("="*80)

        if report['duplicates']:
            print("\nğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©:")
            for hash_val, info in report['duplicates'].items():
                print(
                    f"  ğŸ“¦ {info['count']} Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© ({info['size_mb']} MB):")
                for file_path in info['files']:
                    print(f"    â€¢ {file_path}")

        if report['unnecessary_files']:
            print("\nğŸ—‘ï¸ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø© (Ø£ÙˆÙ„ 20):")
            for item in report['unnecessary_files'][:20]:
                print(f"  â€¢ {item['file']} - {item['reason']}")

        print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
        for rec in report['recommendations']:
            print(f"  {rec}")


def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = DuplicateAnalyzer()
    analyzer.analyze_project_structure()


if __name__ == "__main__":
    main()
