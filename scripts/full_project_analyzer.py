import ast
import hashlib
import json
import os
from collections import defaultdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple


class FullProjectAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø´Ø§Ù…Ù„ Ù„ÙƒØ§Ù…Ù„ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear"""

    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.analysis_result = {
            "total_files": 0,
            "total_python_files": 0,
            "file_types": defaultdict(int),
            "duplicate_candidates": [],
            "large_files": [],
            "empty_files": [],
            "test_files": [],
            "config_files": [],
            "detailed_analysis": [],
            "file_hashes": defaultdict(list),
            "function_signatures": defaultdict(list),
            "import_dependencies": defaultdict(set),
            "directory_stats": defaultdict(lambda: {"files": 0, "size": 0}),
            "problematic_files": [],
        }

    def analyze_project(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„ÙƒÙ„ Ù…Ù„Ù ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ” Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„...")

        # Ù…Ø³Ø­ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
        for root, dirs, files in os.walk(self.project_root):
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
            dirs[:] = [
                d
                for d in dirs
                if d
                not in [
                    ".git",
                    "__pycache__",
                    "node_modules",
                    ".venv",
                    "venv",
                    ".pytest_cache",
                    ".mypy_cache",
                    "dist",
                    "build",
                    ".idea",
                    "htmlcov",
                    "coverage",
                    ".coverage",
                    "*.egg-info",
                ]
            ]

            # ØªØ­Ù„ÙŠÙ„ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ø¬Ù„Ø¯
            rel_root = os.path.relpath(root, self.project_root)

            for file in files:
                # ØªØ¬Ø§Ù‡Ù„ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©
                if file.endswith(
                    (
                        ".pyc",
                        ".pyo",
                        ".pyd",
                        ".so",
                        ".dll",
                        ".dylib",
                        ".db",
                        ".sqlite",
                        ".log",
                        ".tmp",
                        ".swp",
                        ".swo",
                        ".DS_Store",
                        "Thumbs.db",
                        ".gitignore",
                    )
                ):
                    continue

                file_path = os.path.join(root, file)

                # Ø­Ø³Ø§Ø¨ Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
                try:
                    file_size = os.path.getsize(file_path)
                    self.analysis_result["directory_stats"][rel_root]["files"] += 1
                    self.analysis_result["directory_stats"][rel_root][
                        "size"
                    ] += file_size
                except:
                    continue

                if file.endswith(".py"):
                    self.analyze_python_file(file_path)
                elif file.endswith((".js", ".jsx", ".ts", ".tsx")):
                    self.analysis_result["file_types"]["javascript"] += 1
                elif file.endswith((".json", ".yaml", ".yml")):
                    self.analysis_result["file_types"]["config"] += 1
                elif file.endswith((".md", ".rst", ".txt")):
                    self.analysis_result["file_types"]["docs"] += 1
                elif file.endswith((".html", ".css")):
                    self.analysis_result["file_types"]["web"] += 1

                self.analysis_result["total_files"] += 1

        # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„Ù…Ø´Ø§ÙƒÙ„
        self._find_duplicates()
        self._analyze_project_structure()
        self._generate_summary()

        return self.analysis_result

    def analyze_python_file(self, file_path: str) -> None:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Python ÙˆØ§Ø­Ø¯"""
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                content = f.read()
                lines = content.splitlines()

            self.analysis_result["total_python_files"] += 1

            # Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ù„Ù
            file_hash = hashlib.md5(content.encode()).hexdigest()
            self.analysis_result["file_hashes"][file_hash].append(file_path)

            # ØªØ­Ù„ÙŠÙ„ AST
            analysis_data = self._analyze_ast(content, file_path)

            # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
            file_type = self._determine_file_type(file_path, content)
            self.analysis_result["file_types"][file_type] += 1

            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©
            importance = self._determine_importance(
                file_path, content, analysis_data["classes"], analysis_data["functions"]
            )

            # Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„
            issues = self._find_issues(content, file_path)

            # Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø£ÙØ¶Ù„
            suggested_location = self._suggest_location(file_path, file_type)

            # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            complexity = self._calculate_complexity(analysis_data)

            # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„Ù
            file_report = {
                "path": file_path,
                "relative_path": str(Path(file_path).relative_to(self.project_root)),
                "type": file_type,
                "importance": importance,
                "stats": {
                    "lines": len(lines),
                    "loc": len(
                        [
                            l
                            for l in lines
                            if l.strip() and not l.strip().startswith("#")
                        ]
                    ),
                    "classes": len(analysis_data["classes"]),
                    "functions": len(analysis_data["functions"]),
                    "imports": len(analysis_data["imports"]),
                },
                "hash": file_hash,
                "issues": issues,
                "suggested_location": suggested_location,
                "complexity": complexity,
                "dependencies": list(analysis_data["dependencies"]),
            }

            self.analysis_result["detailed_analysis"].append(file_report)

            # ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø®Ø§ØµØ©
            if len(lines) == 0 or len(content.strip()) == 0:
                self.analysis_result["empty_files"].append(file_path)
            elif len(lines) > 500:
                self.analysis_result["large_files"].append((file_path, len(lines)))

            if file_type == "test":
                self.analysis_result["test_files"].append(file_path)
            elif file_type == "config":
                self.analysis_result["config_files"].append(file_path)

            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø°Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ÙƒØ«ÙŠØ±Ø©
            if len(issues) > 3 or importance == "trash":
                self.analysis_result["problematic_files"].append(
                    {"path": file_path, "issues": issues, "importance": importance}
                )

        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")

    def _analyze_ast(self, content: str, file_path: str) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ AST Ù„Ù„Ù…Ù„Ù"""
        result = {"imports": [], "classes": [], "functions": [], "dependencies": set()}

        try:
            tree = ast.parse(content)

            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        result["imports"].append(alias.name)
                        result["dependencies"].add(alias.name.split(".")[0])

                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        result["imports"].append(node.module)
                        result["dependencies"].add(node.module.split(".")[0])

                elif isinstance(node, ast.ClassDef):
                    result["classes"].append(
                        {
                            "name": node.name,
                            "line": node.lineno,
                            "methods": len(
                                [n for n in node.body if isinstance(n, ast.FunctionDef)]
                            ),
                        }
                    )

                elif isinstance(node, ast.FunctionDef):
                    # ØªØ³Ø¬ÙŠÙ„ ØªÙˆÙ‚ÙŠØ¹ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù„ÙƒØ´Ù Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
                    args = [arg.arg for arg in node.args.args]
                    signature = f"{node.name}({','.join(args)})"
                    self.analysis_result["function_signatures"][signature].append(
                        file_path
                    )

                    result["functions"].append(
                        {
                            "name": node.name,
                            "line": node.lineno,
                            "args": len(node.args.args),
                            "decorators": len(node.decorator_list),
                        }
                    )

        except:
            pass

        return result

    def _determine_file_type(self, file_path: str, content: str) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ÙˆØ§Ù„Ù…Ø³Ø§Ø±"""
        path_lower = file_path.lower()

        # Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        if "test_" in os.path.basename(file_path) or "_test.py" in file_path:
            return "test"
        elif "/tests/" in file_path or "/test/" in file_path:
            return "test"

        # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª
        elif "config" in path_lower or "settings" in path_lower:
            return "config"

        # Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        elif any(x in path_lower for x in ["model", "entity", "schema"]):
            return "model"

        # Ø§Ù„Ø®Ø¯Ù…Ø§Øª
        elif "service" in path_lower or "manager" in path_lower:
            return "service"

        # Ø§Ù„Ù…Ø³ØªÙˆØ¯Ø¹Ø§Øª
        elif "repository" in path_lower or "repo" in path_lower:
            return "repository"

        # ÙˆØ§Ø¬Ù‡Ø§Øª API
        elif any(x in path_lower for x in ["controller", "endpoint", "route", "view"]):
            return "controller"

        # Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø©
        elif any(x in path_lower for x in ["util", "helper", "common"]):
            return "utility"

        # Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙ‡ÙŠØ¦Ø©
        elif "__init__.py" in file_path:
            return "init"

        # Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©
        elif "infrastructure" in path_lower:
            return "infrastructure"

        # Ø§Ù„Ù…Ø¬Ø§Ù„
        elif "domain" in path_lower:
            return "domain"

        # Ø§Ù„ØªØ·Ø¨ÙŠÙ‚
        elif "application" in path_lower:
            return "application"

        # Ø³ÙƒØ±ÙŠØ¨ØªØ§Øª
        elif "script" in path_lower:
            return "script"

        else:
            return "other"

    def _determine_importance(
        self, file_path: str, content: str, classes: List, functions: List
    ) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ø£Ù‡Ù…ÙŠØ© Ø§Ù„Ù…Ù„Ù"""
        path_lower = file_path.lower()

        # Critical files - Ù…Ù„ÙØ§Øª Ø­Ø±Ø¬Ø©
        critical_patterns = [
            "main.py",
            "app.py",
            "wsgi.py",
            "__main__.py",
            "manage.py",
            "server.py",
            "run.py",
        ]
        if any(pattern in os.path.basename(file_path) for pattern in critical_patterns):
            return "critical"

        # Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ù…ØµØ§Ø¯Ù‚Ø©
        if any(
            x in path_lower for x in ["security", "auth", "child_safety", "encryption"]
        ):
            return "critical"

        # Ù†Ù…Ø§Ø°Ø¬ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if "models" in path_lower and any(
            x in path_lower for x in ["child", "parent", "device"]
        ):
            return "critical"

        # Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© Ø§Ù„Ø­Ø±Ø¬Ø©
        if any(x in path_lower for x in ["database", "cache", "queue"]):
            return "critical"

        # High importance - Ø£Ù‡Ù…ÙŠØ© Ø¹Ø§Ù„ÙŠØ©
        if len(classes) > 2 or len(functions) > 5:
            return "high"
        elif any(x in path_lower for x in ["service", "repository", "controller"]):
            return "high"
        elif "api" in path_lower and "endpoint" in path_lower:
            return "high"

        # Trash - Ù‚Ù…Ø§Ù…Ø©
        if len(content.strip()) == 0:
            return "trash"
        elif any(
            x in os.path.basename(file_path)
            for x in ["_old", "_backup", "_temp", "_copy", "_bak"]
        ):
            return "trash"
        elif "deprecated" in path_lower or "obsolete" in path_lower:
            return "trash"

        # Low importance - Ø£Ù‡Ù…ÙŠØ© Ù…Ù†Ø®ÙØ¶Ø©
        if len(classes) == 0 and len(functions) == 0:
            return "low"
        elif any(x in path_lower for x in ["example", "sample", "demo", "test_data"]):
            return "low"

        return "medium"

    def _find_issues(self, content: str, file_path: str) -> List[str]:
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„Ù…Ù„Ù"""
        issues = []

        # ÙØ­Øµ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ù†ÙŠØ©
        import re

        security_patterns = [
            (r"eval\s*\(", "Uses eval() - security risk"),
            (r"exec\s*\(", "Uses exec() - security risk"),
            (r"pickle\.loads", "Uses pickle.loads - security risk"),
            (r"os\.system", "Uses os.system - use subprocess instead"),
            (
                r'hardcoded.*password|password\s*=\s*["\']',
                "Possible hardcoded password",
            ),
            (r'api[_\s]?key\s*=\s*["\'][^"\']+["\']', "Possible hardcoded API key"),
            (r'secret\s*=\s*["\'][^"\']+["\']', "Possible hardcoded secret"),
        ]

        for pattern, issue in security_patterns:
            if re.search(pattern, content, re.IGNORECASE):
                issues.append(issue)

        # ÙØ­Øµ Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯
        if "except:" in content or "except Exception:" in content:
            issues.append("Generic exception handling")

        if (
            re.search(r"print\s*\(", content)
            and "test" not in file_path
            and "script" not in file_path
        ):
            issues.append("Print statements in production code")

        if "TODO" in content or "FIXME" in content or "XXX" in content:
            issues.append("Contains TODO/FIXME/XXX")

        # ÙØ­Øµ Ø§Ù„Ø­Ø¬Ù… ÙˆØ§Ù„ØªØ¹Ù‚ÙŠØ¯
        lines = content.splitlines()
        if len(lines) > 500:
            issues.append(f"File too large ({len(lines)} lines)")
        elif len(lines) > 300:
            issues.append(f"File is getting large ({len(lines)} lines)")

        if len(content.strip()) == 0:
            issues.append("Empty file")

        # ÙØ­Øµ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª
        if "from . import *" in content or "import *" in content:
            issues.append("Uses wildcard imports")

        # ÙØ­Øµ Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¹Ø§Ù…Ø©
        if re.search(r"^[A-Z_]+\s*=\s*(?!.*\()", content, re.MULTILINE):
            if "constant" not in file_path and "config" not in file_path:
                issues.append("Contains global variables")

        return issues

    def _suggest_location(self, current_path: str, file_type: str) -> Optional[str]:
        """Ø§Ù‚ØªØ±Ø§Ø­ Ù…ÙˆÙ‚Ø¹ Ø£ÙØ¶Ù„ Ù„Ù„Ù…Ù„Ù"""
        # Ø®Ø±ÙŠØ·Ø© Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ù‚ØªØ±Ø­Ø©
        type_to_location = {
            "model": "src/core/domain/entities/",
            "service": "src/core/services/",
            "repository": "src/infrastructure/persistence/repositories/",
            "controller": "src/api/endpoints/",
            "test": "tests/",
            "config": "configs/",
            "utility": "src/shared/utils/",
            "domain": "src/core/domain/",
            "infrastructure": "src/infrastructure/",
            "application": "src/application/",
            "script": "scripts/",
        }

        suggested = type_to_location.get(file_type)
        if not suggested:
            return None

        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù Ù„ÙŠØ³ ÙÙŠ Ø§Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­
        if suggested in current_path:
            return None

        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ù…Ù‚ØªØ±Ø­
        filename = os.path.basename(current_path)
        return os.path.join(suggested, filename)

    def _calculate_complexity(self, analysis_data: Dict) -> int:
        """Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù„Ù„Ù…Ù„Ù"""
        complexity = 0

        # ØªØ¹Ù‚ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„
        complexity += len(analysis_data["classes"]) * 2
        complexity += len(analysis_data["functions"])

        # ØªØ¹Ù‚ÙŠØ¯ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªÙŠØ±Ø§Ø¯Ø§Øª
        complexity += len(analysis_data["imports"]) // 5

        return complexity

    def _find_duplicates(self) -> None:
        """Ø¥ÙŠØ¬Ø§Ø¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        # Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø© (Ù†ÙØ³ Ø§Ù„Ù€ hash)
        for file_hash, files in self.analysis_result["file_hashes"].items():
            if len(files) > 1:
                self.analysis_result["duplicate_candidates"].append(
                    {"type": "exact", "hash": file_hash, "files": files}
                )

        # Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„ÙˆØ¸ÙŠÙÙŠØ© (Ù†ÙØ³ ØªÙˆÙ‚ÙŠØ¹Ø§Øª Ø§Ù„Ø¯ÙˆØ§Ù„)
        for signature, files in self.analysis_result["function_signatures"].items():
            if len(files) > 1:
                # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¯ÙˆØ§Ù„ Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© Ù…Ø«Ù„ __init__
                if signature not in [
                    "__init__(self)",
                    "__str__(self)",
                    "__repr__(self)",
                ]:
                    self.analysis_result["duplicate_candidates"].append(
                        {"type": "functional", "signature": signature, "files": files}
                    )

    def _analyze_project_structure(self) -> None:
        """ØªØ­Ù„ÙŠÙ„ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        # Ø­Ø³Ø§Ø¨ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª
        directory_distribution = defaultdict(int)

        for file_info in self.analysis_result["detailed_analysis"]:
            directory = os.path.dirname(file_info["relative_path"])
            directory_distribution[directory] += 1

        self.analysis_result["directory_distribution"] = dict(directory_distribution)

        # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙÙˆØ¶ÙˆÙŠØ© (ÙƒØ«ÙŠØ± Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ù…Ø¬Ù„Ø¯ ÙˆØ§Ø­Ø¯)
        messy_directories = []
        for directory, count in directory_distribution.items():
            if count > 20:  # Ø£ÙƒØ«Ø± Ù…Ù† 20 Ù…Ù„Ù ÙÙŠ Ù…Ø¬Ù„Ø¯ ÙˆØ§Ø­Ø¯
                messy_directories.append({"directory": directory, "file_count": count})

        self.analysis_result["messy_directories"] = messy_directories

    def _generate_summary(self) -> None:
        """Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        total_python = self.analysis_result["total_python_files"]

        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        importance_stats = defaultdict(int)
        type_stats = defaultdict(int)
        total_issues = 0

        for file_report in self.analysis_result["detailed_analysis"]:
            importance_stats[file_report["importance"]] += 1
            type_stats[file_report["type"]] += 1
            total_issues += len(file_report["issues"])

        self.analysis_result["summary"] = {
            "total_files": self.analysis_result["total_files"],
            "python_files": total_python,
            "importance_distribution": dict(importance_stats),
            "type_distribution": dict(type_stats),
            "total_issues": total_issues,
            "duplicate_groups": len(self.analysis_result["duplicate_candidates"]),
            "empty_files": len(self.analysis_result["empty_files"]),
            "large_files": len(self.analysis_result["large_files"]),
            "problematic_files": len(self.analysis_result["problematic_files"]),
            "messy_directories": len(self.analysis_result["messy_directories"]),
        }

    def save_report(self, output_file: str = "full_project_analysis.json") -> None:
        """Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(self.analysis_result, f, indent=2, ensure_ascii=False)
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: {output_file}")

    def print_summary(self) -> None:
        """Ø·Ø¨Ø§Ø¹Ø© Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„"""
        summary = self.analysis_result.get("summary", {})

        print("\n" + "=" * 60)
        print("ğŸ“Š Ù…Ù„Ø®Øµ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ÙƒØ§Ù…Ù„")
        print("=" * 60)

        print(f"\nğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {summary.get('total_files', 0)}")
        print(f"ğŸ Ù…Ù„ÙØ§Øª Python: {summary.get('python_files', 0)}")

        print("\nğŸ“ˆ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù‡Ù…ÙŠØ©:")
        for importance, count in summary.get("importance_distribution", {}).items():
            emoji = {
                "critical": "ğŸ”´",
                "high": "ğŸŸ ",
                "medium": "ğŸŸ¡",
                "low": "ğŸŸ¢",
                "trash": "âš«",
            }.get(importance, "âšª")
            print(f"  {emoji} {importance}: {count} Ù…Ù„Ù")

        print("\nğŸ“‚ ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹:")
        for file_type, count in summary.get("type_distribution", {}).items():
            print(f"  â€¢ {file_type}: {count} Ù…Ù„Ù")

        print(f"\nâš ï¸  Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {summary.get('total_issues', 0)}")
        print(f"ğŸ”„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {summary.get('duplicate_groups', 0)}")
        print(f"ğŸ“„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©: {summary.get('empty_files', 0)}")
        print(f"ğŸ“¦ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©: {summary.get('large_files', 0)}")
        print(f"ğŸš¨ Ø§Ù„Ù…Ù„ÙØ§Øª Ø°Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„: {summary.get('problematic_files', 0)}")
        print(f"ğŸ—‚ï¸ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙÙˆØ¶ÙˆÙŠØ©: {summary.get('messy_directories', 0)}")

        # Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ÙÙˆØ¶ÙˆÙŠØ©
        if self.analysis_result.get("messy_directories"):
            print("\nğŸ—‚ï¸ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„ØªÙŠ ØªØ­ØªØ§Ø¬ Ù„ØªÙ†Ø¸ÙŠÙ…:")
            for dir_info in self.analysis_result["messy_directories"][:5]:
                print(f"  â€¢ {dir_info['directory']}: {dir_info['file_count']} Ù…Ù„Ù")


def main():
    """ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ø´Ø§Ù…Ù„"""
    analyzer = FullProjectAnalyzer()

    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear Ø¨Ø§Ù„ÙƒØ§Ù…Ù„...")
    print("â³ Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ù‡Ø°Ø§ Ø¨Ø¹Ø¶ Ø§Ù„ÙˆÙ‚Øª...")

    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹
    result = analyzer.analyze_project()

    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ù…Ù„Ø®Øµ
    analyzer.print_summary()

    # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„
    analyzer.save_report("full_project_analysis.json")

    # Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ù…Ø¨Ø³Ø· Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©
    with open("full_analysis_summary.md", "w", encoding="utf-8") as f:
        f.write("# ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø±ÙˆØ¹ AI Teddy Bear Ø§Ù„ÙƒØ§Ù…Ù„\n\n")
        f.write(f"ğŸ“… Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

        summary = result.get("summary", {})
        f.write("## ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¹Ø§Ù…Ø©\n\n")
        f.write(f"- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {summary.get('total_files', 0)}\n")
        f.write(f"- Ù…Ù„ÙØ§Øª Python: {summary.get('python_files', 0)}\n")
        f.write(f"- Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©: {summary.get('total_issues', 0)}\n")
        f.write(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {summary.get('duplicate_groups', 0)} Ù…Ø¬Ù…ÙˆØ¹Ø©\n")
        f.write(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø°Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„: {summary.get('problematic_files', 0)}\n\n")

        f.write("## âš« Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø­Ø°ÙÙ‡Ø§ ÙÙˆØ±Ø§Ù‹\n\n")
        trash_files = [
            f for f in result["detailed_analysis"] if f["importance"] == "trash"
        ]
        for file in trash_files[:30]:  # Ø£ÙˆÙ„ 30 Ù…Ù„Ù ÙÙ‚Ø·
            f.write(f"- `{file['relative_path']}`: {', '.join(file['issues'])}\n")

        if len(trash_files) > 30:
            f.write(f"\n... Ùˆ {len(trash_files) - 30} Ù…Ù„Ù Ø¢Ø®Ø±\n")

        f.write("\n## ğŸ”´ Ø§Ù„Ù…Ù„ÙØ§Øª Ø°Ø§Øª Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„ÙƒØ«ÙŠØ±Ø©\n\n")
        for file_info in result.get("problematic_files", [])[:20]:
            f.write(f"- `{file_info['path']}`:\n")
            for issue in file_info["issues"]:
                f.write(f"  - {issue}\n")

    print("\nâœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ±:")
    print("  â€¢ full_project_analysis.json - Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…ÙØµÙ„")
    print("  â€¢ full_analysis_summary.md - Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„Ø³Ø±ÙŠØ¹Ø©")


if __name__ == "__main__":
    main()
