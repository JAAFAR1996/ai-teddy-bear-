"""
Ø£Ø¯Ø§Ø© Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©
ÙŠØ¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹ ÙÙŠ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
"""
import hashlib
import os
from collections import defaultdict
from pathlib import Path
import json
from datetime import datetime


class ExactDuplicateFinder:
    def __init__(self, root_dir="."):
        self.root_dir = Path(root_dir)
        self.ignore_dirs = {'.git', '__pycache__', 'node_modules', '.venv', 'venv', '.pytest_cache'}
        self.ignore_patterns = ['*.pyc', '*.log', '*.db', '*.sqlite']
        self.file_hashes = defaultdict(list)
        self.results = {
            'exact_duplicates': [],
            'total_files_scanned': 0,
            'duplicate_groups': 0,
            'space_wasted': 0
        }
    
    def calculate_file_hash(self, filepath):
        """Ø­Ø³Ø§Ø¨ hash SHA256 Ù„Ù„Ù…Ù„Ù"""
        sha256_hash = hashlib.sha256()
        try:
            with open(filepath, "rb") as f:
                for byte_block in iter(lambda: f.read(4096), b""):
                    sha256_hash.update(byte_block)
            return sha256_hash.hexdigest()
        except Exception as e:
            print(f"Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {filepath}: {e}")
            return None
    
    def should_ignore_file(self, filepath):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø£Ù† Ø§Ù„Ù…Ù„Ù ÙŠØ¬Ø¨ ØªØ¬Ø§Ù‡Ù„Ù‡"""
        path = Path(filepath)
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ØµØºÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (Ø£Ù‚Ù„ Ù…Ù† 10 Ø¨Ø§ÙŠØª)
        try:
            if path.stat().st_size < 10:
                return True
        except:
            return True
            
        # ØªØ¬Ø§Ù‡Ù„ Ø£Ù†Ù…Ø§Ø· Ù…Ø¹ÙŠÙ†Ø©
        for pattern in self.ignore_patterns:
            if path.match(pattern):
                return True
        
        return False
    
    def scan_directory(self):
        """Ù…Ø³Ø­ Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø¨Ø­Ø«Ø§Ù‹ Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        print("ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©...")
        
        for root, dirs, files in os.walk(self.root_dir):
            # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
            dirs[:] = [d for d in dirs if d not in self.ignore_dirs]
            
            for file in files:
                filepath = Path(root) / file
                
                if self.should_ignore_file(filepath):
                    continue
                
                self.results['total_files_scanned'] += 1
                
                # Ø­Ø³Ø§Ø¨ hash Ø§Ù„Ù…Ù„Ù
                file_hash = self.calculate_file_hash(filepath)
                if file_hash:
                    file_size = filepath.stat().st_size
                    self.file_hashes[file_hash].append({
                        'path': str(filepath),
                        'size': file_size
                    })
    
    def find_duplicates(self):
        """ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©"""
        print("ğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬...")
        
        for file_hash, files in self.file_hashes.items():
            if len(files) > 1:
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©
                file_size = files[0]['size']
                wasted_space = file_size * (len(files) - 1)
                self.results['space_wasted'] += wasted_space
                
                # Ø¥Ø¶Ø§ÙØ© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø©
                duplicate_group = {
                    'hash': file_hash,
                    'files': [f['path'] for f in files],
                    'file_size': file_size,
                    'wasted_space': wasted_space,
                    'count': len(files)
                }
                
                # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ù…Ù„Ù Ù„Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡
                duplicate_group['recommended_keep'] = self.recommend_file_to_keep(files)
                
                self.results['exact_duplicates'].append(duplicate_group)
                self.results['duplicate_groups'] += 1
    
    def recommend_file_to_keep(self, files):
        """ØªÙˆØµÙŠØ© Ø¨Ø£ÙŠ Ù…Ù„Ù ÙŠØ¬Ø¨ Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù‡"""
        # Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ© Ù„Ù„Ù…Ù„ÙØ§Øª ÙÙŠ src/
        for f in files:
            if 'src\\' in f['path'] or 'src/' in f['path']:
                return f['path']
        
        # Ø«Ù… Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ tests/
        for f in files:
            if 'tests\\' in f['path'] or 'tests/' in f['path']:
                return f['path']
        
        # ØªØ¬Ù†Ø¨ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ backup Ø£Ùˆ temp
        for f in files:
            path_lower = f['path'].lower()
            if 'backup' not in path_lower and 'temp' not in path_lower and 'old' not in path_lower:
                return f['path']
        
        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ØŒ Ù†Ø®ØªØ§Ø± Ø§Ù„Ø£ÙˆÙ„
        return files[0]['path']
    
    def generate_report(self):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„"""
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©
        self.results['exact_duplicates'].sort(key=lambda x: x['wasted_space'], reverse=True)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± JSON
        report_data = {
            'scan_date': datetime.now().isoformat(),
            'summary': {
                'total_files_scanned': self.results['total_files_scanned'],
                'duplicate_groups': self.results['duplicate_groups'],
                'total_duplicate_files': sum(d['count'] for d in self.results['exact_duplicates']),
                'space_wasted_bytes': self.results['space_wasted'],
                'space_wasted_mb': round(self.results['space_wasted'] / (1024 * 1024), 2)
            },
            'duplicates': self.results['exact_duplicates']
        }
        
        with open('exact_duplicates_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        # Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown
        self.create_markdown_report(report_data)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ù„Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù†
        self.create_cleanup_script(self.results['exact_duplicates'])
        
        return report_data
    
    def create_markdown_report(self, report_data):
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Markdown"""
        md = f"""# ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©

## ğŸ“… Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙØ­Øµ
- **Ø§Ù„ØªØ§Ø±ÙŠØ®**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙØ­ÙˆØµØ©**: {report_data['summary']['total_files_scanned']}
- **Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…ÙƒØ±Ø±Ø©**: {report_data['summary']['duplicate_groups']}
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©**: {report_data['summary']['total_duplicate_files']}
- **Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©**: {report_data['summary']['space_wasted_mb']} MB

## ğŸ“Š Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
"""
        
        for i, dup in enumerate(report_data['duplicates'][:20], 1):
            md += f"\n### {i}. Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø© ({dup['count']} Ù…Ù„ÙØ§Øª)\n"
            md += f"- **Ø§Ù„Ø­Ø¬Ù…**: {dup['file_size']:,} Ø¨Ø§ÙŠØª\n"
            md += f"- **Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©**: {dup['wasted_space']:,} Ø¨Ø§ÙŠØª\n"
            md += f"- **ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€**: `{dup['recommended_keep']}`\n"
            md += "- **Ø§Ù„Ù…Ù„ÙØ§Øª**:\n"
            
            for file in dup['files']:
                if file == dup['recommended_keep']:
                    md += f"  - âœ… `{file}` (Ø§Ø­ØªÙØ¸ Ø¨Ù‡Ø°Ø§)\n"
                else:
                    md += f"  - âŒ `{file}` (ÙŠÙ…ÙƒÙ† Ø­Ø°ÙÙ‡)\n"
        
        with open('exact_duplicates_report.md', 'w', encoding='utf-8') as f:
            f.write(md)
    
    def create_cleanup_script(self, duplicates):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª PowerShell Ù„Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù†"""
        ps_script = """# PowerShell Script Ù„Ù„Ø­Ø°Ù Ø§Ù„Ø¢Ù…Ù† Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
# ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø© ExactDuplicateFinder

$deletedCount = 0
$freedSpace = 0

Write-Host "ğŸ§¹ Ø¨Ø¯Ø¡ Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©..." -ForegroundColor Yellow

"""
        
        for dup in duplicates:
            ps_script += f"\n# Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø© - Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€: {dup['recommended_keep']}\n"
            
            for file in dup['files']:
                if file != dup['recommended_keep']:
                    ps_script += f"""
if (Test-Path "{file}") {{
    Remove-Item "{file}" -Force
    Write-Host "âœ… Ø­Ø°Ù: {file}" -ForegroundColor Green
    $deletedCount++
    $freedSpace += {dup['file_size']}
}}
"""
        
        ps_script += """
Write-Host ""
Write-Host "âœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡!" -ForegroundColor Green
Write-Host "ğŸ“Š Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©: $deletedCount" -ForegroundColor Cyan
Write-Host "ğŸ’¾ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ø­Ø±Ø±Ø©: $([math]::Round($freedSpace/1MB, 2)) MB" -ForegroundColor Cyan
"""
        
        with open('cleanup_exact_duplicates.ps1', 'w', encoding='utf-8') as f:
            f.write(ps_script)
        
        print("âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø³ÙƒØ±ÙŠØ¨Øª Ø§Ù„Ø­Ø°Ù: cleanup_exact_duplicates.ps1")


def main():
    finder = ExactDuplicateFinder()
    finder.scan_directory()
    finder.find_duplicates()
    report = finder.generate_report()
    
    print("\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ø¨Ø­Ø«!")
    print(f"ğŸ“Š Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {report['summary']['duplicate_groups']}")
    print(f"ğŸ’¾ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©: {report['summary']['space_wasted_mb']} MB")
    print("\nğŸ“„ Ø§Ù„ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ù…ÙÙ†Ø´Ø£Ø©:")
    print("   - exact_duplicates_report.json")
    print("   - exact_duplicates_report.md")
    print("   - cleanup_exact_duplicates.ps1")


if __name__ == "__main__":
    main() 