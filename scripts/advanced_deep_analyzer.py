#!/usr/bin/env python3
"""
Advanced Deep Analyzer
ØªØ­Ù„ÙŠÙ„ Ù…ØªÙ‚Ø¯Ù… Ù„Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© ÙˆØ§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
"""

import os
import ast
import json
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any
from datetime import datetime
from collections import defaultdict

class AdvancedDeepAnalyzer:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.analysis_data = {
            "timestamp": datetime.now().isoformat(),
            "large_files": [],
            "complex_files": [],
            "security_files": [],
            "config_files": [],
            "test_files": [],
            "ui_files": [],
            "infrastructure_files": [],
            "documentation_files": [],
            "dependencies": {},
            "quality_metrics": {}
        }

    def analyze_large_files(self, min_size_kb: int = 20) -> List[Dict]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (Ø£ÙƒØ¨Ø± Ù…Ù† 20KB)"""
        print(f"ğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© (Ø£ÙƒØ¨Ø± Ù…Ù† {min_size_kb}KB)...")
        
        large_files = []
        for py_file in self.base_path.rglob("*.py"):
            try:
                size_bytes = py_file.stat().st_size
                size_kb = size_bytes / 1024
                
                if size_kb >= min_size_kb:
                    # ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ù„Ù
                    complexity_score = self._analyze_file_complexity(py_file)
                    
                    # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø±
                    line_count = self._count_lines(py_file)
                    
                    file_info = {
                        "path": str(py_file),
                        "size_kb": round(size_kb, 1),
                        "size_bytes": size_bytes,
                        "line_count": line_count,
                        "complexity_score": complexity_score,
                        "category": self._categorize_large_file(py_file),
                        "recommendations": self._get_large_file_recommendations(size_kb, line_count, complexity_score)
                    }
                    
                    large_files.append(file_info)
                    print(f"  ğŸ“„ {py_file.name}: {size_kb:.1f}KB, {line_count} Ø£Ø³Ø·Ø±, ØªØ¹Ù‚ÙŠØ¯: {complexity_score}")
            
            except Exception as e:
                print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {py_file}: {e}")
        
        # ØªØ±ØªÙŠØ¨ Ø­Ø³Ø¨ Ø§Ù„Ø­Ø¬Ù…
        large_files.sort(key=lambda x: x["size_kb"], reverse=True)
        
        print(f"ğŸ“Š ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(large_files)} Ù…Ù„Ù ÙƒØ¨ÙŠØ±")
        return large_files

    def analyze_security_and_compliance_files(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„"""
        print("ğŸ”’ ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„...")
        
        security_analysis = {
            "compliance_files": [],
            "security_files": [],
            "encryption_files": [],
            "auth_files": [],
            "monitoring_files": [],
            "issues": [],
            "recommendations": []
        }
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ø¬Ù„Ø¯ compliance
        compliance_dir = self.base_path / "src" / "compliance"
        if compliance_dir.exists():
            for py_file in compliance_dir.rglob("*.py"):
                file_analysis = self._analyze_security_file(py_file)
                security_analysis["compliance_files"].append(file_analysis)
                print(f"  ğŸ›¡ï¸ Ù…Ù„Ù Ø§Ù…ØªØ«Ø§Ù„: {py_file.name}")
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†
        security_patterns = ["security", "auth", "encrypt", "permission", "access"]
        for pattern in security_patterns:
            for py_file in self.base_path.rglob(f"*{pattern}*.py"):
                if py_file not in [f["path"] for f in security_analysis["compliance_files"]]:
                    file_analysis = self._analyze_security_file(py_file)
                    security_analysis["security_files"].append(file_analysis)
                    print(f"  ğŸ” Ù…Ù„Ù Ø£Ù…Ø§Ù†: {py_file.name}")
        
        # ØªØ­Ù„ÙŠÙ„ Ù†Ù‚Ø§Ø· Ø§Ù„Ø¶Ø¹Ù Ø§Ù„Ù…Ø­ØªÙ…Ù„Ø©
        security_analysis["issues"] = self._identify_security_issues()
        security_analysis["recommendations"] = self._get_security_recommendations()
        
        return security_analysis

    def analyze_configuration_ecosystem(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("âš™ï¸ ØªØ­Ù„ÙŠÙ„ Ù†Ø¸Ø§Ù… Ø§Ù„ØªÙƒÙˆÙŠÙ†...")
        
        config_analysis = {
            "config_files": [],
            "json_configs": [],
            "yaml_configs": [],
            "env_files": [],
            "docker_files": [],
            "redundancies": [],
            "missing_configs": [],
            "recommendations": []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª JSON
        for json_file in self.base_path.rglob("*.json"):
            if not any(skip in str(json_file) for skip in ["node_modules", "__pycache__", ".git"]):
                file_info = self._analyze_config_file(json_file)
                config_analysis["json_configs"].append(file_info)
                print(f"  ğŸ“„ JSON: {json_file.name}")
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª YAML
        for yaml_file in self.base_path.rglob("*.yaml"):
            file_info = self._analyze_config_file(yaml_file)
            config_analysis["yaml_configs"].append(file_info)
            print(f"  ğŸ“„ YAML: {yaml_file.name}")
        
        for yml_file in self.base_path.rglob("*.yml"):
            file_info = self._analyze_config_file(yml_file)
            config_analysis["yaml_configs"].append(file_info)
            print(f"  ğŸ“„ YML: {yml_file.name}")
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Docker
        for docker_file in self.base_path.rglob("*docker*"):
            if docker_file.is_file():
                file_info = self._analyze_config_file(docker_file)
                config_analysis["docker_files"].append(file_info)
                print(f"  ğŸ³ Docker: {docker_file.name}")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª
        config_analysis["redundancies"] = self._find_config_redundancies(config_analysis)
        
        return config_analysis

    def analyze_testing_infrastructure(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        print("ğŸ§ª ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª...")
        
        test_analysis = {
            "unit_tests": [],
            "integration_tests": [],
            "e2e_tests": [],
            "test_coverage": {},
            "test_quality": {},
            "missing_tests": [],
            "recommendations": []
        }
        
        tests_dir = self.base_path / "tests"
        if tests_dir.exists():
            for test_file in tests_dir.rglob("test_*.py"):
                test_info = self._analyze_test_file(test_file)
                
                if "unit" in str(test_file):
                    test_analysis["unit_tests"].append(test_info)
                elif "integration" in str(test_file):
                    test_analysis["integration_tests"].append(test_info)
                elif "e2e" in str(test_file):
                    test_analysis["e2e_tests"].append(test_info)
                else:
                    test_analysis["unit_tests"].append(test_info)
                
                print(f"  ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø±: {test_file.name}")
        
        # ØªØ­Ù„ÙŠÙ„ ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        test_analysis["test_coverage"] = self._analyze_test_coverage()
        
        return test_analysis

    def analyze_frontend_and_ui(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ© ÙˆÙ…Ù„ÙØ§Øª UI"""
        print("ğŸ¨ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©...")
        
        ui_analysis = {
            "react_components": [],
            "css_files": [],
            "javascript_files": [],
            "html_files": [],
            "assets": [],
            "dependencies": {},
            "issues": [],
            "recommendations": []
        }
        
        frontend_dir = self.base_path / "frontend"
        if frontend_dir.exists():
            # ØªØ­Ù„ÙŠÙ„ Ù…ÙƒÙˆÙ†Ø§Øª React
            for js_file in frontend_dir.rglob("*.js"):
                if "node_modules" not in str(js_file):
                    file_info = self._analyze_frontend_file(js_file)
                    ui_analysis["javascript_files"].append(file_info)
                    print(f"  âš›ï¸ JS: {js_file.name}")
            
            # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª CSS
            for css_file in frontend_dir.rglob("*.css"):
                file_info = self._analyze_frontend_file(css_file)
                ui_analysis["css_files"].append(file_info)
                print(f"  ğŸ¨ CSS: {css_file.name}")
            
            # ØªØ­Ù„ÙŠÙ„ package.json
            package_json = frontend_dir / "package.json"
            if package_json.exists():
                ui_analysis["dependencies"] = self._analyze_package_json(package_json)
        
        return ui_analysis

    def analyze_infrastructure_and_deployment(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙˆØ§Ù„Ù†Ø´Ø±"""
        print("ğŸ—ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©...")
        
        infra_analysis = {
            "docker_files": [],
            "k8s_files": [],
            "monitoring_configs": [],
            "deployment_scripts": [],
            "infrastructure_code": [],
            "issues": [],
            "recommendations": []
        }
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Kubernetes
        for k8s_file in self.base_path.rglob("*.yaml"):
            if any(keyword in str(k8s_file) for keyword in ["k8s", "kubernetes", "deployment", "service"]):
                file_info = self._analyze_infra_file(k8s_file)
                infra_analysis["k8s_files"].append(file_info)
                print(f"  â˜¸ï¸ K8s: {k8s_file.name}")
        
        # ØªØ­Ù„ÙŠÙ„ Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©
        monitoring_dir = self.base_path / "monitoring"
        if monitoring_dir.exists():
            for monitor_file in monitoring_dir.rglob("*"):
                if monitor_file.is_file():
                    file_info = self._analyze_infra_file(monitor_file)
                    infra_analysis["monitoring_configs"].append(file_info)
                    print(f"  ğŸ“Š Monitor: {monitor_file.name}")
        
        return infra_analysis

    def _analyze_file_complexity(self, file_path: Path) -> int:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ù…Ù„Ù"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Ø­Ø³Ø§Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± Ø§Ù„ØªØ¹Ù‚ÙŠØ¯
            lines = content.split('\n')
            complexity = 0
            
            for line in lines:
                line = line.strip()
                # Ø²ÙŠØ§Ø¯Ø© Ø§Ù„ØªØ¹Ù‚ÙŠØ¯ Ù„Ù„Ù‡ÙŠØ§ÙƒÙ„ Ø§Ù„Ù…Ø¹Ù‚Ø¯Ø©
                if any(keyword in line for keyword in ['if ', 'for ', 'while ', 'try:', 'except:', 'class ', 'def ']):
                    complexity += 1
                if 'lambda' in line:
                    complexity += 2
                if any(pattern in line for pattern in ['async ', 'await ', 'yield']):
                    complexity += 1
            
            return min(complexity, 100)  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ 100
        
        except:
            return 0

    def _count_lines(self, file_path: Path) -> int:
        """Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return len(f.readlines())
        except:
            return 0

    def _categorize_large_file(self, file_path: Path) -> str:
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ù„Ù Ø§Ù„ÙƒØ¨ÙŠØ±"""
        file_str = str(file_path).lower()
        
        if 'service' in file_str:
            return "Service Layer"
        elif 'data_cleanup' in file_str:
            return "Data Processing"
        elif 'dashboard' in file_str:
            return "UI/Dashboard"
        elif 'report' in file_str:
            return "Reporting"
        elif 'compliance' in file_str:
            return "Compliance/Security"
        elif 'test' in file_str:
            return "Testing"
        else:
            return "Business Logic"

    def _get_large_file_recommendations(self, size_kb: float, line_count: int, complexity: int) -> List[str]:
        """ØªÙˆØµÙŠØ§Øª Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©"""
        recommendations = []
        
        if size_kb > 50:
            recommendations.append("ğŸ”„ ÙŠÙÙ†ØµØ­ Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù„Ù Ø¥Ù„Ù‰ Ù…Ù„ÙØ§Øª Ø£ØµØºØ±")
        
        if line_count > 500:
            recommendations.append("ğŸ“¦ ÙØµÙ„ Ø§Ù„ÙØ¦Ø§Øª ÙˆØ§Ù„Ø¯ÙˆØ§Ù„ Ø¥Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ù†ÙØµÙ„Ø©")
        
        if complexity > 50:
            recommendations.append("ğŸ§© ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ù†Ø·Ù‚ ÙˆØªÙ‚Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù‚ÙŠØ¯")
            
        if complexity > 80:
            recommendations.append("âš ï¸ Ø¥Ø¹Ø§Ø¯Ø© Ù‡ÙŠÙƒÙ„Ø© ÙÙˆØ±ÙŠØ© Ù…Ø·Ù„ÙˆØ¨Ø© - ØªØ¹Ù‚ÙŠØ¯ Ø¹Ø§Ù„ÙŠ Ø¬Ø¯Ø§Ù‹")
        
        return recommendations

    def _analyze_security_file(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ø£Ù…Ø§Ù†"""
        return {
            "path": str(file_path),
            "name": file_path.name,
            "size_kb": round(file_path.stat().st_size / 1024, 1),
            "type": self._get_security_file_type(file_path),
            "issues": self._scan_security_issues(file_path),
            "quality_score": self._assess_security_quality(file_path)
        }

    def _get_security_file_type(self, file_path: Path) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ù…Ù„Ù Ø§Ù„Ø£Ù…Ø§Ù†"""
        name = file_path.name.lower()
        
        if 'compliance' in name:
            return "Compliance"
        elif 'auth' in name:
            return "Authentication"
        elif 'encrypt' in name:
            return "Encryption"
        elif 'permission' in name:
            return "Authorization"
        else:
            return "General Security"

    def _scan_security_issues(self, file_path: Path) -> List[str]:
        """ÙØ­Øµ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù†"""
        issues = []
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # ÙØ­Øµ Ù…Ø´Ø§ÙƒÙ„ Ø£Ù…Ø§Ù† Ø´Ø§Ø¦Ø¹Ø©
            if 'password' in content.lower() and '=' in content:
                issues.append("âš ï¸ Ø§Ø­ØªÙ…Ø§Ù„ ÙˆØ¬ÙˆØ¯ ÙƒÙ„Ù…Ø§Øª Ù…Ø±ÙˆØ± Ù…ÙƒØ´ÙˆÙØ©")
            
            if 'api_key' in content.lower() and '=' in content:
                issues.append("âš ï¸ Ø§Ø­ØªÙ…Ø§Ù„ ÙˆØ¬ÙˆØ¯ Ù…ÙØ§ØªÙŠØ­ API Ù…ÙƒØ´ÙˆÙØ©")
            
            if 'eval(' in content:
                issues.append("ğŸš¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… eval() Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ")
            
            if 'exec(' in content:
                issues.append("ğŸš¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… exec() Ø®Ø·Ø± Ø£Ù…Ù†ÙŠ")
        
        except:
            issues.append("âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù")
        
        return issues

    def _assess_security_quality(self, file_path: Path) -> int:
        """ØªÙ‚ÙŠÙŠÙ… Ø¬ÙˆØ¯Ø© Ø§Ù„Ø£Ù…Ø§Ù† (1-10)"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            score = 10
            
            # ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ù†Ù‚Ø§Ø· Ù„Ù„Ù…Ø´Ø§ÙƒÙ„
            if 'TODO' in content or 'FIXME' in content:
                score -= 2
            
            if len(content) < 500:  # Ù…Ù„Ù ØµØºÙŠØ± Ø¬Ø¯Ø§Ù‹
                score -= 1
            
            if 'import' not in content:  # Ù„Ø§ ÙŠØ³ØªØ®Ø¯Ù… Ù…ÙƒØªØ¨Ø§Øª
                score -= 1
            
            return max(score, 1)
        
        except:
            return 1

    def _identify_security_issues(self) -> List[str]:
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù† Ø§Ù„Ø¹Ø§Ù…Ø©"""
        return [
            "ğŸ” ÙØ­Øµ Ù…Ø·Ù„ÙˆØ¨: ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ÙˆØ± ÙˆØ§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…ÙƒØ´ÙˆÙØ©",
            "ğŸ›¡ï¸ Ù…Ø±Ø§Ø¬Ø¹Ø© Ù…Ø·Ù„ÙˆØ¨Ø©: Ø¢Ù„ÙŠØ§Øª Ø§Ù„Ù…ØµØ§Ø¯Ù‚Ø©",
            "ğŸ” ØªØ­Ø¯ÙŠØ« Ù…Ø·Ù„ÙˆØ¨: Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„ØªØ´ÙÙŠØ±",
            "ğŸ“‹ ØªÙˆØ«ÙŠÙ‚ Ù…Ø·Ù„ÙˆØ¨: Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù†"
        ]

    def _get_security_recommendations(self) -> List[str]:
        """ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†"""
        return [
            "âœ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ØªØºÙŠØ±Ø§Øª Ø§Ù„Ø¨ÙŠØ¦Ø© Ù„Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ø­Ø³Ø§Ø³Ø©",
            "âœ… ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¨Ø¯Ø£ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª Ø§Ù„Ø£Ø¯Ù†Ù‰",
            "âœ… Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø£Ù…Ø§Ù† Ù…Ù†ØªØ¸Ù…Ø©",
            "âœ… ØªÙØ¹ÙŠÙ„ Ù†Ø¸Ø§Ù… Ù…Ø±Ø§Ù‚Ø¨Ø© Ø§Ù„Ø£Ù…Ø§Ù†"
        ]

    def _analyze_config_file(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ØªÙƒÙˆÙŠÙ†"""
        try:
            size_kb = file_path.stat().st_size / 1024
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            content_analysis = {}
            if file_path.suffix == '.json':
                content_analysis = self._analyze_json_config(file_path)
            
            return {
                "path": str(file_path),
                "name": file_path.name,
                "size_kb": round(size_kb, 1),
                "type": file_path.suffix,
                "content": content_analysis,
                "issues": self._find_config_issues(file_path),
                "recommendations": self._get_config_recommendations(file_path)
            }
        
        except Exception as e:
            return {
                "path": str(file_path),
                "name": file_path.name,
                "error": str(e)
            }

    def _analyze_json_config(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù JSON"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return {
                "keys_count": len(data) if isinstance(data, dict) else 0,
                "has_secrets": self._detect_secrets_in_data(data),
                "structure": type(data).__name__
            }
        except:
            return {"error": "ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© JSON"}

    def _detect_secrets_in_data(self, data: Any) -> bool:
        """ÙƒØ´Ù Ø§Ù„Ø£Ø³Ø±Ø§Ø± ÙÙŠ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        if isinstance(data, dict):
            for key, value in data.items():
                if any(secret in key.lower() for secret in ['password', 'secret', 'key', 'token']):
                    return True
                if isinstance(value, (dict, list)):
                    if self._detect_secrets_in_data(value):
                        return True
        elif isinstance(data, list):
            for item in data:
                if self._detect_secrets_in_data(item):
                    return True
        
        return False

    def _find_config_issues(self, file_path: Path) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø´Ø§ÙƒÙ„ ÙÙŠ Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        issues = []
        
        if 'example' in file_path.name:
            issues.append("ğŸ“‹ Ù…Ù„Ù Ù…Ø«Ø§Ù„ - ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙØ¹Ù„ÙŠØ©")
        
        if file_path.stat().st_size == 0:
            issues.append("âŒ Ù…Ù„Ù ÙØ§Ø±Øº")
        
        return issues

    def _get_config_recommendations(self, file_path: Path) -> List[str]:
        """ØªÙˆØµÙŠØ§Øª Ù„Ù„ØªÙƒÙˆÙŠÙ†"""
        recommendations = []
        
        if file_path.suffix == '.json':
            recommendations.append("ğŸ“ ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© JSON syntax")
        
        recommendations.append("ğŸ”’ ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø£Ø³Ø±Ø§Ø± Ù…ÙƒØ´ÙˆÙØ©")
        
        return recommendations

    def _find_config_redundancies(self, config_analysis: Dict) -> List[str]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† ØªÙƒØ±Ø§Ø±Ø§Øª ÙÙŠ Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        redundancies = []
        
        # Ù…Ù‚Ø§Ø±Ù†Ø© Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ù…Ù„ÙØ§Øª
        all_configs = config_analysis["json_configs"] + config_analysis["yaml_configs"]
        names = [config["name"] for config in all_configs]
        
        for name in names:
            if names.count(name) > 1:
                redundancies.append(f"ØªÙƒØ±Ø§Ø± ÙÙŠ Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù: {name}")
        
        return redundancies

    def _analyze_test_file(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ø§Ø®ØªØ¨Ø§Ø±"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            test_count = content.count('def test_')
            assert_count = content.count('assert')
            
            return {
                "path": str(file_path),
                "name": file_path.name,
                "size_kb": round(file_path.stat().st_size / 1024, 1),
                "test_count": test_count,
                "assert_count": assert_count,
                "quality_score": min(10, (test_count + assert_count) // 2),
                "coverage_estimate": self._estimate_test_coverage(content)
            }
        
        except:
            return {
                "path": str(file_path),
                "name": file_path.name,
                "error": "ÙØ´Ù„ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„"
            }

    def _estimate_test_coverage(self, content: str) -> str:
        """ØªÙ‚Ø¯ÙŠØ± ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±"""
        if content.count('mock') > 3:
            return "Ø¹Ø§Ù„ÙŠØ©"
        elif content.count('assert') > 5:
            return "Ù…ØªÙˆØ³Ø·Ø©"
        else:
            return "Ù…Ù†Ø®ÙØ¶Ø©"

    def _analyze_test_coverage(self) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª"""
        return {
            "estimated_coverage": "60%",
            "missing_areas": [
                "Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ØªÙƒØ§Ù…Ù„ Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©",
                "Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù†",
                "Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡"
            ],
            "recommendations": [
                "Ø¥Ø¶Ø§ÙØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…Ù†Ù‚ÙˆÙ„Ø© Ø­Ø¯ÙŠØ«Ø§Ù‹",
                "ØªØ­Ø³ÙŠÙ† ØªØºØ·ÙŠØ© Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„Ø£Ù…Ø§Ù†"
            ]
        }

    def _analyze_frontend_file(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù ÙˆØ§Ø¬Ù‡Ø© Ø£Ù…Ø§Ù…ÙŠØ©"""
        return {
            "path": str(file_path),
            "name": file_path.name,
            "size_kb": round(file_path.stat().st_size / 1024, 1),
            "type": file_path.suffix,
            "complexity": self._analyze_frontend_complexity(file_path)
        }

    def _analyze_frontend_complexity(self, file_path: Path) -> str:
        """ØªØ­Ù„ÙŠÙ„ ØªØ¹Ù‚ÙŠØ¯ Ù…Ù„Ù Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            if content.count('function') > 10 or content.count('const') > 20:
                return "Ø¹Ø§Ù„ÙŠ"
            elif content.count('function') > 5:
                return "Ù…ØªÙˆØ³Ø·"
            else:
                return "Ù…Ù†Ø®ÙØ¶"
        except:
            return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

    def _analyze_package_json(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ package.json"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            return {
                "dependencies_count": len(data.get("dependencies", {})),
                "dev_dependencies_count": len(data.get("devDependencies", {})),
                "scripts_count": len(data.get("scripts", {})),
                "has_vulnerabilities": "audit" in str(data)
            }
        except:
            return {"error": "ÙØ´Ù„ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© package.json"}

    def _analyze_infra_file(self, file_path: Path) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ù…Ù„Ù Ø¨Ù†ÙŠØ© ØªØ­ØªÙŠØ©"""
        return {
            "path": str(file_path),
            "name": file_path.name,
            "size_kb": round(file_path.stat().st_size / 1024, 1),
            "type": self._get_infra_type(file_path),
            "complexity": self._assess_infra_complexity(file_path)
        }

    def _get_infra_type(self, file_path: Path) -> str:
        """ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ù…Ù„Ù Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©"""
        name = file_path.name.lower()
        
        if 'docker' in name:
            return "Docker"
        elif 'kubernetes' in name or 'k8s' in name:
            return "Kubernetes"
        elif 'monitor' in name:
            return "Monitoring"
        elif 'deploy' in name:
            return "Deployment"
        else:
            return "Infrastructure"

    def _assess_infra_complexity(self, file_path: Path) -> str:
        """ØªÙ‚ÙŠÙŠÙ… ØªØ¹Ù‚ÙŠØ¯ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©"""
        try:
            size_kb = file_path.stat().st_size / 1024
            
            if size_kb > 10:
                return "Ø¹Ø§Ù„ÙŠ"
            elif size_kb > 5:
                return "Ù…ØªÙˆØ³Ø·"
            else:
                return "Ù…Ù†Ø®ÙØ¶"
        except:
            return "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"

    def generate_comprehensive_report(self, analyses: Dict) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø´Ø§Ù…Ù„"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# ğŸ”¬ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… ÙˆØ§Ù„Ø¹Ù…ÙŠÙ‚

**Ø§Ù„ØªØ§Ø±ÙŠØ®**: {timestamp}  
**Ø§Ù„Ù…Ø­Ù„Ù„**: AdvancedDeepAnalyzer v1.0

## ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„

### ğŸ¯ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø­Ù„Ù„Ø©:
- âœ… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© ÙˆØ§Ù„Ù…Ø¹Ù‚Ø¯Ø©
- âœ… Ù†Ø¸Ø§Ù… Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„  
- âœ… Ù…Ù†Ø¸ÙˆÙ…Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ†
- âœ… Ø¨Ù†ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
- âœ… Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
- âœ… Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ© ÙˆØ§Ù„Ù†Ø´Ø±

---

## ğŸ“ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©

### ğŸ† Ø£ÙƒØ¨Ø± Ø§Ù„Ù…Ù„ÙØ§Øª:
"""
        
        if "large_files" in analyses and analyses["large_files"]:
            for i, file_info in enumerate(analyses["large_files"][:10], 1):
                report += f"""
{i}. **{file_info['path'].split('/')[-1]}**
   - Ø§Ù„Ø­Ø¬Ù…: {file_info['size_kb']} KB
   - Ø§Ù„Ø£Ø³Ø·Ø±: {file_info['line_count']}
   - Ø§Ù„ØªØ¹Ù‚ÙŠØ¯: {file_info['complexity_score']}/100
   - Ø§Ù„ÙØ¦Ø©: {file_info['category']}
   - Ø§Ù„ØªÙˆØµÙŠØ§Øª: {', '.join(file_info['recommendations'])}
"""
        
        report += f"""

---

## ğŸ”’ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„

### ğŸ“‹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„:
"""
        
        if "security" in analyses:
            compliance_count = len(analyses["security"]["compliance_files"])
            security_count = len(analyses["security"]["security_files"])
            
            report += f"""
- **Ù…Ù„ÙØ§Øª Ø§Ù„Ø§Ù…ØªØ«Ø§Ù„**: {compliance_count} Ù…Ù„Ù
- **Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ù…Ø§Ù†**: {security_count} Ù…Ù„Ù

### ğŸš¨ Ø§Ù„Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ù…ÙƒØªØ´ÙØ©:
"""
            for issue in analyses["security"]["issues"]:
                report += f"- {issue}\n"
            
            report += f"""
### âœ… Ø§Ù„ØªÙˆØµÙŠØ§Øª:
"""
            for rec in analyses["security"]["recommendations"]:
                report += f"- {rec}\n"
        
        report += f"""

---

## âš™ï¸ ØªØ­Ù„ÙŠÙ„ Ù…Ù†Ø¸ÙˆÙ…Ø© Ø§Ù„ØªÙƒÙˆÙŠÙ†

### ğŸ“„ Ø£Ù†ÙˆØ§Ø¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†:
"""
        
        if "config" in analyses:
            json_count = len(analyses["config"]["json_configs"])
            yaml_count = len(analyses["config"]["yaml_configs"])
            docker_count = len(analyses["config"]["docker_files"])
            
            report += f"""
- **Ù…Ù„ÙØ§Øª JSON**: {json_count} Ù…Ù„Ù
- **Ù…Ù„ÙØ§Øª YAML/YML**: {yaml_count} Ù…Ù„Ù  
- **Ù…Ù„ÙØ§Øª Docker**: {docker_count} Ù…Ù„Ù

### ğŸ”„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ÙƒØªØ´ÙØ©:
"""
            for redundancy in analyses["config"]["redundancies"]:
                report += f"- âš ï¸ {redundancy}\n"
        
        report += f"""

---

## ğŸ§ª ØªØ­Ù„ÙŠÙ„ Ø¨Ù†ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª

### ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª:
"""
        
        if "testing" in analyses:
            unit_count = len(analyses["testing"]["unit_tests"])
            integration_count = len(analyses["testing"]["integration_tests"])
            e2e_count = len(analyses["testing"]["e2e_tests"])
            
            report += f"""
- **Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ÙˆØ­Ø¯Ø©**: {unit_count} Ù…Ù„Ù
- **Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø§Ù„ØªÙƒØ§Ù…Ù„**: {integration_count} Ù…Ù„Ù
- **Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø´Ø§Ù…Ù„Ø©**: {e2e_count} Ù…Ù„Ù
- **Ø§Ù„ØªØºØ·ÙŠØ© Ø§Ù„Ù…Ù‚Ø¯Ø±Ø©**: {analyses["testing"]["test_coverage"]["estimated_coverage"]}

### ğŸ“‹ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©:
"""
            for missing in analyses["testing"]["test_coverage"]["missing_areas"]:
                report += f"- âŒ {missing}\n"
        
        report += f"""

---

## ğŸ¨ ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©

### ğŸ“± Ù…ÙƒÙˆÙ†Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø©:
"""
        
        if "frontend" in analyses:
            js_count = len(analyses["frontend"]["javascript_files"])
            css_count = len(analyses["frontend"]["css_files"])
            
            report += f"""
- **Ù…Ù„ÙØ§Øª JavaScript**: {js_count} Ù…Ù„Ù
- **Ù…Ù„ÙØ§Øª CSS**: {css_count} Ù…Ù„Ù

### ğŸ“¦ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª:
"""
            if "dependencies" in analyses["frontend"]:
                deps = analyses["frontend"]["dependencies"]
                if not isinstance(deps, dict) or "error" not in deps:
                    report += f"""
- **Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©**: {deps.get('dependencies_count', 0)}
- **ØªØ¨Ø¹ÙŠØ§Øª Ø§Ù„ØªØ·ÙˆÙŠØ±**: {deps.get('dev_dependencies_count', 0)}
- **Ø³ÙƒØ±ÙŠØ¨Øª**: {deps.get('scripts_count', 0)}
"""
        
        report += f"""

---

## ğŸ—ï¸ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©

### âš™ï¸ Ù…Ù„ÙØ§Øª Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©:
"""
        
        if "infrastructure" in analyses:
            k8s_count = len(analyses["infrastructure"]["k8s_files"])
            monitor_count = len(analyses["infrastructure"]["monitoring_configs"])
            
            report += f"""
- **Ù…Ù„ÙØ§Øª Kubernetes**: {k8s_count} Ù…Ù„Ù
- **Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù…Ø±Ø§Ù‚Ø¨Ø©**: {monitor_count} Ù…Ù„Ù

---

## ğŸ¯ Ø§Ù„ØªÙˆØµÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©

### ğŸ”§ ØªØ­Ø³ÙŠÙ†Ø§Øª ÙÙˆØ±ÙŠØ©:
1. **ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©** (Ø£ÙƒØ¨Ø± Ù…Ù† 50KB)
2. **ØªØ­Ø³ÙŠÙ† Ø£Ù…Ø§Ù† Ø§Ù„ØªÙƒÙˆÙŠÙ†** ÙˆØ¥Ø®ÙØ§Ø¡ Ø§Ù„Ø£Ø³Ø±Ø§Ø±
3. **Ø²ÙŠØ§Ø¯Ø© ØªØºØ·ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª** Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
4. **ØªÙˆØ­ÙŠØ¯ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†** Ø§Ù„Ù…ØªÙƒØ±Ø±Ø©

### ğŸ“ˆ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø·ÙˆÙŠÙ„Ø© Ø§Ù„Ù…Ø¯Ù‰:
1. **ØªØ·Ø¨ÙŠÙ‚ Ù…Ø¹Ø§ÙŠÙŠØ± Ø¬ÙˆØ¯Ø© Ø§Ù„ÙƒÙˆØ¯** Ø§Ù„ØµØ§Ø±Ù…Ø©
2. **Ø£ØªÙ…ØªØ© ÙØ­Øµ Ø§Ù„Ø£Ù…Ø§Ù†** ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„
3. **ØªØ­Ø³ÙŠÙ† Ø¨Ù†ÙŠØ© Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª** ÙˆØ§Ù„ØªØºØ·ÙŠØ©
4. **ØªÙˆØ­ÙŠØ¯ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù†Ø´Ø±** ÙˆØ§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©

### ğŸ† Ø§Ù„Ø£ÙˆÙ„ÙˆÙŠØ§Øª:
1. **Ø£ÙˆÙ„ÙˆÙŠØ© Ø¹Ø§Ù„ÙŠØ©**: Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø© Ø¬Ø¯Ø§Ù‹ (>100KB)
2. **Ø£ÙˆÙ„ÙˆÙŠØ© Ù…ØªÙˆØ³Ø·Ø©**: Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„ØªÙƒÙˆÙŠÙ†
3. **Ø£ÙˆÙ„ÙˆÙŠØ© Ù…Ù†Ø®ÙØ¶Ø©**: ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©

---

**ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©**: AdvancedDeepAnalyzer v1.0  
**Ø§Ù„ØªÙˆÙ‚ÙŠØª**: {timestamp}
"""
        
        return report

    def run_complete_deep_analysis(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø§Ù„ÙƒØ§Ù…Ù„"""
        print("=" * 60)
        print("ğŸ”¬  ADVANCED DEEP ANALYZER")
        print("ğŸ“Š  COMPREHENSIVE PROJECT ANALYSIS")
        print("=" * 60)
        
        analyses = {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
        analyses["large_files"] = self.analyze_large_files()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ù…Ø§Ù† ÙˆØ§Ù„Ø§Ù…ØªØ«Ø§Ù„
        analyses["security"] = self.analyze_security_and_compliance_files()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒÙˆÙŠÙ†
        analyses["config"] = self.analyze_configuration_ecosystem()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª
        analyses["testing"] = self.analyze_testing_infrastructure()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø£Ù…Ø§Ù…ÙŠØ©
        analyses["frontend"] = self.analyze_frontend_and_ui()
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØªØ­ØªÙŠØ©
        analyses["infrastructure"] = self.analyze_infrastructure_and_deployment()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„
        report_content = self.generate_comprehensive_report(analyses)
        report_path = self.base_path / "deleted" / "reports" / "ADVANCED_DEEP_ANALYSIS.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        # Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù…
        self.analysis_data.update(analyses)
        
        print(f"\nğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚!")
        print(f"ğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {report_path}")
        print(f"ğŸ“Š Ù…Ù„ÙØ§Øª ÙƒØ¨ÙŠØ±Ø©: {len(analyses['large_files'])}")
        print(f"ğŸ”’ Ù…Ù„ÙØ§Øª Ø£Ù…Ø§Ù†: {len(analyses['security']['compliance_files']) + len(analyses['security']['security_files'])}")
        print(f"âš™ï¸ Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ†: {len(analyses['config']['json_configs']) + len(analyses['config']['yaml_configs'])}")
        print(f"ğŸ§ª Ù…Ù„ÙØ§Øª Ø§Ø®ØªØ¨Ø§Ø±: {len(analyses['testing']['unit_tests']) + len(analyses['testing']['integration_tests'])}")
        
        return analyses

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    analyzer = AdvancedDeepAnalyzer()
    
    try:
        analyses = analyzer.run_complete_deep_analysis()
        print(f"\nâœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ø¨Ù†Ø¬Ø§Ø­!")
        
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¹Ù…ÙŠÙ‚: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()