#!/usr/bin/env python3
"""
Intelligent Config Merger
Ø£Ø¯Ø§Ø© Ø°ÙƒÙŠØ© Ù„Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ù…Ø¹ ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ ÙˆØ§Ù„Ù…ÙŠØ²Ø§Øª
"""

import os
import json
import yaml
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple, Any, Optional
from datetime import datetime
from collections import defaultdict
import difflib
import re

class IntelligentConfigMerger:
    def __init__(self, base_path: str = "."):
        self.base_path = Path(base_path)
        self.config_analysis = {
            "timestamp": datetime.now().isoformat(),
            "total_configs": 0,
            "identical_duplicates": [],
            "similar_configs": [],
            "merged_configs": [],
            "deleted_configs": [],
            "updated_references": [],
            "errors": []
        }

    def discover_all_config_files(self) -> Dict[str, List[Path]]:
        """Ø§ÙƒØªØ´Ø§Ù Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print("ğŸ” Ø§ÙƒØªØ´Ø§Ù Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†...")
        
        config_files = {
            "json": [],
            "yaml": [],
            "yml": [],
            "env": [],
            "conf": [],
            "ini": [],
            "toml": []
        }
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ù…Ù„ÙØ§Øª Ù„Ù„Ø¨Ø­Ø«
        patterns = {
            "json": ["*.json"],
            "yaml": ["*.yaml"],
            "yml": ["*.yml"],
            "env": ["*.env", ".env*"],
            "conf": ["*.conf", "*.config"],
            "ini": ["*.ini"],
            "toml": ["*.toml"]
        }
        
        # Ù…Ø¬Ù„Ø¯Ø§Øª Ù„Ù„ØªØ¬Ø§Ù‡Ù„
        ignore_dirs = {
            "__pycache__", ".git", "node_modules", ".venv", "venv",
            ".mypy_cache", ".pytest_cache", "dist", "build"
        }
        
        for file_type, file_patterns in patterns.items():
            for pattern in file_patterns:
                for file_path in self.base_path.rglob(pattern):
                    # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª Ø§Ù„Ù…Ø³ØªØ¨Ø¹Ø¯Ø©
                    if any(ignore_dir in str(file_path) for ignore_dir in ignore_dirs):
                        continue
                    
                    config_files[file_type].append(file_path)
                    print(f"  ğŸ“„ {file_type.upper()}: {file_path.name}")
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
        total = sum(len(files) for files in config_files.values())
        print(f"\nğŸ“Š ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {total} Ù…Ù„Ù ØªÙƒÙˆÙŠÙ†:")
        for file_type, files in config_files.items():
            if files:
                print(f"  {file_type.upper()}: {len(files)} Ù…Ù„Ù")
        
        self.config_analysis["total_configs"] = total
        return config_files

    def load_config_content(self, file_path: Path) -> Tuple[Optional[Dict], str, str]:
        """ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                raw_content = f.read()
            
            # Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø®Ø§Ù…
            content_hash = hashlib.md5(raw_content.encode()).hexdigest()
            
            # Ù…Ø­Ø§ÙˆÙ„Ø© ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            parsed_content = None
            file_ext = file_path.suffix.lower()
            
            if file_ext == '.json':
                try:
                    parsed_content = json.loads(raw_content)
                except json.JSONDecodeError:
                    pass
            elif file_ext in ['.yaml', '.yml']:
                try:
                    parsed_content = yaml.safe_load(raw_content)
                except yaml.YAMLError:
                    pass
            
            return parsed_content, raw_content, content_hash
        
        except Exception as e:
            print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ù‚Ø±Ø§Ø¡Ø© {file_path}: {e}")
            return None, "", ""

    def calculate_content_similarity(self, content1: str, content2: str) -> float:
        """Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† Ù…Ø­ØªÙˆÙŠÙŠÙ†"""
        if not content1 or not content2:
            return 0.0
        
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©
        clean1 = self._clean_content_for_comparison(content1)
        clean2 = self._clean_content_for_comparison(content2)
        
        if clean1 == clean2:
            return 100.0
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… difflib Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        similarity = difflib.SequenceMatcher(None, clean1, clean2).ratio()
        return similarity * 100

    def _clean_content_for_comparison(self, content: str) -> str:
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª
        lines = []
        for line in content.split('\n'):
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª ÙÙŠ JSON Ùˆ YAML
            if '//' in line:
                line = line.split('//')[0]
            if '#' in line and not line.strip().startswith('"'):
                line = line.split('#')[0]
            
            # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ø³Ø§ÙØ§Øª Ø§Ù„Ø²Ø§Ø¦Ø¯Ø©
            line = line.strip()
            if line:
                lines.append(line)
        
        return '\n'.join(lines)

    def analyze_config_duplicates(self, config_files: Dict[str, List[Path]]) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        print("\nğŸ” ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª...")
        
        analysis = {
            "identical_groups": [],      # Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%
            "similar_groups": [],        # Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© 70-99%
            "unique_files": [],          # Ù…Ù„ÙØ§Øª ÙØ±ÙŠØ¯Ø©
            "content_map": {},           # Ø®Ø±ÙŠØ·Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰
            "similarity_matrix": {}      # Ù…ØµÙÙˆÙØ© Ø§Ù„ØªØ´Ø§Ø¨Ù‡
        }
        
        # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        all_files = []
        for file_type, files in config_files.items():
            all_files.extend(files)
        
        print(f"ğŸ“‹ ØªØ­Ù„ÙŠÙ„ {len(all_files)} Ù…Ù„Ù...")
        
        # ØªØ­Ù…ÙŠÙ„ Ù…Ø­ØªÙˆÙ‰ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª
        file_contents = {}
        hash_to_files = defaultdict(list)
        
        for file_path in all_files:
            parsed, raw, content_hash = self.load_config_content(file_path)
            
            file_contents[str(file_path)] = {
                "parsed": parsed,
                "raw": raw,
                "hash": content_hash,
                "size": len(raw),
                "path": file_path
            }
            
            # ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø­Ø³Ø¨ hash (Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%)
            if content_hash and raw.strip():
                hash_to_files[content_hash].append(str(file_path))
        
        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©
        for content_hash, file_paths in hash_to_files.items():
            if len(file_paths) > 1:
                analysis["identical_groups"].append({
                    "hash": content_hash,
                    "files": file_paths,
                    "count": len(file_paths),
                    "size": file_contents[file_paths[0]]["size"]
                })
                print(f"  ğŸ”„ Ø¹Ø«Ø± Ø¹Ù„Ù‰ {len(file_paths)} Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø©:")
                for fp in file_paths[:3]:  # Ø£ÙˆÙ„ 3 ÙÙ‚Ø·
                    print(f"    - {Path(fp).name}")
                if len(file_paths) > 3:
                    print(f"    ... Ùˆ{len(file_paths) - 3} Ø£Ø®Ø±Ù‰")
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª (70-99%)
        unique_files = [fp for fp in file_contents.keys() 
                       if all(fp not in group["files"] for group in analysis["identical_groups"])]
        
        print(f"\nğŸ” ÙØ­Øµ Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ø¨ÙŠÙ† {len(unique_files)} Ù…Ù„Ù ÙØ±ÙŠØ¯...")
        
        similar_groups = []
        processed_files = set()
        
        for i, file1 in enumerate(unique_files):
            if file1 in processed_files:
                continue
            
            content1 = file_contents[file1]["raw"]
            if not content1.strip():
                continue
            
            similar_files = [file1]
            
            for j, file2 in enumerate(unique_files[i+1:], i+1):
                if file2 in processed_files:
                    continue
                
                content2 = file_contents[file2]["raw"]
                similarity = self.calculate_content_similarity(content1, content2)
                
                if 70 <= similarity < 100:
                    similar_files.append(file2)
                    analysis["similarity_matrix"][f"{file1}:{file2}"] = similarity
            
            if len(similar_files) > 1:
                # Ø­Ø³Ø§Ø¨ Ù…ØªÙˆØ³Ø· Ø§Ù„ØªØ´Ø§Ø¨Ù‡ Ù„Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©
                similarities = []
                for x in range(len(similar_files)):
                    for y in range(x+1, len(similar_files)):
                        sim = self.calculate_content_similarity(
                            file_contents[similar_files[x]]["raw"],
                            file_contents[similar_files[y]]["raw"]
                        )
                        similarities.append(sim)
                
                avg_similarity = sum(similarities) / len(similarities) if similarities else 0
                
                similar_groups.append({
                    "files": similar_files,
                    "count": len(similar_files),
                    "average_similarity": avg_similarity
                })
                
                print(f"  ğŸ”— Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªØ´Ø§Ø¨Ù‡Ø© ({avg_similarity:.1f}% ØªØ´Ø§Ø¨Ù‡):")
                for sf in similar_files[:3]:
                    print(f"    - {Path(sf).name}")
                if len(similar_files) > 3:
                    print(f"    ... Ùˆ{len(similar_files) - 3} Ø£Ø®Ø±Ù‰")
                
                processed_files.update(similar_files)
        
        analysis["similar_groups"] = similar_groups
        analysis["content_map"] = file_contents
        
        return analysis

    def merge_similar_configs(self, similar_group: Dict, content_map: Dict) -> Dict:
        """Ø¯Ù…Ø¬ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©"""
        files = similar_group["files"]
        print(f"\nğŸ”„ Ø¯Ù…Ø¬ Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† {len(files)} Ù…Ù„ÙØ§Øª...")
        
        # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ (Ø§Ù„Ø£ÙƒØ¨Ø± Ø­Ø¬Ù…Ø§Ù‹ Ø£Ùˆ Ø§Ù„Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹)
        primary_file = max(files, key=lambda f: content_map[f]["size"])
        other_files = [f for f in files if f != primary_file]
        
        print(f"  ğŸ¯ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ: {Path(primary_file).name}")
        print(f"  ğŸ”— Ù…Ù„ÙØ§Øª Ù„Ù„Ø¯Ù…Ø¬: {[Path(f).name for f in other_files]}")
        
        try:
            # ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø­Ù„Ù„
            primary_parsed = content_map[primary_file]["parsed"]
            
            if primary_parsed is None:
                # Ø¥Ø°Ø§ Ù„Ù… ÙŠØªÙ… ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§Ù…
                merged_content = content_map[primary_file]["raw"]
                merged_parsed = None
            else:
                merged_parsed = primary_parsed.copy() if isinstance(primary_parsed, dict) else primary_parsed
                
                # Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ø£Ø®Ø±Ù‰
                for other_file in other_files:
                    other_parsed = content_map[other_file]["parsed"]
                    if other_parsed and isinstance(other_parsed, dict) and isinstance(merged_parsed, dict):
                        merged_parsed = self._merge_dict_configs(merged_parsed, other_parsed)
                
                # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¹ÙˆØ¯Ø© Ø¥Ù„Ù‰ Ù†Øµ
                if Path(primary_file).suffix.lower() == '.json':
                    merged_content = json.dumps(merged_parsed, indent=2, ensure_ascii=False)
                elif Path(primary_file).suffix.lower() in ['.yaml', '.yml']:
                    merged_content = yaml.dump(merged_parsed, default_flow_style=False, allow_unicode=True)
                else:
                    merged_content = content_map[primary_file]["raw"]
            
            return {
                "primary_file": primary_file,
                "merged_files": other_files,
                "merged_content": merged_content,
                "merged_parsed": merged_parsed
            }
        
        except Exception as e:
            print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø¯Ù…Ø¬: {e}")
            return {
                "primary_file": primary_file,
                "merged_files": other_files,
                "merged_content": content_map[primary_file]["raw"],
                "error": str(e)
            }

    def _merge_dict_configs(self, base_config: Dict, other_config: Dict) -> Dict:
        """Ø¯Ù…Ø¬ Ù‚ÙˆØ§Ù…ÙŠØ³ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø¨Ø°ÙƒØ§Ø¡"""
        merged = base_config.copy()
        
        for key, value in other_config.items():
            if key not in merged:
                # Ù…ÙØªØ§Ø­ Ø¬Ø¯ÙŠØ¯ - Ø¥Ø¶Ø§ÙØ© Ù…Ø¨Ø§Ø´Ø±Ø©
                merged[key] = value
            else:
                # Ù…ÙØªØ§Ø­ Ù…ÙˆØ¬ÙˆØ¯ - Ø¯Ù…Ø¬ Ø°ÙƒÙŠ
                if isinstance(merged[key], dict) and isinstance(value, dict):
                    # Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙˆØ§Ù…ÙŠØ³ Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø©
                    merged[key] = self._merge_dict_configs(merged[key], value)
                elif isinstance(merged[key], list) and isinstance(value, list):
                    # Ø¯Ù…Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù… Ù…Ø¹ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±
                    merged[key] = list(set(merged[key] + value))
                elif merged[key] != value:
                    # Ù‚ÙŠÙ… Ù…Ø®ØªÙ„ÙØ© - Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹
                    if len(str(value)) > len(str(merged[key])):
                        merged[key] = value
        
        return merged

    def find_config_references(self, config_file_path: str) -> List[Dict]:
        """Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø±Ø§Ø¬Ø¹ Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹"""
        print(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ø±Ø§Ø¬Ø¹ {Path(config_file_path).name}...")
        
        references = []
        config_name = Path(config_file_path).name
        config_stem = Path(config_file_path).stem
        
        # Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø­Ø«
        search_patterns = [
            config_name,                    # Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù ÙƒØ§Ù…Ù„
            config_stem,                    # Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù Ø¨Ø¯ÙˆÙ† Ø§Ù…ØªØ¯Ø§Ø¯
            f'"{config_name}"',            # Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ
            f"'{config_name}'",            # Ø¨ÙŠÙ† Ø¹Ù„Ø§Ù…Ø§Øª ØªÙ†ØµÙŠØµ Ù…ÙØ±Ø¯Ø©
            f"/{config_name}",             # ÙƒÙ…Ø³Ø§Ø±
            f"{config_stem}.",             # ÙƒÙ…Ø±Ø¬Ø¹ Python
        ]
        
        # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ù…Ù„ÙØ§Øª Python
        python_files = list(self.base_path.rglob("*.py"))
        
        for py_file in python_files:
            try:
                with open(py_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                for pattern in search_patterns:
                    if pattern in content:
                        # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø£Ø³Ø·Ø±
                        lines = content.split('\n')
                        line_numbers = []
                        
                        for i, line in enumerate(lines, 1):
                            if pattern in line:
                                line_numbers.append(i)
                        
                        if line_numbers:
                            references.append({
                                "file": str(py_file),
                                "pattern": pattern,
                                "lines": line_numbers,
                                "type": "python_import"
                            })
                            print(f"  ğŸ“ Ø¹Ø«Ø± ÙÙŠ {py_file.name} (Ø£Ø³Ø·Ø±: {line_numbers})")
            
            except Exception:
                continue
        
        return references

    def update_config_references(self, old_path: str, new_path: str, references: List[Dict]) -> List[Dict]:
        """ØªØ­Ø¯ÙŠØ« Ù…Ø±Ø§Ø¬Ø¹ Ù…Ù„Ù Ø§Ù„ØªÙƒÙˆÙŠÙ†"""
        if not references:
            return []
        
        print(f"ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ù…Ù† {Path(old_path).name} Ø¥Ù„Ù‰ {Path(new_path).name}...")
        
        updated_files = []
        old_name = Path(old_path).name
        new_name = Path(new_path).name
        
        for ref in references:
            try:
                file_path = Path(ref["file"])
                
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
                updated_content = content.replace(old_name, new_name)
                
                if updated_content != content:
                    # ÙƒØªØ§Ø¨Ø© Ø§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª
                    with open(file_path, 'w', encoding='utf-8') as f:
                        f.write(updated_content)
                    
                    updated_files.append({
                        "file": str(file_path),
                        "old_name": old_name,
                        "new_name": new_name,
                        "changes": len(ref["lines"])
                    })
                    
                    print(f"  âœ… ØªÙ… ØªØ­Ø¯ÙŠØ« {file_path.name}")
            
            except Exception as e:
                print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ø¯ÙŠØ« {ref['file']}: {e}")
        
        return updated_files

    def execute_intelligent_merge(self, analysis: Dict) -> Dict:
        """ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ Ù„Ù„Ù…Ù„ÙØ§Øª"""
        print("\nğŸš€ ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ...")
        
        results = {
            "identical_removed": 0,
            "similar_merged": 0,
            "references_updated": 0,
            "space_saved_kb": 0,
            "errors": []
        }
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù„Ø¯ Ù„Ù„Ù†Ø³Ø® Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©
        backup_dir = self.base_path / "deleted" / "config_backups"
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        try:
            # 1. Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø© 100%
            print("\nğŸ—‘ï¸ Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©...")
            
            for group in analysis["identical_groups"]:
                files = group["files"]
                primary_file = files[0]  # Ø§Ù„Ø§Ø­ØªÙØ§Ø¸ Ø¨Ø§Ù„Ø£ÙˆÙ„
                duplicate_files = files[1:]
                
                print(f"  ğŸ“ Ù…Ø¬Ù…ÙˆØ¹Ø©: Ø§Ø­ØªÙØ§Ø¸ Ø¨Ù€ {Path(primary_file).name}")
                
                for dup_file in duplicate_files:
                    try:
                        dup_path = Path(dup_file)
                        
                        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹
                        references = self.find_config_references(dup_file)
                        
                        # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ Ø¥Ù„Ù‰ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ
                        if references:
                            updated = self.update_config_references(dup_file, primary_file, references)
                            results["references_updated"] += len(updated)
                            self.config_analysis["updated_references"].extend(updated)
                        
                        # Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ
                        backup_path = backup_dir / f"identical_{dup_path.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                        dup_path.rename(backup_path)
                        
                        size_kb = backup_path.stat().st_size / 1024
                        results["space_saved_kb"] += size_kb
                        results["identical_removed"] += 1
                        
                        self.config_analysis["deleted_configs"].append({
                            "original": str(dup_path),
                            "backup": str(backup_path),
                            "reason": "identical_duplicate",
                            "size_kb": size_kb
                        })
                        
                        print(f"    ğŸ—‘ï¸ Ø­Ø°Ù: {dup_path.name} ({size_kb:.1f}KB)")
                    
                    except Exception as e:
                        error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ø­Ø°Ù {dup_file}: {str(e)}"
                        results["errors"].append(error_msg)
                        print(f"    âŒ {error_msg}")
            
            # 2. Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©
            print("\nğŸ”— Ø¯Ù…Ø¬ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©...")
            
            for group in analysis["similar_groups"]:
                try:
                    merge_result = self.merge_similar_configs(group, analysis["content_map"])
                    
                    if "error" not in merge_result:
                        primary_file = merge_result["primary_file"]
                        merged_files = merge_result["merged_files"]
                        merged_content = merge_result["merged_content"]
                        
                        # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬
                        with open(primary_file, 'w', encoding='utf-8') as f:
                            f.write(merged_content)
                        
                        print(f"  âœ… ØªÙ… Ø¯Ù…Ø¬ ÙÙŠ: {Path(primary_file).name}")
                        
                        # Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬Ø©
                        for merged_file in merged_files:
                            try:
                                merged_path = Path(merged_file)
                                
                                # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹ ÙˆØªØ­Ø¯ÙŠØ«Ù‡Ø§
                                references = self.find_config_references(merged_file)
                                if references:
                                    updated = self.update_config_references(merged_file, primary_file, references)
                                    results["references_updated"] += len(updated)
                                
                                # Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠ
                                backup_path = backup_dir / f"merged_{merged_path.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                                merged_path.rename(backup_path)
                                
                                size_kb = backup_path.stat().st_size / 1024
                                results["space_saved_kb"] += size_kb
                                
                                self.config_analysis["deleted_configs"].append({
                                    "original": str(merged_path),
                                    "backup": str(backup_path),
                                    "reason": "merged_into_primary",
                                    "primary_file": primary_file,
                                    "size_kb": size_kb
                                })
                                
                                print(f"    ğŸ—‘ï¸ Ø¯ÙÙ…Ø¬ ÙˆØ­ÙØ°Ù: {merged_path.name}")
                            
                            except Exception as e:
                                error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ø¯Ù…Ø¬ {merged_file}: {str(e)}"
                                results["errors"].append(error_msg)
                                print(f"    âŒ {error_msg}")
                        
                        results["similar_merged"] += len(merged_files)
                        
                        self.config_analysis["merged_configs"].append({
                            "primary_file": primary_file,
                            "merged_files": merged_files,
                            "similarity": group["average_similarity"]
                        })
                    
                    else:
                        print(f"  âŒ ÙØ´Ù„ Ø¯Ù…Ø¬ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {merge_result['error']}")
                
                except Exception as e:
                    error_msg = f"Ø®Ø·Ø£ ÙÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ØªØ´Ø§Ø¨Ù‡Ø©: {str(e)}"
                    results["errors"].append(error_msg)
                    print(f"  âŒ {error_msg}")
        
        except Exception as e:
            error_msg = f"Ø®Ø·Ø£ Ø¹Ø§Ù… ÙÙŠ Ø§Ù„Ø¯Ù…Ø¬: {str(e)}"
            results["errors"].append(error_msg)
            print(f"âŒ {error_msg}")
        
        return results

    def generate_merge_report(self, analysis: Dict, results: Dict) -> str:
        """Ø¥Ù†Ø´Ø§Ø¡ ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ"""
        timestamp = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        
        report = f"""
# ğŸ§¹ ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ

**Ø§Ù„ØªØ§Ø±ÙŠØ®**: {timestamp}  
**Ø§Ù„Ø£Ø¯Ø§Ø©**: IntelligentConfigMerger v1.0

## ğŸ“Š Ù…Ù„Ø®Øµ Ø§Ù„Ø¹Ù…Ù„ÙŠØ©

### ğŸ¯ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠØ©:
- **Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†**: {self.config_analysis['total_configs']}
- **Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ù…Ø­Ø°ÙˆÙØ©**: {results['identical_removed']}
- **Ù…Ù„ÙØ§Øª Ù…Ø¯Ù…ÙˆØ¬Ø©**: {results['similar_merged']}
- **Ù…Ø±Ø§Ø¬Ø¹ Ù…Ø­Ø¯Ø«Ø©**: {results['references_updated']}
- **Ù…Ø³Ø§Ø­Ø© Ù…ÙˆÙØ±Ø©**: {results['space_saved_kb']:.1f} KB
- **Ø£Ø®Ø·Ø§Ø¡**: {len(results['errors'])}

---

## ğŸ”„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª Ø§Ù„Ù…ØªØ·Ø§Ø¨Ù‚Ø©

### âœ… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©:
"""
        
        if analysis["identical_groups"]:
            for i, group in enumerate(analysis["identical_groups"], 1):
                files = group["files"]
                primary = Path(files[0]).name
                duplicates = [Path(f).name for f in files[1:]]
                
                report += f"""
#### Ù…Ø¬Ù…ÙˆØ¹Ø© {i}:
- **Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**: `{primary}`
- **Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©**: {len(duplicates)} Ù…Ù„Ù
- **Ø§Ù„Ø­Ø¬Ù…**: {group['size']} Ø¨Ø§ÙŠØª
- **Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©**: {', '.join(duplicates[:3])}{"..." if len(duplicates) > 3 else ""}
"""
        
        report += f"""

---

## ğŸ”— Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬Ø©

### âœ… Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ù…Ø¯Ù…ÙˆØ¬Ø©:
"""
        
        if analysis["similar_groups"]:
            for i, group in enumerate(analysis["similar_groups"], 1):
                files = [Path(f).name for f in group["files"]]
                primary = files[0]
                others = files[1:]
                
                report += f"""
#### Ù…Ø¬Ù…ÙˆØ¹Ø© {i}:
- **Ø§Ù„ØªØ´Ø§Ø¨Ù‡**: {group['average_similarity']:.1f}%
- **Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ**: `{primary}`
- **Ù…Ù„ÙØ§Øª Ù…Ø¯Ù…ÙˆØ¬Ø©**: {', '.join(others[:3])}{"..." if len(others) > 3 else ""}
"""
        
        report += f"""

---

## ğŸ”„ ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹

### âœ… Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø¯Ø«Ø©:
"""
        
        if self.config_analysis["updated_references"]:
            for update in self.config_analysis["updated_references"][:10]:  # Ø£ÙˆÙ„ 10
                report += f"""
- **Ø§Ù„Ù…Ù„Ù**: `{Path(update['file']).name}`
- **Ù…Ù†**: `{update['old_name']}`  
- **Ø¥Ù„Ù‰**: `{update['new_name']}`
- **ØªØºÙŠÙŠØ±Ø§Øª**: {update['changes']} Ù…ÙƒØ§Ù†
"""
            
            if len(self.config_analysis["updated_references"]) > 10:
                remaining = len(self.config_analysis["updated_references"]) - 10
                report += f"\n... Ùˆ{remaining} Ù…Ù„Ù Ø£Ø®Ø±Ù‰\n"
        
        report += f"""

---

## ğŸ“ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø­Ø°ÙˆÙØ©

### ğŸ—‘ï¸ Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©:
"""
        
        if self.config_analysis["deleted_configs"]:
            for delete in self.config_analysis["deleted_configs"][:15]:  # Ø£ÙˆÙ„ 15
                original_name = Path(delete['original']).name
                reason_text = {
                    "identical_duplicate": "ØªÙƒØ±Ø§Ø± Ù…ØªØ·Ø§Ø¨Ù‚",
                    "merged_into_primary": "Ø¯ÙÙ…Ø¬ ÙÙŠ Ù…Ù„Ù Ø£Ø³Ø§Ø³ÙŠ"
                }.get(delete['reason'], delete['reason'])
                
                report += f"""
- **Ø§Ù„Ù…Ù„Ù**: `{original_name}`
- **Ø§Ù„Ø³Ø¨Ø¨**: {reason_text}
- **Ø§Ù„Ø­Ø¬Ù…**: {delete['size_kb']:.1f} KB
- **Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠØ©**: `{Path(delete['backup']).name}`
"""
            
            if len(self.config_analysis["deleted_configs"]) > 15:
                remaining = len(self.config_analysis["deleted_configs"]) - 15
                report += f"\n... Ùˆ{remaining} Ù…Ù„Ù Ø£Ø®Ø±Ù‰\n"
        
        # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ Ø¥Ù† ÙˆØ¬Ø¯Øª
        if results["errors"]:
            report += f"""

---

## âš ï¸ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆØ§Ù„ØªØ­Ø°ÙŠØ±Ø§Øª

### ğŸš¨ Ù…Ø´Ø§ÙƒÙ„ ÙˆØ§Ø¬Ù‡ØªÙ‡Ø§:
"""
            for error in results["errors"][:10]:  # Ø£ÙˆÙ„ 10 Ø£Ø®Ø·Ø§Ø¡
                report += f"- âŒ {error}\n"
        
        report += f"""

---

## ğŸ¯ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ø­Ù‚Ù‚Ø©

### âœ… Ø§Ù„ØªØ­Ø³ÙŠÙ†Ø§Øª:
1. **ØªÙ†Ø¸ÙŠÙ Ø´Ø§Ù…Ù„** - Ø¥Ø²Ø§Ù„Ø© {results['identical_removed']} Ù…Ù„Ù Ù…ÙƒØ±Ø±
2. **Ø¯Ù…Ø¬ Ø°ÙƒÙŠ** - Ø¯Ù…Ø¬ {results['similar_merged']} Ù…Ù„Ù Ù…ØªØ´Ø§Ø¨Ù‡
3. **ØªÙˆÙÙŠØ± Ù…Ø³Ø§Ø­Ø©** - {results['space_saved_kb']:.1f} KB ØªÙ… ØªÙˆÙÙŠØ±Ù‡Ø§
4. **ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹** - {results['references_updated']} Ù…Ø±Ø¬Ø¹ ØªÙ… ØªØ­Ø¯ÙŠØ«Ù‡
5. **Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© Ø¢Ù…Ù†Ø©** - Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ù…Ø­ÙÙˆØ¸Ø© ÙÙŠ `deleted/config_backups/`

### ğŸ“ˆ Ø§Ù„ÙÙˆØ§Ø¦Ø¯:
- **Ù…Ø³Ø§Ø­Ø© Ø£Ù‚Ù„** - ØªÙ‚Ù„ÙŠÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù‚Ø±Øµ Ø§Ù„ØµÙ„Ø¨
- **Ø¥Ø¯Ø§Ø±Ø© Ø£Ø³Ù‡Ù„** - Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ† Ø£Ù‚Ù„ ÙˆØ£ÙƒØ«Ø± ØªÙ†Ø¸ÙŠÙ…Ø§Ù‹
- **Ø£Ø¯Ø§Ø¡ Ù…Ø­Ø³Ù†** - ØªØ­Ù…ÙŠÙ„ Ø£Ø³Ø±Ø¹ Ù„Ù„ØªÙƒÙˆÙŠÙ†
- **ØµÙŠØ§Ù†Ø© Ø£Ø³Ù‡Ù„** - ØªØ­Ø¯ÙŠØ« ÙˆØ§Ø­Ø¯ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø¹Ø¯Ø© Ù…Ù„ÙØ§Øª

### ğŸ”’ Ø§Ù„Ø£Ù…Ø§Ù†:
- **Ù†Ø³Ø® Ø§Ø­ØªÙŠØ§Ø·ÙŠØ© ÙƒØ§Ù…Ù„Ø©** - ÙŠÙ…ÙƒÙ† Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø£ÙŠ Ù…Ù„Ù
- **ØªØªØ¨Ø¹ Ø§Ù„ØªØºÙŠÙŠØ±Ø§Øª** - Ø³Ø¬Ù„ ÙƒØ§Ù…Ù„ Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª
- **Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø±Ø§Ø¬Ø¹** - ØªØ­Ø¯ÙŠØ« Ø¢Ù…Ù† Ù„Ù„Ø±ÙˆØ§Ø¨Ø·

---

**ØªÙ… Ø¥Ù†Ø´Ø§Ø¤Ù‡ Ø¨ÙˆØ§Ø³Ø·Ø©**: IntelligentConfigMerger v1.0  
**Ø§Ù„ØªÙˆÙ‚ÙŠØª**: {timestamp}
"""
        
        return report

    def run_intelligent_config_cleanup(self) -> Dict:
        """ØªØ´ØºÙŠÙ„ ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ø°ÙƒÙŠ"""
        print("=" * 60)
        print("ğŸ§¹  INTELLIGENT CONFIG MERGER")
        print("ğŸ”  SMART DUPLICATION ANALYSIS & MERGING")
        print("=" * 60)
        
        # Ø§ÙƒØªØ´Ø§Ù Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†
        config_files = self.discover_all_config_files()
        
        if self.config_analysis["total_configs"] == 0:
            print("âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ†!")
            return {}
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø±Ø§Øª ÙˆØ§Ù„ØªØ´Ø§Ø¨Ù‡Ø§Øª
        analysis = self.analyze_config_duplicates(config_files)
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        identical_count = sum(len(group["files"]) - 1 for group in analysis["identical_groups"])
        similar_count = sum(len(group["files"]) - 1 for group in analysis["similar_groups"])
        
        print(f"\nğŸ“Š Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        print(f"  ğŸ”„ Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© Ù„Ù„Ø­Ø°Ù: {identical_count}")
        print(f"  ğŸ”— Ù…Ù„ÙØ§Øª Ù…ØªØ´Ø§Ø¨Ù‡Ø© Ù„Ù„Ø¯Ù…Ø¬: {similar_count}")
        print(f"  ğŸ’¾ Ù…Ø³Ø§Ø­Ø© Ù…ØªÙˆÙ‚Ø¹Ø© Ù„Ù„ØªÙˆÙÙŠØ±: {sum(group['size'] for group in analysis['identical_groups']) / 1024:.1f} KB")
        
        if identical_count == 0 and similar_count == 0:
            print("âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ ØªÙƒØ±Ø§Ø±Ø§Øª Ø£Ùˆ ØªØ´Ø§Ø¨Ù‡Ø§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©!")
            return analysis
        
        # ØªØ£ÙƒÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
        print(f"\nâš ï¸  Ø³ÙŠØªÙ… Ø­Ø°Ù/Ø¯Ù…Ø¬ {identical_count + similar_count} Ù…Ù„Ù")
        print("ğŸ“ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª Ø³ØªÙØ­ÙØ¸ ÙÙŠ deleted/config_backups/")
        
        # ØªÙ†ÙÙŠØ° Ø§Ù„Ø¯Ù…Ø¬ Ø§Ù„Ø°ÙƒÙŠ
        results = self.execute_intelligent_merge(analysis)
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_content = self.generate_merge_report(analysis, results)
        report_path = self.base_path / "deleted" / "reports" / "INTELLIGENT_CONFIG_CLEANUP.md"
        report_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"\nğŸ‰ ØªÙ… Ø¥ÙƒÙ…Ø§Ù„ ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†!")
        print(f"ğŸ“‹ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø´Ø§Ù…Ù„: {report_path}")
        print(f"ğŸ—‘ï¸ Ù…Ù„ÙØ§Øª Ù…Ø­Ø°ÙˆÙØ©: {results['identical_removed']}")
        print(f"ğŸ”— Ù…Ù„ÙØ§Øª Ù…Ø¯Ù…ÙˆØ¬Ø©: {results['similar_merged']}")
        print(f"ğŸ’¾ Ù…Ø³Ø§Ø­Ø© Ù…ÙˆÙØ±Ø©: {results['space_saved_kb']:.1f} KB")
        print(f"ğŸ”„ Ù…Ø±Ø§Ø¬Ø¹ Ù…Ø­Ø¯Ø«Ø©: {results['references_updated']}")
        
        return {
            "analysis": analysis,
            "results": results,
            "config_analysis": self.config_analysis
        }

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©"""
    merger = IntelligentConfigMerger()
    
    try:
        results = merger.run_intelligent_config_cleanup()
        
        if results:
            print(f"\nâœ… ØªÙ… Ø§Ù„ØªÙ†Ø¸ÙŠÙ Ø¨Ù†Ø¬Ø§Ø­!")
        else:
            print(f"\nâš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù„ÙØ§Øª Ù„Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©!")
            
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ†: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 