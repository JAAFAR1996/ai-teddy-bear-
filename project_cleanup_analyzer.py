#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Project Cleanup Analyzer - ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù„Ù„Ù…Ø´Ø±ÙˆØ¹ Ù„ØªÙ†Ø¸ÙŠÙÙ‡
"""

import os
import hashlib
import re
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime

def analyze_project():
    """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„"""
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...")
    
    stats = {
        "total_files": 0,
        "empty_files": [],
        "large_files": [],
        "duplicate_files": [],
        "file_types": defaultdict(int),
        "trash_files": [],
        "misplaced_files": []
    }
    
    # Ø¬Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
    all_files = []
    for root, dirs, files in os.walk('.'):
        # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.venv', 'venv']]
        
        for file in files:
            if not file.endswith(('.pyc', '.pyo')):
                file_path = os.path.join(root, file)
                all_files.append(file_path)
    
    print(f"ğŸ“„ ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(all_files)} Ù…Ù„Ù")
    
    # ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ù„Ù
    file_hashes = defaultdict(list)
    
    for file_path in all_files:
        stats["total_files"] += 1
        
        # Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
        ext = os.path.splitext(file_path)[1]
        stats["file_types"][ext] += 1
        
        try:
            # Ø­Ø¬Ù… Ø§Ù„Ù…Ù„Ù
            size = os.path.getsize(file_path)
            
            # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©
            if size == 0:
                stats["empty_files"].append(file_path)
                continue
            
            # Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù†ØµÙŠØ©
            if ext in ['.py', '.js', '.json', '.txt', '.md', '.yml', '.yaml']:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø³Ø·Ø±
                lines = len(content.splitlines())
                
                # Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©
                if lines > 500:
                    stats["large_files"].append({
                        "path": file_path,
                        "lines": lines
                    })
                
                # Ø­Ø³Ø§Ø¨ hash Ù„Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…ÙƒØ±Ø±Ø§Øª
                file_hash = hashlib.md5(content.encode()).hexdigest()
                file_hashes[file_hash].append(file_path)
                
                # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø©
                if any(pattern in file_path.lower() for pattern in ['_old', '_backup', '_temp', '_copy']):
                    stats["trash_files"].append(file_path)
                
                # ÙØ­Øµ Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ø®Ø§Ø·Ø¦Ø©
                if ext == '.py':
                    check_file_location(file_path, stats)
                    
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªØ­Ù„ÙŠÙ„ {file_path}: {e}")
    
    # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
    for file_hash, files in file_hashes.items():
        if len(files) > 1:
            stats["duplicate_files"].append({
                "files": files,
                "count": len(files)
            })
    
    return stats

def check_file_location(file_path, stats):
    """ÙØ­Øµ Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ù…Ù„Ù"""
    path_lower = file_path.lower()
    
    # ØªØ­Ø¯ÙŠØ¯ Ù†ÙˆØ¹ Ø§Ù„Ù…Ù„Ù
    if 'model' in path_lower or 'entity' in path_lower:
        if 'domain' not in path_lower:
            stats["misplaced_files"].append({
                "file": file_path,
                "type": "model",
                "suggested": "src/core/domain/entities/"
            })
    elif 'service' in path_lower:
        if 'application' not in path_lower and 'core' not in path_lower:
            stats["misplaced_files"].append({
                "file": file_path,
                "type": "service",
                "suggested": "src/core/services/"
            })
    elif 'repository' in path_lower:
        if 'infrastructure' not in path_lower:
            stats["misplaced_files"].append({
                "file": file_path,
                "type": "repository",
                "suggested": "src/infrastructure/persistence/"
            })

def print_report(stats):
    """Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
    print("\n" + "="*60)
    print("ğŸ“Š ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹")
    print("="*60)
    
    print(f"\nğŸ“ˆ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©:")
    print(f"- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats['total_files']}")
    print(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©: {len(stats['empty_files'])}")
    print(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙƒØ¨ÙŠØ±Ø©: {len(stats['large_files'])}")
    print(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {len(stats['duplicate_files'])}")
    print(f"- Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø©: {len(stats['trash_files'])}")
    print(f"- Ù…Ù„ÙØ§Øª ÙÙŠ Ø£Ù…Ø§ÙƒÙ† Ø®Ø§Ø·Ø¦Ø©: {len(stats['misplaced_files'])}")
    
    print("\nğŸ“‚ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª:")
    for ext, count in sorted(stats['file_types'].items(), key=lambda x: x[1], reverse=True)[:10]:
        print(f"  {ext or 'Ø¨Ø¯ÙˆÙ† Ø§Ù…ØªØ¯Ø§Ø¯'}: {count}")
    
    if stats['empty_files']:
        print(f"\nğŸ—‘ï¸ Ù…Ù„ÙØ§Øª ÙØ§Ø±ØºØ© (Ø£ÙˆÙ„ 10):")
        for file in stats['empty_files'][:10]:
            print(f"  - {file}")
    
    if stats['duplicate_files']:
        print(f"\nğŸ”„ Ù…Ù„ÙØ§Øª Ù…ÙƒØ±Ø±Ø© (Ø£ÙˆÙ„ 5 Ù…Ø¬Ù…ÙˆØ¹Ø§Øª):")
        for i, dup in enumerate(stats['duplicate_files'][:5]):
            print(f"\n  Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© {i+1} ({dup['count']} Ù…Ù„Ù):")
            for file in dup['files'][:3]:
                print(f"    - {file}")
    
    if stats['trash_files']:
        print(f"\nğŸš® Ù…Ù„ÙØ§Øª Ù‚Ù…Ø§Ù…Ø© (Ø£ÙˆÙ„ 10):")
        for file in stats['trash_files'][:10]:
            print(f"  - {file}")
    
    print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
    print(f"1. Ø§Ø­Ø°Ù {len(stats['empty_files'])} Ù…Ù„Ù ÙØ§Ø±Øº")
    print(f"2. Ø±Ø§Ø¬Ø¹ ÙˆØ¯Ù…Ø¬ {len(stats['duplicate_files'])} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©")
    print(f"3. Ø§Ø­Ø°Ù {len(stats['trash_files'])} Ù…Ù„Ù Ù‚Ù…Ø§Ù…Ø©")
    print(f"4. Ø§Ù†Ù‚Ù„ {len(stats['misplaced_files'])} Ù…Ù„Ù Ù„Ù„Ù…ÙƒØ§Ù† Ø§Ù„ØµØ­ÙŠØ­")

def save_report(stats):
    """Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # Ø­ÙØ¸ JSON
    with open(f"cleanup_report_{timestamp}.json", 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)
    
    # Ø­ÙØ¸ Markdown
    with open(f"cleanup_report_{timestamp}.md", 'w', encoding='utf-8') as f:
        f.write("# ØªÙ‚Ø±ÙŠØ± ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹\n\n")
        f.write(f"Ø§Ù„ØªØ§Ø±ÙŠØ®: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        
        f.write("## Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª\n")
        f.write(f"- Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {stats['total_files']}\n")
        f.write(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©: {len(stats['empty_files'])}\n")
        f.write(f"- Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©: {len(stats['duplicate_files'])}\n")
        f.write(f"- Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø©: {len(stats['trash_files'])}\n\n")
        
        f.write("## Ø®Ø·Ø© Ø§Ù„Ø¹Ù…Ù„\n\n")
        f.write("### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ù‡Ù…Ø©\n")
        f.write("```bash\n")
        f.write("# Ø­Ø°Ù Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„ÙØ§Ø±ØºØ©\n")
        for file in stats['empty_files'][:20]:
            f.write(f'rm "{file}"\n')
        f.write("```\n\n")
        
        f.write("### Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø­Ø°Ù Ù…Ù„ÙØ§Øª Ø§Ù„Ù‚Ù…Ø§Ù…Ø©\n")
        f.write("```bash\n")
        for file in stats['trash_files'][:20]:
            f.write(f'rm "{file}"\n')
        f.write("```\n")
    
    print(f"\nâœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ:")
    print(f"  - cleanup_report_{timestamp}.json")
    print(f"  - cleanup_report_{timestamp}.md")

def main():
    """Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ"""
    try:
        stats = analyze_project()
        print_report(stats)
        save_report(stats)
    except Exception as e:
        print(f"\nâŒ Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 