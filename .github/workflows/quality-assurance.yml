name: 🚀 Quality Assurance Pipeline - AI Teddy Bear

on:
  push:
    branches: [ main, develop, feature/* ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run nightly tests at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  # 📊 Code Quality Analysis
  code-quality:
    name: 🔍 Code Quality & Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better analysis
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: 🎯 Run MyPy Type Checking
      run: |
        mypy core/ --config-file=mypy.ini --html-report=reports/mypy
    
    - name: 🧹 Run Black Code Formatting Check
      run: |
        black --check --diff core/ tests/
    
    - name: 📏 Run Flake8 Linting
      run: |
        flake8 core/ tests/ --output-file=reports/flake8.txt
    
    - name: 🔒 Run Bandit Security Scan
      run: |
        bandit -r core/ -f json -o reports/bandit.json
    
    - name: 📈 Run Complexity Analysis
      run: |
        radon cc core/ --json > reports/complexity.json
        radon mi core/ --json > reports/maintainability.json
    
    - name: 📊 Upload Quality Reports
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: quality-reports
        path: reports/
        retention-days: 30

  # 🧪 Unit Tests - Multi-OS & Multi-Python
  unit-tests:
    name: 🧪 Unit Tests (${{ matrix.os }}, Python ${{ matrix.python }})
    runs-on: ${{ matrix.os }}
    
    strategy:
      fail-fast: false
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python: ['3.9', '3.10', '3.11', '3.12']
        exclude:
          # Reduce matrix for faster builds
          - os: macos-latest
            python: '3.9'
          - os: windows-latest
            python: '3.12'
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python ${{ matrix.python }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python }}
        cache: 'pip'
    
    - name: 🖥️ Setup Virtual Display (Linux)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1280x1024x24 &
    
    - name: 📦 Install System Dependencies (Linux)
      if: runner.os == 'Linux'
      run: |
        sudo apt-get install -y libasound2-dev portaudio19-dev
    
    - name: 📦 Install Python Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: 🧪 Run Unit Tests with Coverage
      env:
        QT_QPA_PLATFORM: offscreen
        DISPLAY: ':99'
      run: |
        pytest tests/unit/ \
          --cov=core \
          --cov-report=xml \
          --cov-report=html \
          --cov-report=term \
          --junit-xml=reports/junit.xml \
          --html=reports/pytest-report.html \
          --self-contained-html \
          -v
    
    - name: 📊 Upload Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: test-results-${{ matrix.os }}-py${{ matrix.python }}
        path: |
          reports/
          htmlcov/
        retention-days: 30
    
    - name: 📈 Upload Coverage to Codecov
      uses: codecov/codecov-action@v3
      if: matrix.os == 'ubuntu-latest' && matrix.python == '3.11'
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # 🔗 Integration Tests
  integration-tests:
    name: 🔗 Integration Tests
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    services:
      # Mock WebSocket server for testing
      websocket-server:
        image: nginx:alpine
        ports:
          - 8080:80
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 🖥️ Setup Virtual Display
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb pulseaudio
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1280x1024x24 &
        pulseaudio --start
    
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: 🔗 Run Integration Tests
      env:
        QT_QPA_PLATFORM: offscreen
        DISPLAY: ':99'
        WEBSOCKET_TEST_URL: 'ws://localhost:8080/ws'
      run: |
        pytest tests/integration/ \
          --cov=core \
          --cov-append \
          --junit-xml=reports/integration-junit.xml \
          --html=reports/integration-report.html \
          --self-contained-html \
          -v --tb=short
    
    - name: 📊 Upload Integration Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: integration-test-results
        path: reports/
        retention-days: 30

  # 🎭 End-to-End Tests
  e2e-tests:
    name: 🎭 End-to-End Tests
    runs-on: ubuntu-latest
    needs: [integration-tests]
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 🌐 Setup Node.js for Playwright
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'
        cache-dependency-path: tests/e2e/package-lock.json
    
    - name: 🖥️ Setup Virtual Display
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1920x1080x24 &
    
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        cd tests/e2e && npm ci
        npx playwright install
    
    - name: 🎭 Run E2E Tests
      env:
        QT_QPA_PLATFORM: offscreen
        DISPLAY: ':99'
      run: |
        pytest tests/e2e/ \
          --junit-xml=reports/e2e-junit.xml \
          --html=reports/e2e-report.html \
          --self-contained-html \
          -v --tb=short
    
    - name: 📊 Upload E2E Test Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: e2e-test-results
        path: |
          reports/
          tests/e2e/test-results/
        retention-days: 30

  # 📦 Build & Package
  build-package:
    name: 📦 Build & Package
    runs-on: ${{ matrix.os }}
    needs: [code-quality, unit-tests]
    
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Build Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install build wheel setuptools
        pip install -r requirements.txt
    
    - name: 🔨 Build Package
      run: |
        python -m build
    
    - name: 📊 Upload Build Artifacts
      uses: actions/upload-artifact@v4
      with:
        name: package-${{ matrix.os }}
        path: |
          dist/
          build/
        retention-days: 7

  # 🚀 Performance Tests
  performance-tests:
    name: 🚀 Performance Benchmarks
    runs-on: ubuntu-latest
    needs: [unit-tests]
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 🖥️ Setup Virtual Display
      run: |
        sudo apt-get update
        sudo apt-get install -y xvfb
        export DISPLAY=:99
        Xvfb :99 -ac -screen 0 1280x1024x24 &
    
    - name: 📦 Install Dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
        pip install pytest-benchmark
    
    - name: 🚀 Run Performance Tests
      env:
        QT_QPA_PLATFORM: offscreen
        DISPLAY: ':99'
      run: |
        pytest tests/performance/ \
          --benchmark-json=reports/benchmark.json \
          --benchmark-html=reports/benchmark.html \
          -v
    
    - name: 📊 Upload Performance Results
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: performance-results
        path: reports/
        retention-days: 30

  # 📋 Generate Final Report
  final-report:
    name: 📋 Generate Quality Report
    runs-on: ubuntu-latest
    needs: [code-quality, unit-tests, integration-tests, e2e-tests, performance-tests]
    if: always()
    
    steps:
    - name: 📥 Checkout Code
      uses: actions/checkout@v4
    
    - name: 📥 Download All Artifacts
      uses: actions/download-artifact@v4
      with:
        path: artifacts/
    
    - name: 🐍 Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
        cache: 'pip'
    
    - name: 📦 Install Report Dependencies
      run: |
        pip install jinja2 markdown
    
    - name: 📋 Generate Comprehensive Report
      run: |
        python scripts/generate_qa_report.py \
          --artifacts-dir artifacts/ \
          --output reports/qa-summary.html
    
    - name: 📊 Upload Final Report
      uses: actions/upload-artifact@v4
      with:
        name: qa-comprehensive-report
        path: reports/
        retention-days: 90
    
    - name: 💬 Comment PR with Results
      if: github.event_name == 'pull_request'
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = 'reports/qa-summary.md';
          
          if (fs.existsSync(path)) {
            const summary = fs.readFileSync(path, 'utf8');
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: `## 📊 Quality Assurance Report\n\n${summary}`
            });
          }

  # 🏆 Success Notification
  success-notification:
    name: 🏆 Success Notification
    runs-on: ubuntu-latest
    needs: [final-report]
    if: success()
    
    steps:
    - name: 🎉 All Tests Passed
      run: |
        echo "🎉 All quality checks passed successfully!"
        echo "✅ Code Quality: PASSED"
        echo "✅ Unit Tests: PASSED" 
        echo "✅ Integration Tests: PASSED"
        echo "✅ E2E Tests: PASSED"
        echo "✅ Performance Tests: PASSED"
        echo "🚀 Ready for deployment!" 