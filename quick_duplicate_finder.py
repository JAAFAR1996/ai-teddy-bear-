#!/usr/bin/env python3
"""
Quick Duplicate Finder for AI Teddy Bear Project
Ù…Ø­Ù„Ù„ Ø³Ø±ÙŠØ¹ Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
"""

import os
import ast
import hashlib
from pathlib import Path
from collections import defaultdict
import json

class QuickDuplicateFinder:
    def __init__(self):
        self.files_info = {}
        self.duplicates = []
        self.service_duplicates = {}
        
    def scan_project(self):
        print('ğŸ” Ù…Ø³Ø­ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹...')
        for file_path in Path('.').rglob('*.py'):
            if any(skip in str(file_path) for skip in ['.git', '__pycache__', 'venv', '.venv']):
                continue
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                if len(content.strip()) > 0:
                    self.files_info[str(file_path)] = {
                        'content': content,
                        'hash': hashlib.md5(content.encode()).hexdigest(),
                        'size': len(content),
                        'lines': len(content.splitlines()),
                        'classes': self.extract_classes(content),
                        'functions': self.extract_functions(content)
                    }
            except:
                pass
        print(f'âœ… ØªÙ… Ù…Ø³Ø­ {len(self.files_info)} Ù…Ù„Ù')
        
    def extract_classes(self, content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª"""
        try:
            tree = ast.parse(content)
            classes = []
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    classes.append(node.name)
            return classes
        except:
            return []
    
    def extract_functions(self, content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£Ø³Ù…Ø§Ø¡ Ø§Ù„Ø¯ÙˆØ§Ù„"""
        try:
            tree = ast.parse(content)
            functions = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    functions.append(node.name)
            return functions
        except:
            return []
        
    def find_exact_duplicates(self):
        print('ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹...')
        hash_groups = defaultdict(list)
        
        for file_path, info in self.files_info.items():
            hash_groups[info['hash']].append((file_path, info))
        
        for file_hash, files in hash_groups.items():
            if len(files) > 1:
                self.duplicates.append(files)
        
        print(f'âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ {len(self.duplicates)} Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø©')
        
    def find_service_duplicates(self):
        print('ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©...')
        service_patterns = {
            'ai_service': [],
            'audio_service': [],
            'emotion_service': [],
            'transcription_service': [],
            'cache_service': [],
            'moderation_service': [],
            'synthesis_service': [],
            'streaming_service': [],
            'memory_service': [],
            'database_service': []
        }
        
        for file_path in self.files_info.keys():
            filename = Path(file_path).name.lower()
            for pattern in service_patterns.keys():
                pattern_words = pattern.split('_')
                if any(word in filename for word in pattern_words):
                    service_patterns[pattern].append(file_path)
        
        self.service_duplicates = {k: v for k, v in service_patterns.items() if len(v) > 1}
        
        if self.service_duplicates:
            print('ğŸ› ï¸ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©:')
            for service_type, files in self.service_duplicates.items():
                print(f'  - {service_type}: {len(files)} Ù…Ù„Ù')
                for file_path in files:
                    print(f'    â€¢ {file_path}')
        else:
            print('âœ… Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø®Ø¯Ù…Ø§Øª Ù…ÙƒØ±Ø±Ø©')
    
    def find_similar_classes(self):
        print('ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ù…ØªØ´Ø§Ø¨Ù‡Ø©...')
        class_groups = defaultdict(list)
        
        for file_path, info in self.files_info.items():
            for class_name in info['classes']:
                class_groups[class_name].append(file_path)
        
        duplicate_classes = {k: v for k, v in class_groups.items() if len(v) > 1}
        
        if duplicate_classes:
            print('ğŸ—ï¸ Ø§Ù„ÙƒÙ„Ø§Ø³Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©:')
            for class_name, files in duplicate_classes.items():
                print(f'  - {class_name}: {len(files)} Ù…Ù„Ù')
                for file_path in files:
                    print(f'    â€¢ {file_path}')
        else:
            print('âœ… Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒÙ„Ø§Ø³Ø§Øª Ù…ÙƒØ±Ø±Ø©')
            
        return duplicate_classes
    
    def find_config_duplicates(self):
        print('ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙƒØ±Ø±Ø©...')
        config_files = []
        
        for file_path in self.files_info.keys():
            if 'config' in Path(file_path).name.lower():
                config_files.append(file_path)
        
        if len(config_files) > 3:  # Ø£ÙƒØ«Ø± Ù…Ù† 3 Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ†
            print(f'âš ï¸ ÙŠÙˆØ¬Ø¯ {len(config_files)} Ù…Ù„Ù ØªÙƒÙˆÙŠÙ†:')
            for file_path in config_files:
                print(f'  â€¢ {file_path}')
        else:
            print('âœ… Ø¹Ø¯Ø¯ Ù…Ù„ÙØ§Øª Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø·Ø¨ÙŠØ¹ÙŠ')
        
        return config_files
        
    def generate_report(self):
        print('ğŸ“Š Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±...')
        
        total_wasted_space = 0
        
        print('=' * 80)
        print('ğŸ” ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø© Ø§Ù„Ø´Ø§Ù…Ù„')
        print('=' * 80)
        print(f'ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ù„ÙØ§Øª: {len(self.files_info)}')
        print(f'ğŸ”„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…ÙƒØ±Ø±Ø©: {len(self.duplicates)}')
        print(f'ğŸ› ï¸ Ø£Ù†ÙˆØ§Ø¹ Ø®Ø¯Ù…Ø§Øª Ù…ÙƒØ±Ø±Ø©: {len(self.service_duplicates)}')
        
        if self.duplicates:
            print('\nğŸ”„ Ø§Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹:')
            for i, group in enumerate(self.duplicates, 1):
                print(f'\n{i}. Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…ÙƒØ±Ø±Ø© ({len(group)} Ù…Ù„ÙØ§Øª):')
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©
                file_size = group[0][1]['size']
                wasted_space = file_size * (len(group) - 1)
                total_wasted_space += wasted_space
                
                print(f'   Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©: {wasted_space} Ø¨Ø§ÙŠØª')
                
                for file_path, info in group:
                    print(f'   â€¢ {file_path} ({info["lines"]} Ø³Ø·Ø±)')
        
        if self.service_duplicates:
            print('\nğŸ› ï¸ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©:')
            for service_type, files in self.service_duplicates.items():
                print(f'\n{service_type.upper()}:')
                for file_path in files:
                    print(f'  â€¢ {file_path}')
        
        print(f'\nğŸ’¾ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ù…Ù‡Ø¯Ø±Ø©: {total_wasted_space} Ø¨Ø§ÙŠØª')
        print('=' * 80)
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ Ù…Ù„Ù JSON
        report_data = {
            'total_files': len(self.files_info),
            'exact_duplicates': len(self.duplicates),
            'service_duplicates': len(self.service_duplicates),
            'wasted_space': total_wasted_space,
            'duplicate_groups': [
                {
                    'files': [f[0] for f in group],
                    'size': group[0][1]['size'],
                    'lines': group[0][1]['lines']
                }
                for group in self.duplicates
            ],
            'service_duplicates_detail': self.service_duplicates
        }
        
        with open('duplicate_report.json', 'w', encoding='utf-8') as f:
            json.dump(report_data, f, indent=2, ensure_ascii=False)
        
        print('âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ± ÙÙŠ: duplicate_report.json')
        
    def create_cleanup_suggestions(self):
        print('ğŸ’¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ...')
        
        suggestions = []
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ù…Ù„ÙØ§Øª Ø§Ù„Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹
        for group in self.duplicates:
            files = [f[0] for f in group]
            # Ø§Ø®ØªØ± Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ÙØ¶Ù„ (ÙÙŠ src Ø¹Ø§Ø¯Ø©)
            best_file = None
            for file_path in files:
                if 'src/' in file_path and 'test' not in file_path:
                    best_file = file_path
                    break
            
            if not best_file:
                best_file = files[0]  # Ø§Ø®ØªØ± Ø§Ù„Ø£ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø£ÙØ¶Ù„
            
            files_to_delete = [f for f in files if f != best_file]
            
            suggestions.append({
                'type': 'exact_duplicate',
                'keep': best_file,
                'delete': files_to_delete,
                'reason': 'Ù…Ù„ÙØ§Øª Ù…ØªØ·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹'
            })
        
        # Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ù„Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ÙƒØ±Ø±Ø©
        for service_type, files in self.service_duplicates.items():
            if len(files) > 2:  # Ø£ÙƒØ«Ø± Ù…Ù† Ù…Ù„ÙÙŠÙ†
                suggestions.append({
                    'type': 'service_consolidation',
                    'service': service_type,
                    'files': files,
                    'reason': f'Ø¯Ù…Ø¬ Ø®Ø¯Ù…Ø§Øª {service_type}'
                })
        
        # Ø­ÙØ¸ Ø§Ù„Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª
        with open('cleanup_suggestions.json', 'w', encoding='utf-8') as f:
            json.dump(suggestions, f, indent=2, ensure_ascii=False)
        
        print('âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª Ø§Ù„ØªÙ†Ø¸ÙŠÙ ÙÙŠ: cleanup_suggestions.json')
        
        return suggestions
        
    def run(self):
        self.scan_project()
        self.find_exact_duplicates()
        self.find_service_duplicates()
        duplicate_classes = self.find_similar_classes()
        config_files = self.find_config_duplicates()
        self.generate_report()
        suggestions = self.create_cleanup_suggestions()
        
        print('\nğŸ¯ Ù…Ù„Ø®Øµ Ø§Ù„Ù†ØªØ§Ø¦Ø¬:')
        print(f'  - Ù…Ù„ÙØ§Øª Ù…Ø·Ø§Ø¨Ù‚Ø© ØªÙ…Ø§Ù…Ø§Ù‹: {len(self.duplicates)}')
        print(f'  - Ø®Ø¯Ù…Ø§Øª Ù…ÙƒØ±Ø±Ø©: {len(self.service_duplicates)}')
        print(f'  - ÙƒÙ„Ø§Ø³Ø§Øª Ù…ÙƒØ±Ø±Ø©: {len(duplicate_classes)}')
        print(f'  - Ù…Ù„ÙØ§Øª ØªÙƒÙˆÙŠÙ†: {len(config_files)}')
        print(f'  - Ø§Ù‚ØªØ±Ø§Ø­Ø§Øª ØªÙ†Ø¸ÙŠÙ: {len(suggestions)}')

if __name__ == "__main__":
    finder = QuickDuplicateFinder()
    finder.run() 