"""
Chaos Engineering Metrics and Monitoring
SRE Team Implementation - Task 15
Advanced monitoring and metrics collection for chaos experiments
"""

import asyncio
import logging
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List

import requests

logger = logging.getLogger(__name__)


@dataclass
class ChaosMetric:
    """Individual chaos metric data point"""

    timestamp: datetime
    experiment_id: str
    service_name: str
    metric_name: str
    metric_value: float
    tags: Dict[str, str]


@dataclass
class SystemHealthSnapshot:
    """System health snapshot during chaos"""

    timestamp: datetime
    experiment_id: str
    services_healthy: int
    services_total: int
    avg_response_time: float
    error_rate: float
    throughput: float
    safety_violations: int


class ChaosMetricsCollector:
    """Advanced metrics collector for chaos experiments"""

    def __init__(self):
        self.metrics_buffer: List[ChaosMetric] = []
        self.health_snapshots: List[SystemHealthSnapshot] = []
        self.service_endpoints = {
            "child-service": "http://child-service:8000",
            "ai-service": "http://ai-service:8000",
            "safety-service": "http://safety-service:8000",
            "graphql-federation": "http://graphql-federation:8000",
        }
        self.prometheus_endpoint = "http://prometheus:9090"

    async def start_monitoring(self, experiment_id: str, duration_seconds: int):
        """Start monitoring during chaos experiment"""
        logger.info(f"ðŸ“Š Starting chaos monitoring for {experiment_id}")
        end_time = time.time() + duration_seconds
        while time.time() < end_time:
            await self._collect_system_metrics(experiment_id)
            await self._collect_health_snapshot(experiment_id)
            await self._collect_safety_metrics(experiment_id)
            await asyncio.sleep(10)
        logger.info(f"ðŸ“Š Chaos monitoring completed for {experiment_id}")

    async def _collect_system_metrics(self, experiment_id: str):
        """Collect system performance metrics"""
        for service_name, endpoint in self.service_endpoints.items():
            try:
                start_time = time.time()
                response = requests.get(f"{endpoint}/health", timeout=5)
                response_time = time.time() - start_time
                self.metrics_buffer.append(
                    ChaosMetric(
                        timestamp=datetime.now(),
                        experiment_id=experiment_id,
                        service_name=service_name,
                        metric_name="response_time",
                        metric_value=response_time,
                        tags={"status": str(response.status_code)},
                    )
                )
                if response.status_code == 200:
                    data = response.json()
                    if "memory_usage" in data:
                        self.metrics_buffer.append(
                            ChaosMetric(
                                timestamp=datetime.now(),
                                experiment_id=experiment_id,
                                service_name=service_name,
                                metric_name="memory_usage",
                                metric_value=data["memory_usage"],
                                tags={"unit": "MB"},
                            )
                        )
            except Exception as e:
                logger.error(f"Failed to collect metrics for {service_name}: {e}")

    async def _collect_health_snapshot(self, experiment_id: str):
        """Collect overall system health snapshot"""
        healthy_services = 0
        total_services = len(self.service_endpoints)
        response_times = []
        error_count = 0
        for service_name, endpoint in self.service_endpoints.items():
            try:
                start_time = time.time()
                response = requests.get(f"{endpoint}/health", timeout=5)
                response_time = time.time() - start_time
                response_times.append(response_time)
                if response.status_code == 200:
                    healthy_services += 1
                else:
                    error_count += 1
            except Exception:
                error_count += 1
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else 0
        )
        error_rate = error_count / total_services
        throughput = healthy_services * 10.0
        snapshot = SystemHealthSnapshot(
            timestamp=datetime.now(),
            experiment_id=experiment_id,
            services_healthy=healthy_services,
            services_total=total_services,
            avg_response_time=avg_response_time,
            error_rate=error_rate,
            throughput=throughput,
            safety_violations=0,
        )
        self.health_snapshots.append(snapshot)

    async def _collect_safety_metrics(self, experiment_id: str):
        """Collect safety-specific metrics"""
        try:
            safety_test = requests.post(
                "http://safety-service:8000/moderate",
                json={"content": "test safety monitoring"},
                timeout=5,
            )
            if safety_test.status_code == 200:
                data = safety_test.json()
                confidence = data.get("confidence", 0.0)
                self.metrics_buffer.append(
                    ChaosMetric(
                        timestamp=datetime.now(),
                        experiment_id=experiment_id,
                        service_name="safety-service",
                        metric_name="safety_confidence",
                        metric_value=confidence,
                        tags={"test_type": "monitoring"},
                    )
                )
        except Exception as e:
            logger.error(f"Safety metrics collection failed: {e}")

    def generate_experiment_report(self, experiment_id: str) -> Dict[str, Any]:
        """Generate comprehensive experiment report"""
        experiment_metrics = [
            m for m in self.metrics_buffer if m.experiment_id == experiment_id
        ]
        experiment_snapshots = [
            s for s in self.health_snapshots if s.experiment_id == experiment_id
        ]
        if not experiment_metrics and not experiment_snapshots:
            return {"error": f"No metrics found for experiment {experiment_id}"}
        response_times = [
            m.metric_value
            for m in experiment_metrics
            if m.metric_name == "response_time"
        ]
        memory_usage = [
            m.metric_value
            for m in experiment_metrics
            if m.metric_name == "memory_usage"
        ]
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else 0
        )
        max_response_time = max(response_times) if response_times else 0
        min_response_time = min(response_times) if response_times else 0
        avg_memory_usage = sum(memory_usage) / len(memory_usage) if memory_usage else 0
        if experiment_snapshots:
            avg_health_ratio = sum(
                s.services_healthy / s.services_total for s in experiment_snapshots
            ) / len(experiment_snapshots)
            max_error_rate = max(s.error_rate for s in experiment_snapshots)
            min_throughput = min(s.throughput for s in experiment_snapshots)
            total_safety_violations = sum(
                s.safety_violations for s in experiment_snapshots
            )
        else:
            avg_health_ratio = 0
            max_error_rate = 0
            min_throughput = 0
            total_safety_violations = 0
        service_analysis = {}
        for service_name in self.service_endpoints.keys():
            service_metrics = [
                m for m in experiment_metrics if m.service_name == service_name
            ]
            service_response_times = [
                m.metric_value
                for m in service_metrics
                if m.metric_name == "response_time"
            ]
            if service_response_times:
                service_analysis[service_name] = {
                    "avg_response_time": sum(service_response_times)
                    / len(service_response_times),
                    "max_response_time": max(service_response_times),
                    "metric_count": len(service_metrics),
                    "availability": len(
                        [rt for rt in service_response_times if rt < 5.0]
                    )
                    / len(service_response_times),
                }
        grade = self._calculate_experiment_grade(
            avg_health_ratio, max_error_rate, avg_response_time, total_safety_violations
        )
        return {
            "experiment_id": experiment_id,
            "duration_minutes": len(experiment_snapshots) / 6,
            "metrics_collected": len(experiment_metrics),
            "snapshots_collected": len(experiment_snapshots),
            "performance_summary": {
                "avg_response_time": round(avg_response_time, 3),
                "max_response_time": round(max_response_time, 3),
                "min_response_time": round(min_response_time, 3),
                "avg_memory_usage": round(avg_memory_usage, 2),
            },
            "health_summary": {
                "avg_health_ratio": round(avg_health_ratio, 3),
                "max_error_rate": round(max_error_rate, 3),
                "min_throughput": round(min_throughput, 2),
                "total_safety_violations": total_safety_violations,
            },
            "service_analysis": service_analysis,
            "grade": grade,
            "timestamp": datetime.now().isoformat(),
        }

    def _calculate_experiment_grade(
        self,
        health_ratio: float,
        error_rate: float,
        response_time: float,
        safety_violations: int,
    ) -> str:
        """Calculate experiment grade based on metrics"""
        score = 0
        if health_ratio >= 0.95:
            score += 40
        elif health_ratio >= 0.9:
            score += 35
        elif health_ratio >= 0.8:
            score += 25
        elif health_ratio >= 0.7:
            score += 15
        if error_rate <= 0.05:
            score += 30
        elif error_rate <= 0.1:
            score += 25
        elif error_rate <= 0.2:
            score += 15
        elif error_rate <= 0.3:
            score += 10
        if response_time <= 1.0:
            score += 20
        elif response_time <= 2.0:
            score += 15
        elif response_time <= 5.0:
            score += 10
        elif response_time <= 10.0:
            score += 5
        if safety_violations == 0:
            score += 10
        elif safety_violations <= 2:
            score += 5
        if score >= 90:
            return "A+ (Excellent)"
        elif score >= 80:
            return "A (Very Good)"
        elif score >= 70:
            return "B (Good)"
        elif score >= 60:
            return "C (Fair)"
        elif score >= 50:
            return "D (Poor)"
        else:
            return "F (Fail)"

    def export_metrics_to_prometheus(self, experiment_id: str) -> bool:
        """Export metrics to Prometheus"""
        try:
            experiment_metrics = [
                m for m in self.metrics_buffer if m.experiment_id == experiment_id
            ]
            prometheus_data = []
            for metric in experiment_metrics:
                prometheus_line = f'chaos_{metric.metric_name}{{experiment_id="{metric.experiment_id}",service="{metric.service_name}"}} {metric.metric_value} {int(metric.timestamp.timestamp() * 1000)}'
                prometheus_data.append(prometheus_line)
            if prometheus_data:
                logger.info(f"ðŸ“Š Exported {len(prometheus_data)} metrics to Prometheus")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to export metrics to Prometheus: {e}")
            return False

    def cleanup_old_metrics(self, retention_hours: int = 24):
        """Clean up old metrics to manage memory"""
        cutoff_time = datetime.now() - timedelta(hours=retention_hours)
        initial_count = len(self.metrics_buffer)
        self.metrics_buffer = [
            m for m in self.metrics_buffer if m.timestamp > cutoff_time
        ]
        initial_snapshots = len(self.health_snapshots)
        self.health_snapshots = [
            s for s in self.health_snapshots if s.timestamp > cutoff_time
        ]
        cleaned_metrics = initial_count - len(self.metrics_buffer)
        cleaned_snapshots = initial_snapshots - len(self.health_snapshots)
        if cleaned_metrics > 0 or cleaned_snapshots > 0:
            logger.info(
                f"ðŸ§¹ Cleaned up {cleaned_metrics} metrics and {cleaned_snapshots} snapshots"
            )

    def get_real_time_dashboard_data(self, experiment_id: str) -> Dict[str, Any]:
        """Get real-time dashboard data for ongoing experiment"""
        recent_time = datetime.now() - timedelta(minutes=5)
        recent_metrics = [
            m
            for m in self.metrics_buffer
            if m.experiment_id == experiment_id and m.timestamp > recent_time
        ]
        recent_snapshots = [
            s
            for s in self.health_snapshots
            if s.experiment_id == experiment_id and s.timestamp > recent_time
        ]
        if not recent_metrics and not recent_snapshots:
            return {"status": "no_data", "experiment_id": experiment_id}
        latest_snapshot = recent_snapshots[-1] if recent_snapshots else None
        current_status = {
            "healthy_services": (
                latest_snapshot.services_healthy if latest_snapshot else 0
            ),
            "total_services": latest_snapshot.services_total if latest_snapshot else 0,
            "health_ratio": (
                latest_snapshot.services_healthy / latest_snapshot.services_total
                if latest_snapshot
                else 0
            ),
            "avg_response_time": (
                latest_snapshot.avg_response_time if latest_snapshot else 0
            ),
            "error_rate": latest_snapshot.error_rate if latest_snapshot else 0,
        }
        service_status = {}
        for service_name in self.service_endpoints.keys():
            service_metrics = [
                m for m in recent_metrics if m.service_name == service_name
            ]
            response_times = [
                m.metric_value
                for m in service_metrics
                if m.metric_name == "response_time"
            ]
            if response_times:
                service_status[service_name] = {
                    "status": "healthy" if response_times[-1] < 5.0 else "degraded",
                    "last_response_time": response_times[-1],
                    "avg_response_time": sum(response_times) / len(response_times),
                }
            else:
                service_status[service_name] = {
                    "status": "unknown",
                    "last_response_time": 0,
                    "avg_response_time": 0,
                }
        time_series = {
            "timestamps": [s.timestamp.isoformat() for s in recent_snapshots],
            "health_ratios": [
                (s.services_healthy / s.services_total) for s in recent_snapshots
            ],
            "response_times": [s.avg_response_time for s in recent_snapshots],
            "error_rates": [s.error_rate for s in recent_snapshots],
        }
        return {
            "status": "active",
            "experiment_id": experiment_id,
            "current_status": current_status,
            "service_status": service_status,
            "time_series": time_series,
            "last_updated": datetime.now().isoformat(),
        }
