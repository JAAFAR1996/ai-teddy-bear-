"""
Chaos Engineering Metrics and Monitoring
SRE Team Implementation - Task 15
Advanced monitoring and metrics collection for chaos experiments
"""

import asyncio
import logging
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List

import requests

logger = logging.getLogger(__name__)


@dataclass
class ChaosMetric:
    """Individual chaos metric data point"""

    timestamp: datetime
    experiment_id: str
    service_name: str
    metric_name: str
    metric_value: float
    tags: Dict[str, str]


@dataclass
class SystemHealthSnapshot:
    """System health snapshot during chaos"""

    timestamp: datetime
    experiment_id: str
    services_healthy: int
    services_total: int
    avg_response_time: float
    error_rate: float
    throughput: float
    safety_violations: int


class ChaosMetricsCollector:
    """Advanced metrics collector for chaos experiments"""

    def __init__(self):
        self.metrics_buffer: List[ChaosMetric] = []
        self.health_snapshots: List[SystemHealthSnapshot] = []
        self.service_endpoints = {
            "child-service": "http://child-service:8000",
            "ai-service": "http://ai-service:8000",
            "safety-service": "http://safety-service:8000",
            "graphql-federation": "http://graphql-federation:8000",
        }
        self.prometheus_endpoint = "http://prometheus:9090"

    async def start_monitoring(self, experiment_id: str, duration_seconds: int):
        """Start monitoring during chaos experiment"""
        logger.info(f"ðŸ“Š Starting chaos monitoring for {experiment_id}")
        end_time = time.time() + duration_seconds
        while time.time() < end_time:
            await self._collect_system_metrics(experiment_id)
            await self._collect_health_snapshot(experiment_id)
            await self._collect_safety_metrics(experiment_id)
            await asyncio.sleep(10)
        logger.info(f"ðŸ“Š Chaos monitoring completed for {experiment_id}")

    async def _collect_system_metrics(self, experiment_id: str):
        """Collect system performance metrics"""
        for service_name, endpoint in self.service_endpoints.items():
            try:
                start_time = time.time()
                response = requests.get(f"{endpoint}/health", timeout=5)
                response_time = time.time() - start_time
                self.metrics_buffer.append(
                    ChaosMetric(
                        timestamp=datetime.now(),
                        experiment_id=experiment_id,
                        service_name=service_name,
                        metric_name="response_time",
                        metric_value=response_time,
                        tags={"status": str(response.status_code)},
                    )
                )
                if response.status_code == 200:
                    data = response.json()
                    if "memory_usage" in data:
                        self.metrics_buffer.append(
                            ChaosMetric(
                                timestamp=datetime.now(),
                                experiment_id=experiment_id,
                                service_name=service_name,
                                metric_name="memory_usage",
                                metric_value=data["memory_usage"],
                                tags={"unit": "MB"},
                            )
                        )
            except Exception as e:
                logger.error(
                    f"Failed to collect metrics for {service_name}: {e}")

    async def _collect_health_snapshot(self, experiment_id: str):
        """Collect overall system health snapshot"""
        healthy_services = 0
        total_services = len(self.service_endpoints)
        response_times = []
        error_count = 0
        for service_name, endpoint in self.service_endpoints.items():
            try:
                start_time = time.time()
                response = requests.get(f"{endpoint}/health", timeout=5)
                response_time = time.time() - start_time
                response_times.append(response_time)
                if response.status_code == 200:
                    healthy_services += 1
                else:
                    error_count += 1
            except Exception as exc:
                error_count += 1
        avg_response_time = (
            sum(response_times) / len(response_times) if response_times else 0
        )
        error_rate = error_count / total_services
        throughput = healthy_services * 10.0
        snapshot = SystemHealthSnapshot(
            timestamp=datetime.now(),
            experiment_id=experiment_id,
            services_healthy=healthy_services,
            services_total=total_services,
            avg_response_time=avg_response_time,
            error_rate=error_rate,
            throughput=throughput,
            safety_violations=0,
        )
        self.health_snapshots.append(snapshot)

    async def _collect_safety_metrics(self, experiment_id: str):
        """Collect safety-specific metrics"""
        try:
            safety_test = requests.post(
                "http://safety-service:8000/moderate",
                json={"content": "test safety monitoring"},
                timeout=5,
            )
            if safety_test.status_code == 200:
                data = safety_test.json()
                confidence = data.get("confidence", 0.0)
                self.metrics_buffer.append(
                    ChaosMetric(
                        timestamp=datetime.now(),
                        experiment_id=experiment_id,
                        service_name="safety-service",
                        metric_name="safety_confidence",
                        metric_value=confidence,
                        tags={"test_type": "monitoring"},
                    )
                )
        except Exception as e:
            logger.error(f"Safety metrics collection failed: {e}")

    def _filter_experiment_data(self, experiment_id: str) -> tuple:
        """Filter metrics and snapshots for specific experiment."""
        experiment_metrics = [
            m for m in self.metrics_buffer if m.experiment_id == experiment_id]
        experiment_snapshots = [
            s for s in self.health_snapshots if s.experiment_id == experiment_id]
        return experiment_metrics, experiment_snapshots

    def _calculate_performance_metrics(self, experiment_metrics: List[ChaosMetric]) -> Dict[str, float]:
        """Calculate performance metrics from experiment data."""
        response_times = [
            m.metric_value for m in experiment_metrics if m.metric_name == "response_time"]
        memory_usage = [
            m.metric_value for m in experiment_metrics if m.metric_name == "memory_usage"]

        return {
            "avg_response_time": sum(response_times) / len(response_times) if response_times else 0,
            "max_response_time": max(response_times) if response_times else 0,
            "min_response_time": min(response_times) if response_times else 0,
            "avg_memory_usage": sum(memory_usage) / len(memory_usage) if memory_usage else 0,
        }

    def _calculate_health_metrics(self, experiment_snapshots: List[SystemHealthSnapshot]) -> Dict[str, float]:
        """Calculate health metrics from experiment snapshots."""
        if not experiment_snapshots:
            return {
                "avg_health_ratio": 0,
                "max_error_rate": 0,
                "min_throughput": 0,
                "total_safety_violations": 0,
            }

        return {
            "avg_health_ratio": sum(s.services_healthy / s.services_total for s in experiment_snapshots) / len(experiment_snapshots),
            "max_error_rate": max(s.error_rate for s in experiment_snapshots),
            "min_throughput": min(s.throughput for s in experiment_snapshots),
            "total_safety_violations": sum(s.safety_violations for s in experiment_snapshots),
        }

    def _analyze_services(self, experiment_metrics: List[ChaosMetric]) -> Dict[str, Dict[str, float]]:
        """Analyze metrics per service."""
        service_analysis = {}
        for service_name in self.service_endpoints.keys():
            service_metrics = [
                m for m in experiment_metrics if m.service_name == service_name]
            service_response_times = [
                m.metric_value for m in service_metrics if m.metric_name == "response_time"]

            if service_response_times:
                service_analysis[service_name] = {
                    "avg_response_time": sum(service_response_times) / len(service_response_times),
                    "max_response_time": max(service_response_times),
                    "metric_count": len(service_metrics),
                    "availability": len([rt for rt in service_response_times if rt < 5.0]) / len(service_response_times),
                }
        return service_analysis

    def generate_experiment_report(self, experiment_id: str) -> Dict[str, Any]:
        """Generate comprehensive experiment report"""
        experiment_metrics, experiment_snapshots = self._filter_experiment_data(
            experiment_id)

        if not experiment_metrics and not experiment_snapshots:
            return {"error": f"No metrics found for experiment {experiment_id}"}

        performance_metrics = self._calculate_performance_metrics(
            experiment_metrics)
        health_metrics = self._calculate_health_metrics(experiment_snapshots)
        service_analysis = self._analyze_services(experiment_metrics)

        grade = self._calculate_experiment_grade(
            health_metrics["avg_health_ratio"],
            health_metrics["max_error_rate"],
            performance_metrics["avg_response_time"],
            health_metrics["total_safety_violations"]
        )

        return {
            "experiment_id": experiment_id,
            "duration_minutes": len(experiment_snapshots) / 6,
            "metrics_collected": len(experiment_metrics),
            "snapshots_collected": len(experiment_snapshots),
            "performance_summary": {
                "avg_response_time": round(performance_metrics["avg_response_time"], 3),
                "max_response_time": round(performance_metrics["max_response_time"], 3),
                "min_response_time": round(performance_metrics["min_response_time"], 3),
                "avg_memory_usage": round(performance_metrics["avg_memory_usage"], 2),
            },
            "health_summary": {
                "avg_health_ratio": round(health_metrics["avg_health_ratio"], 3),
                "max_error_rate": round(health_metrics["max_error_rate"], 3),
                "min_throughput": round(health_metrics["min_throughput"], 2),
                "total_safety_violations": health_metrics["total_safety_violations"],
            },
            "service_analysis": service_analysis,
            "grade": grade,
            "timestamp": datetime.now().isoformat(),
        }

    def _score_health_ratio(self, health_ratio: float) -> int:
        """Score based on service health ratio."""
        if health_ratio >= 0.95:
            return 40
        elif health_ratio >= 0.9:
            return 35
        elif health_ratio >= 0.8:
            return 25
        elif health_ratio >= 0.7:
            return 15
        return 0

    def _score_error_rate(self, error_rate: float) -> int:
        """Score based on error rate."""
        if error_rate <= 0.05:
            return 30
        elif error_rate <= 0.1:
            return 25
        elif error_rate <= 0.2:
            return 15
        elif error_rate <= 0.3:
            return 10
        return 0

    def _score_response_time(self, response_time: float) -> int:
        """Score based on response time."""
        if response_time <= 1.0:
            return 20
        elif response_time <= 2.0:
            return 15
        elif response_time <= 5.0:
            return 10
        elif response_time <= 10.0:
            return 5
        return 0

    def _score_safety_violations(self, safety_violations: int) -> int:
        """Score based on safety violations."""
        if safety_violations == 0:
            return 10
        elif safety_violations <= 2:
            return 5
        return 0

    def _convert_score_to_grade(self, score: int) -> str:
        """Convert numerical score to letter grade."""
        if score >= 90:
            return "A+ (Excellent)"
        elif score >= 80:
            return "A (Very Good)"
        elif score >= 70:
            return "B (Good)"
        elif score >= 60:
            return "C (Fair)"
        elif score >= 50:
            return "D (Poor)"
        else:
            return "F (Fail)"

    def _calculate_experiment_grade(
        self,
        health_ratio: float,
        error_rate: float,
        response_time: float,
        safety_violations: int,
    ) -> str:
        """Calculate experiment grade based on metrics"""
        score = (
            self._score_health_ratio(health_ratio) +
            self._score_error_rate(error_rate) +
            self._score_response_time(response_time) +
            self._score_safety_violations(safety_violations)
        )
        return self._convert_score_to_grade(score)

    def export_metrics_to_prometheus(self, experiment_id: str) -> bool:
        """Export metrics to Prometheus"""
        try:
            experiment_metrics = [
                m for m in self.metrics_buffer if m.experiment_id == experiment_id
            ]
            prometheus_data = []
            for metric in experiment_metrics:
                prometheus_line = f'chaos_{metric.metric_name}{{experiment_id="{metric.experiment_id}",service="{metric.service_name}"}} {metric.metric_value} {int(metric.timestamp.timestamp() * 1000)}'
                prometheus_data.append(prometheus_line)
            if prometheus_data:
                logger.info(
                    f"ðŸ“Š Exported {len(prometheus_data)} metrics to Prometheus")
                return True
            return False
        except Exception as e:
            logger.error(f"Failed to export metrics to Prometheus: {e}")
            return False

    def cleanup_old_metrics(self, retention_hours: int = 24):
        """Clean up old metrics to manage memory"""
        cutoff_time = datetime.now() - timedelta(hours=retention_hours)
        initial_count = len(self.metrics_buffer)
        self.metrics_buffer = [
            m for m in self.metrics_buffer if m.timestamp > cutoff_time
        ]
        initial_snapshots = len(self.health_snapshots)
        self.health_snapshots = [
            s for s in self.health_snapshots if s.timestamp > cutoff_time
        ]
        cleaned_metrics = initial_count - len(self.metrics_buffer)
        cleaned_snapshots = initial_snapshots - len(self.health_snapshots)
        if cleaned_metrics > 0 or cleaned_snapshots > 0:
            logger.info(
                f"ðŸ§¹ Cleaned up {cleaned_metrics} metrics and {cleaned_snapshots} snapshots"
            )

    def _get_recent_experiment_data(self, experiment_id: str) -> tuple:
        """Get recent metrics and snapshots for experiment."""
        recent_time = datetime.now() - timedelta(minutes=5)
        recent_metrics = [
            m for m in self.metrics_buffer
            if m.experiment_id == experiment_id and m.timestamp > recent_time
        ]
        recent_snapshots = [
            s for s in self.health_snapshots
            if s.experiment_id == experiment_id and s.timestamp > recent_time
        ]
        return recent_metrics, recent_snapshots

    def _build_current_status(self, latest_snapshot: SystemHealthSnapshot) -> Dict[str, Any]:
        """Build current status from latest snapshot."""
        if not latest_snapshot:
            return {
                "healthy_services": 0,
                "total_services": 0,
                "health_ratio": 0,
                "avg_response_time": 0,
                "error_rate": 0,
            }

        return {
            "healthy_services": latest_snapshot.services_healthy,
            "total_services": latest_snapshot.services_total,
            "health_ratio": latest_snapshot.services_healthy / latest_snapshot.services_total,
            "avg_response_time": latest_snapshot.avg_response_time,
            "error_rate": latest_snapshot.error_rate,
        }

    def _build_service_status(self, recent_metrics: List[ChaosMetric]) -> Dict[str, Dict[str, Any]]:
        """Build service status from recent metrics."""
        service_status = {}
        for service_name in self.service_endpoints.keys():
            service_metrics = [
                m for m in recent_metrics if m.service_name == service_name]
            response_times = [
                m.metric_value for m in service_metrics if m.metric_name == "response_time"]

            if response_times:
                service_status[service_name] = {
                    "status": "healthy" if response_times[-1] < 5.0 else "degraded",
                    "last_response_time": response_times[-1],
                    "avg_response_time": sum(response_times) / len(response_times),
                }
            else:
                service_status[service_name] = {
                    "status": "unknown",
                    "last_response_time": 0,
                    "avg_response_time": 0,
                }
        return service_status

    def _build_time_series(self, recent_snapshots: List[SystemHealthSnapshot]) -> Dict[str, List]:
        """Build time series data from recent snapshots."""
        return {
            "timestamps": [s.timestamp.isoformat() for s in recent_snapshots],
            "health_ratios": [(s.services_healthy / s.services_total) for s in recent_snapshots],
            "response_times": [s.avg_response_time for s in recent_snapshots],
            "error_rates": [s.error_rate for s in recent_snapshots],
        }

    def get_real_time_dashboard_data(self, experiment_id: str) -> Dict[str, Any]:
        """Get real-time dashboard data for ongoing experiment"""
        recent_metrics, recent_snapshots = self._get_recent_experiment_data(
            experiment_id)

        if not recent_metrics and not recent_snapshots:
            return {"status": "no_data", "experiment_id": experiment_id}

        latest_snapshot = recent_snapshots[-1] if recent_snapshots else None
        current_status = self._build_current_status(latest_snapshot)
        service_status = self._build_service_status(recent_metrics)
        time_series = self._build_time_series(recent_snapshots)

        return {
            "status": "active",
            "experiment_id": experiment_id,
            "current_status": current_status,
            "service_status": service_status,
            "time_series": time_series,
            "last_updated": datetime.now().isoformat(),
        }
